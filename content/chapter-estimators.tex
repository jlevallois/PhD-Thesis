% !TEX root = ../main.tex
%
% Copyright 2015
% Jérémy Levallois <jeremy.levallois@gmail.com>
%
% This file and related figures are under Creative Commons CC BY-NC-SA 4.0
% See <https://creativecommons.org/licenses/by-nc-sa/4.0/>
%
\chapter{Estimateurs digitaux de courbures}
\label{sec:estimators}
%
\cleanchapterquote{Ce sont nos choix, Harry, qui montrent ce que nous sommes
vraiment, beaucoup plus que nos aptitudes.}{Albus \textsc{Dumbledore}}{Harry Potter et la
Chambre des Secrets.}
%
\setcounter{minitocdepth}{4}
\minitoc
%
\newpage
%
Dans ce chapitre, nous allons définir de nouveaux estimateurs de courbure pour
les surfaces digitales. Après avoir détaillé l'état de l'art
(\RefSection{sec:estimators:SOTA}), nous nous intéresserons à l'estimation de la
courbure par intégration locale (\RefSection{sec:estimators:volume}) afin d'en
extraire des estimateurs de courbure en 2D et courbure moyenne
(\RefSection{sec:estimators:volume}) et courbures principales
(\RefSection{sec:ii-3d}) en 3D. Ces estimateurs nécessitent une taille de boule
d'intégration; nous adapterons ces estimateurs à des versions ne nécessitant
aucun paramètre dans le \RefSection{sec:curvature:parameter-free}. Enfin, nous
comparerons nos estimateurs avec les méthodes représentatives de l'état de l'art
dans le \RefSection{sec:comparaison-courbure}. Dans ce chapitre, nous
considérons que $\Shapes = \Shapes^{C}$ \comJeremy{(famille des formes convexes à mesures finies du
plan euclidien)}.
%
\section{Introduction}
\label{sec:estimators:introduction}
%
Comme nous l'avons vu dans le chapitre précédent (\RefSection{sec:geo-diff}), la
géométrie d'une forme est décrite à partir de ses quantités différentielles
comme la courbure.
% Et nous parlons bien de décrire, puisque toutes ces quantités sont ce
%qui la caractérise de manière unique.
\comJeremy{Ces quantités, utilisées dans plusieurs champs de recherche, doivent être estimées à partir des données à disposition\cite{Petitjean2002, Wang2009}.}
%
% C'est pour cela que plusieurs champs de recherche se tournent vers la manière
% de récupérer ces quantités en les estimant à partir des
% données \cite{Petitjean2002, Wang2009}.
%
%
Les données discrètes prennent différentes formes, qui sont principalement liées à la façon de capturer les
objets ou à leur utilisation. Elles peuvent être des nuages de points, des
maillages triangulés, ou encore des données digitales. Les critères guidant les définitions des
estimateurs sont différents et très liés à l'usage de la quantité différentielle :
certains privilégieront la rapidité d'estimation en dépit de la qualité, notamment
dans un enjeu d'utilisation en temps interactif ou réel, tandis que d'autres (et
c'est notre cas) préféreront garantir la qualité des résultats de l'estimation
en priorité.


Comme nous l'avons vu dans le \RefChapitre{sec:introduction}, notre estimation
de la courbure est utilisée au cœur d'un processus d'analyse de
déformation de microstructures de grains de neige, nécessitant des résultats les
plus justes possibles. Il nous paraît également important de connaître le
comportement de l'estimateur.
%
\comJeremy{En effet, si, dans quelques années, les outils de captures de données digitales augmentent la résolution de la capture, aurons-nous une meilleure estimation de la courbure sur ces données ? }
%
% En effet, si nous choisissons le coût d'augmenter la résolution
% de la capture des données digitales : aurons-nous une meilleure estimation de la courbure ?
Nous devons alors nous focaliser sur la convergence asymptotique de
notre estimation à l'aide de preuves théoriques et de validations pratiques.
%
\section{État de l'art}%
\label{sec:estimators:SOTA}
%
\subsection{Estimation de courbure sur des maillages}
%
Les données digitales étant discrètes par nature, il semble pertinent de
s'intéresser aux techniques d'estimation de courbure sur des maillages
triangulés. Il existe une vaste famille de techniques en \ComputerGraphics et en
\GeometryProcessing pour estimer les courbures moyennes et gaussiennes, et
parfois même le tenseur de courbure entier. La plupart effectuent des analyses
locales (\cad limitées au $1$-voisinage ou $2$-voisinage) de la forme. La
précision de l'estimation dépend fortement de la « qualité » du maillage (au
sens où ce dernier approxime suffisamment précisément la surface continue sous-jacente).

\begin{figure}[ht]
    \begin{center}
      \includegraphics[width=10cm]{figures/OneRingNeighborhood}
    \end{center}
    \caption{$1$-voisinage de valence $6$ et
    $2$-voisinage de valence $5$ du sommet vert.}
    \label{fig:one-ring-neighborhood}
\end{figure}

% Globalement, nous pouvons les distinguer en trois catégories :
% \begin{itemize}
%   \item Méthode par Correspondance
%   \item Méthodes discrètes
%   \item Estimation du tenseur de courbure
% \end{itemize}
%
\cauthors{Surazhsky}{Surazhsky2003} et \cauthors{Gatzke}{Gatzke2006} ont proposé
une étude comparative de ces estimateurs. Nous nous référons à
\cauthors{Bobenko}{Bobenko2008} pour une théorie entièrement discrète.
Cependant, la plupart n'ont aucune garantie de convergence théorique des
valeurs estimées, même sans bruit sur le maillage. Nous pouvons citer
\cite{Rusinkiewicz2004} (et \cite{Page2002} pour les normales), qui essaye de lisser les perturbations de la surface à l'aide d'un moyennage

\paragraph{Approche par formule de Gauss-Bonnet}
%
\cauthor{Xu}{Xu2006} estime la courbure gaussienne avec une approche dérivée de
la formule de Gauss-Bonnet. Cette approche a beaucoup été
étudiée dans la littérature pour l'estimation de courbure gaussienne
\cite{Meek2000, Stokely1992} mais aussi pour de la reconstruction \cite{Dyn2001}
ou de la simplification de surface \cite{Kim2002}, et a même été déclarée « le
meilleur algorithme (parmi 5 approches) pour l'estimation de la courbure
gaussienne » par \cauthors{Surazhsky}{Surazhsky2003}.

\begin{figure}[ht]
    \begin{center}
      \includegraphics[width=5.5cm]{images/Curvature/Notations_Xu}
      \includegraphics[width=7.5cm]{images/Curvature/Notations_Xu_2}
    \end{center}
    \caption[Notations pour \cauthor{Xu}{Xu2006}.]{\emph{À gauche :} Notations pour \cauthor{Xu}{Xu2006} (Figure~1 de \cite{Xu2006}). \emph{À droite :} Triangulation nécessaire du domaine (Figure~3 de \cite{Xu2006}).}
    \label{fig:xu_notations}
\end{figure}

Soit $\vp$ un sommet de valence $6$ du maillage triangulé $M$ et $\vp_j$ avec
$j = 1,\dotsc, 6$ l'ensemble des sommets du $1$-voisinage de $\vp$ (voir
\RefFigureN{fig:xu_notations}-gauche pour les notations). Si $\vp$ et $\vp_j$
sont sur une surface paramétrique suffisamment lisse $F(x,y) \in \R^3$ et
qu'il existe $\vq, \vq_j \in \R^2$ tels que
(\RefFigureN{fig:xu_notations}-droite, le $1$-voisinage est projeté comme des
parallélogrammes sur un plan) :
%
\begin{equation}
  \vp=F(\vq), \quad \vp_j=F(\vq_j) \quad \text{~et~} \vq_j=\vq_{j-} + \vq_{j+} - \vq \,,
\end{equation}
%
Alors, en assumant que la densité de l’échantillonnage est $\delta$ avec $A(\vp, \delta)$ la somme des aires des triangles
$[\vp\vp_j(\delta)\vp_{j+}(\delta)]$, $\omega_j$ l'angle $\angle
\vp_j\vp\vp_{j+}$, ils démontrent que :
%
\begin{equation}
  \frac{3}{A(\vp,\delta)} \left[ 2\pi - \sum \limits^6_{j=1} \omega_j(\delta) \right] \EqDef \GaussCurv(\vp) + O(\delta^2) \,,
\end{equation}
%
quand $\delta$ tend vers $0$, où $\GaussCurv(\vp)$ est la valeur de courbure
gaussienne au point $\vp$.
%
%
L'auteur propose une propriété de convergence supplémentaire lorsque
l'échantillonnage est perturbé par une erreur $O(\delta^\alpha)$, mais avec
$\alpha \ge 3$.


Cette approche n'est pas applicable directement à des surfaces digitales car la
perturbation par rapport à la surface est trop grande et
correspond plus à des données aberrantes (\anglais{outliers}) qu'à du bruit de l'échantillonnage. De
plus, les contraintes topologiques expliquées précédemment (surface
paramétrique, sommets de valence $6$) sont déterminantes pour la convergence :
si elles ne sont pas satisfaites, l'estimation de la courbure gaussienne ne
converge pas.
%
\paragraph{Cycles normaux}
%
Une autre approche notable est l'estimation d'information de courbure par
intégration de mesures des courbures, basée sur la théorie des cycles normaux sur
un maillage triangulé \cite{CohenSteiner2003,CohenSteiner2006}.
%
%
La théorie des cycles normaux, introduite par \cauthor{Wintgen}{Wintgen1982} et
\cauthor{Zähle}{Zahle1986} est une technique qui permet de définir de manière
unifiée la courbure de surfaces lisses et polyédrales. En effet, la mesure de
courbure peut être obtenue par les cycles normaux de la surface. L'idée
principale est assez similaire à l'approche par formule de Gauss-Bonnet
vue juste avant et permet de s'affranchir de la contrainte de la valence des
sommets.


Pour une surface lisse $M$ d'un compact $V \subset \R^3$, les auteurs
définissent la mesure de courbure gaussienne $\phi^\GaussCurv_V$ et de
courbure moyenne $\phi^\MeanCurv_V$ d'un ensemble ouvert $B \subset \R^3$ comme :
%
\begin{align}
  &\phi^\GaussCurv_V(B) \EqDef \int_{B \cap M} \GaussCurv(\vp) d\vp \,,\\
  &\phi^\MeanCurv_V(B)  \EqDef \int_{B \cap M} \MeanCurv(\vp)  d\vp \,,
\end{align}
%
avec $\vp \in M$, où $\GaussCurv(\vp)$ et $\MeanCurv(\vp)$ sont respectivement
la courbure gaussienne et moyenne du point $\vp$. Ces mesures sont des mesures
intégrales, \cad qu'elles ne correspondent pas à une valeur pour un point de
la surface, mais pour l'ensemble $B$.


Les auteurs (\nauthors{Cohen-Steiner}) proposent alors de calculer ces deux
mesures dans le cas d'un maillage triangulé. Si $V$ est un polyèdre dont
l'ensemble des sommets est $P$ et l'ensemble d'arêtes est $E$, ils
proposent de calculer les mesures de courbure gaussienne et moyenne en sommant
des quantités :
%
\begin{align}
  &\phi^\GaussCurv_V(B) \EqDef \sum_{\vp \in B \cap P} g(\vp) \,,\\
  &\phi^\MeanCurv_V(B)  \EqDef \sum_{\ve \in E} \text{longueur}(\ve \cap B) \beta(\ve) \,,
\end{align}
%
où $g(\vp)$ correspond au \colorize{défaut d'angle} de $\Boundary V$ au point
$\vp$, \cad à $2\pi$ moins la somme des angles entre les arêtes incidentes
consécutives à $\vp$  (voir à gauche de \RefFigure{fig:normal-cycle}), et
$|\beta(\ve)|$ est l'angle entre les normales des triangles de $\Boundary V$
incidents au point $\vp$ (voir à droite de \RefFigure{fig:normal-cycle}). Si
$\ve$ est convexe, le signe de $\beta(\ve)$ doit être positif et négatif si
$\ve$ est concave.


\begin{figure}[ht]{
    \begin{center}
    \includegraphics[height=4cm]{figures/NormalCycle1}
    \includegraphics[height=4cm]{figures/NormalCycle2}
    \end{center}}
    \caption{Principe de l'estimation de courbure des cycles normaux. La courbure gaussienne est calculée avec la somme des angles des arêtes incidentes au point concerné (\emph{à gauche}) et la courbure gaussienne avec la somme des angles des normales des faces incidentes au point (\emph{à droite}).
      \label{fig:normal-cycle}}
\end{figure}

Enfin, ils proposent des propriétés de convergence de ces mesures de courbure
sous certaines conditions. Soit $\mathcal{P}$ un sous-ensemble fini de $\dS$,
$T$ la triangulation de Delaunay de $\mathcal{P}$ restreinte à $\Shape$, et
$\epsilon$ la densité de l'échantillonnage de la surface.
%
\comJeremy{$\mathcal{P}$ est le $\epsilon$-échantillonnage de $\dS$ si la boule
$\Ball{\epsilon lfs(\vx)}{\vx}$ centrée au point $\vx$ et de rayon $lfs(\vx)$
rencontre $\mathcal{P}$}.
%
Le $lfs(\vx)$ (pour \anglais{local feature size}) correspond à la
distance entre $\vx$ et l'axe médian de $\Shape$. Pour résumer, cela signifie
que l'échantillonnage doit être plus dense sur les zones à forte courbure.
Alors, lorsque $\epsilon$ est suffisamment petit\footnote{Dans leur article, \cauthors{Cohen-Steiner}{CohenSteiner2003} fixent $\epsilon < 0.08$.} :
%
\begin{align}
  &|\phi^\GaussCurv_T(B) - \tilde{\phi}^\GaussCurv_V(\pi(B)) | \le K\epsilon \,,\\
  &|\phi^\MeanCurv_T(B)  - \tilde{\phi}^\GaussCurv_V(\pi(B)) | \le K\epsilon \,,
\end{align}
%
avec $\pi$ la projection sur $M$ (grâce à un $\epsilon$ suffisamment petit, $\pi$ est défini
sur $\Boundary T$ et est un homéomorphe de $\Boundary T$ vers $M$), et $K$ une constante.


Les deux principaux problèmes avec cette méthode sont son inadaptabilité face à
des données digitales en ce qui concerne l'échantillonnage et les contraintes
sur la topologie, et le fait que la mesure se fait sur un ensemble de
points et non pas ponctuellement.
%
\paragraph{Invariants par intégration}
%
Enfin, en \emph{traitement géométrique}, des outils mathématiques ont été
développés pour concevoir des estimateurs différentiels sur des surfaces lisses
basées sur les invariants par intégration
(\cauthors{Pottmann}{Pottmann2007,Pottmann2009}). Le principe consiste à
déplacer une boule le long de la surface de la forme et à calculer des
intégrales sur l'intersection entre la forme et la boule. Les auteurs ont
démontré que ces quantités par intégration apportent d'intéressantes
informations de courbures lorsque le rayon de la boule tend vers zéro. Ils
amènent également des preuves de stabilité --- dans le cas d'échantillonnage de
maillage par exemple --- dépendant de la taille de la boule et du paramètre
d'échantillonnage $\epsilon$. Nos travaux sur les estimateurs digitaux de
courbures se basant sur la même idée, nous détaillerons cette approche dans un
prochain paragraphe (\RefSection{sec:pottmann-principle}).
%
\subsection{Estimation de courbure sur des nuages de points}
%
\paragraph{Ajustement polynomial des jets osculateurs}
%
Lorsque nous avons des nuages de points, la façon la plus naturelle d'approcher
la courbure est de faire correspondre une surface polynomiale de degré $2$ ou
plus à l'ensemble de points. Le meilleur représentant de ces techniques est
l'estimation des quantités différentielles par ajustement polynomial des jets
osculateurs (\anglais{osculating jets}) de \cauthors{Cazals}{Cazals2005}. Un jet
est un développement de Taylor tronqué, un $n$-jet est un développement de
Taylor tronqué à l'ordre $n$. L'intérêt de ces jets est qu'ils contiennent des
informations sur les quantités géométriques locales (tangentes / normales,
courbures principales, etc.).
%
%
Ainsi, le développement de Taylor d'une courbe à l'ordre $n$ équivaut à :
%
\begin{align}
  &f(x) \EqDef J_{B,n}(x) + O(x^{n+1}) \,, \\
  &J_{B,n}(x) \EqDef B_0 + B_1x + B_2x^2 + B_3x^3 + \cdots + B_nx^n \,.
\end{align}
%
Dans le cas d'une surface, nous avons :
%
\begin{align}
  &f(x,y) \EqDef J_{B,n}(x,y) + O(||(x,y)||^{n+1}) \,, \\
  &J_{B,n}(x,y) \EqDef \sum_{k=1}^{n} \left( \sum_{j=0}^{k} B_{k-j,j}x^{k-j}y^j \right) \,,
\end{align}
%
où les développements tronqués de Taylor $J_{B,n}(x)$ et $J_{B,n}(x,y)$ sont les
$n$-jets. Il est à noter que le terme de jet osculateur désigne les $2$-jets.
Ainsi, comme les $n$-jets contiennent des propriétés différentielles de la
courbe / surface de l'ordre $n$; cela implique que toutes les quantités
différentielles de l'ordre $n$ peuvent être calculées grâce aux $n$-jets. Ce qui
nous intéresse ici, ce sont les informations reliées à la courbure contenues
dans les jets osculateurs.


Le principe est simple : (1) collecter les $N$ voisins du point $\vp$, (2)
résoudre l'ajustement polynomial, (3) collecter les quantités différentielles.
%
%
Pour la première étape, dans le cas d'un maillage, cela correspond à collecter
les $N$ points voisins à $\vp$ en partant du $1$-voisinage puis en augmentant
progressivement jusqu'à atteindre $N$ points.
% Dans le cas d'un nuage de points,
% il faut d'abord estimer la normale au point $\vp$ et les $N$ points voisins
% peuvent alors facilement être récupérés avec un diagramme de puissance sur le
% plan tangent estimé (nécessite cependant que l'échantillonnage soit assez
% dense).
%


%
Ensuite, il faut effectuer une analyse en composantes principales (\ACP) de
l'ensemble de points voisins afin d'estimer la normale au point $\vp$. Cela
nous permet de placer le point $\vp$ à l'origine du repère et de changer de
système de coordonnées pour celui de Monge (l'axe $\vec{z}$ est aligné avec la
normale, et les axes $\vec{x}$ et $\vec{z}$ sont alignés avec les directions
principales de courbure $\PrincDir{1}$ et $\PrincDir{2}$). Alors :
%
\begin{align} \label{eq:monge}
  f(x,y) = & \frac{1}{2}(\PrincCurv{1}x^2 + \PrincCurv{2}y^2)+ \frac{1}{6}(b_0x^3+3b_1x^2y+3b_2xy^2+b_3y^3) \\ & +\frac{1}{24}(c_0x^4+4c_1x^3y+6c_2x^2y^2+4c_3xy^3+c_4y^4) + O(||(x,y)||^{5}) \,.
\end{align}
%
où $\PrincCurv{1}$ et $\PrincCurv{2}$ sont les courbures principales, $b_0$ et
$b_3$ sont les dérivées directionnelles de $\PrincCurv{1}$ et $\PrincCurv{2}$ le
long de leur ligne de courbure respective, et $b_1$ et $b_2$ sont les dérivées
directionnelles de $\PrincCurv{1}$ et $\PrincCurv{2}$ le long de l'autre ligne de
courbure.
%


%
Enfin, en ayant recours à l'ajustement de polynômes, nous pouvons collecter
les $n$-jets de la surface dans ce système de coordonnées. Nous calculons alors
la base de Monge $(\PrincDir{1}, \PrincDir{2}, \NormalDir)$ afin d'en extraire
les coefficients $\PrincCurv{1}$, $\PrincCurv{2}$, $b_0$, $b_1$, $b_2$, $b_3$,
$c_0$, $c_1$, $c_2$, $c_3$ et $c_4$ de l'\RefEquation{eq:monge}.


% Une propriété utile de la forme de Monge
%
%  Ainsi, l'ajustement polynomial peut être calculé en
% supprimant la coordonnée dont l'angle est le plus proche de la normale. Calcul de
% la matrice de Vandermonde $N \times N_n$ :
% %
% \begin{equation}
%   M =  (1,x_i, y_i, x^2_i, \cdots, x_iy_i^{n-1}, y_i^n)_{i=1,\dotsc,N} \,,
% \end{equation}
% %
% À noter que cette matrice doit être mise à l'échelle avec la valeur moyenne des
% normes $||(x_i,y_i)||$. Ainsi, résoudre avec la décomposition en valeurs singulières
% (\anglais{Singular Value Decomposition}).
% %
% \todoInlineJeremy{pour récup la normale ?}
% %
% %\\
% %
% Enfin, calculer l'opérateur de forme (appelé également application de
% Weingarten) nous permet de retrouver des informations du second ordre puisqu'il
% est relié aux coefficients des formes fondamentales par les équations de
% Weingarten.
%
% \\
%
Les auteurs proposent des résultats de convergence en $O(\delta^2)$ lorsque la
donnée d'entrée est un échantillonnage de la surface, $\delta$ étant la densité
de points. Cependant, lorsque les données présentent du bruit, aucun résultat
théorique n'est apporté malgré le fait qu'en pratique l'approximation aux
moindres carrés des jets osculateurs soit très robuste.
%
Nous nommerons cet estimateur par la suite \JetFitting lors de notre analyse comparative.
%
\paragraph{Mesure de covariance des cellules de Voronoï}
%
Une autre famille de techniques exploite le diagramme de Voronoï
\cite{Alliez2007,Merigot2009,Merigot2011,Cuel2014DGCI} principalement pour
estimer les normales, mais peut également extraire des informations de
courbure. \comJeremy{L'idée est qu'au lieu de faire des
correspondances dans l'espace tangent à la surface, ils font l'estimation dans
l'espace orthogonal.} \cauthors{Mérigot}{Merigot2011} proposent alors d'associer
en chaque point $\vp$ une \emph{mesure de covariance des cellules de Voronoï}
$\mathcal{V}_{K,R}$ (\VCMM ou \VCM) pour une forme $K$. Cette mesure
contient des informations sur la géométrie sous-jacente (voir la
\RefFigure{fig:merigot-VCM-c}).
%

\begin{figure}[ht]{
    \begin{center}
    \includegraphics[height=4cm]{images/Feature/VCM}
    \end{center}}
    \caption[Principe de \VCM.]{Principe de \VCM (Figure~2 de \cite{Merigot2011}). En noir : séparation des cellules de Voronoï, en orange : les normales des points centrés dans les cellules de Voronoï.
      \label{fig:merigot-VCM-c}}
\end{figure}

Cela nécessite de calculer un matrice de covariance ($cov$) sur les cellules de
Voronoï (${Vor}_K$) des points $\vp$ du bord de l'objet :
%
\begin{equation}
    cov(K,\vp) \EqDef \int_K (\vx - \vp)(\vx - \vp)^T d\vx .
\end{equation}
%
Afin d'avoir une information locale de la variation de la forme, les auteurs se
limitent à un voisinage de rayon $R$ (voir la \RefFigure{fig:VCM-multiscale-c}) :
%
\begin{equation}
  \mathcal{V}_{K,R}(\{\vp\}) \EqDef cov({Vor}_K(\vp) \cap \Ball{R}{\vp}, \vp) .
\end{equation}

\begin{figure}[ht]{
    \begin{center}
    \includegraphics[height=6cm]{images/Feature/VCM_notations}
    \end{center}}
    \caption[Notations de \VCM.]{Notations de \VCM (Figure~5 de \cite{Merigot2011}). La figure de gauche est sur un nuage de points tandis que celle de droite est sur une forme continue. \label{fig:VCM-multiscale-c}}
\end{figure}

Et, afin d'être plus résistant au bruit de Hausdorff, ils utilisent une
version convoluée de \VCM grâce à une fonction indicatrice $\chi_r$
(ayant pour paramètre un rayon $r$) :
%
\begin{equation}
  \mathcal{V}_{K,R} \ast \chi_r(\{\vp\}) \EqDef \int_{K^R} (\vx - \pi^K(\vx))(\vx - \pi^K(\vx))^T \chi(\pi^K(\vx) - \vp) d\vx .
\end{equation}
%
où $\pi^K(\vx)$ est la projection de tout point $\vx$ vers le point le plus
proche de $K$. $\chi_r$ peut être une fonction « Ball » ($\chi_r(\vp) = 1$ si
$\vx$ appartient à $\Ball{r}{\mathbf{0}}$, $0$ sinon) ou encore une fonction «
Hat » ($\chi_r(\vp) = max(0, r - ||\vp||^2)$). D'autres choix sont bien
évidemment possibles.
%
Alors, si $\vp$ est un point lisse de $K$, les valeurs propres de
$\mathcal{V}_{K,R} \ast \chi_r$ sont proportionnelles à (respectivement) : $1$,
$\PrincCurv{1}^2(\vp)\frac{r^2}{4}$ et $\PrincCurv{2}^2(\vp)\frac{r^2}{4}$.
%


%
La mesure de covariance convoluée est particulièrement intéressante car cette
mesure est robuste même pour des ensembles compacts arbitraires, en
$O(\sqrt{\epsilon})$, car c'est la mesure intégrale de la matrice
de covariance du cône de normale autour du point d'intérêt. Néanmoins, la
convergence des courbures est dépendante de plusieurs paramètres $r$ et $R$ qui
contribuent de manière contradictoire avec l'erreur de Hausdorff. En pratique,
cette approche donne des résultats comparables à \JetFitting pour les courbures.

À noter que \cauthors{Cuel}{Cuel2014DGCI} ont défini une variante digitale du
\VCM ($\DigShape \in \Z^3$) :
%
\begin{equation}
  \hat{\mathcal{V}}_{\DigShape,h,R} \ast \chi_r(\{\vp\})) \EqDef \sum_{\vx \in \Omega_h^R} h^3 (\vx - \pi^{\DigShape}(\vx)) (\vx - \pi^{\DigShape}(\vx))^t \chi(\pi^{\DigShape}(\vx)) ,
\end{equation}
%
où $\Omega_h^R$ est l'ensemble des (centres de) voxels entièrement contenus dans
$\DigShape^R$ (la $R$-dilatation de $\DigShape$) au pas de discrétisation $h$.
La convergence asymptotique de cette variante digitale de l'estimateur \VCM pour
les normales a été prouvée (bord de la surface de classe $C^2$ et de reach $\rho >
0$) \cite{Cuel2014DGCI}.
%
\paragraph{Accumulateur sphérique}
%
Récemment, plusieurs auteurs ont développés de nouvelles approches intéressantes
pour estimer les champs de normales sur des nuages de points bruités, même en
présence de singularités \cite{Li2010, Boulch2012, Zhang2013}. Cela nécessite de
mettre en place un accumulateur auquel les données vont contribuer. La
transformée de Hough \cite{Hough1962} est traditionnellement employée, pour
détecter des formes paramétriques simples comme des lignes ou des arcs de cercle
\cite{Duda1972}. Cependant, elle peut être étendue en dimension 3 :
l'accumulateur n'est plus une image, mais un objet 3D.
\cauthors{Boulch}{Boulch2012} utilisent une boule accumulatrice (voir la
\RefFigure{fig:hough-ball}) de $M$ réceptacles (\anglais{bins}).

\begin{figure}[ht]{
    \begin{center}
    \includegraphics[height=6cm]{images/Curvature/Hough_ball}
    \end{center}}
    \caption[Boule accumulatrice.]{Boule accumulatrice (Figure~3 de \cite{Borrmann2011}). \label{fig:hough-ball}}
\end{figure}

Alors, les auteurs découpent le nuage de points en ensemble de plans (définis
par trois points) et normales. Si le plan est loin d'un coin ou d'une
singularité, alors la normale est correct. Cependant, si ce plan est près d'un
coin, la normale est perturbée par cette singularité (voir la
\RefFigure{fig:Hough-bad} à gauche).

\begin{figure}[ht]{
    \begin{center}
    \includegraphics[width=15cm]{images/Curvature/Hough_bad}
    \end{center}}
    \caption[Erreur de reconstruction de normales et correction.]{Erreur de reconstruction de normales et correction (Figure~1 de \cite{Boulch2012}). \label{fig:Hough-bad}}
\end{figure}

Les normales sont accumulés dans la boule accumulatrice, permettant à chaque
plan de contribuer à un vote. Ils utilisent également la transformée aléatoire
de Hough. La différence avec la transformée de Hough traditionnelle est qu'un
point ne vote pas pour toutes les primitives auxquels il appartient. Le vote est
associé à une primitive calculée à partir d'un sous-ensemble de points
\cite{Xu1990, Xu1993} dont le nombre est défini par l'utilisateur comme
condition globale. Enfin, les normales sont redistribuées aux plans en fonction
des bins les plus votés, en moyennant les normales dans ces bins (voir la
\RefFigure{fig:Hough-bad} à droite).


\comJeremy{Cette méthode possède cependant des inconvénients : nous obtenons un
effet de regroupement par bin des résultats, de segmentation des données.} Cet
effet est surtout visible avec des données bruitées. Ensuite, les bins utilisés
sont anisotropes. Les auteurs rectifient cela en proposant de calculer
l’algorithme plusieurs fois en tournant aléatoirement la boule accumulatrice
pour limiter cet effet. Expérimentalement, ils avancent que cet effet est
estompé au bout de $5$ rotations.


\nauthors{Boulch} apportent des résultats de convergence probabilistes.
Cependant, ces méthodes ne peuvent pas être utilisées directement pour le calcul
de courbure, mais en parallèle de techniques d'estimation de courbure pour
détecter les singularités sur la surface dans un premier temps afin de limiter
l'estimation de la courbure uniquement aux zones lisses.
%
\paragraph{Théorie des varifolds}
%
Enfin, une approche très récente venant de la théorie de la mesure géométrique
utilise la théorie des \anglais{Varifolds} \cite{Almgren1965} pour concevoir une
estimation robuste de la courbure moyenne \cite{Buet2014,Buet2015}. Cette
théorie est suffisamment générique pour fonctionner sur des variétés lisses, des
maillages discrets, des nuages de points ainsi que sur des données digitales.
Afin de mener à bien l'estimation géométrique, cette approche nécessite d'avoir
une position et une approximation des normales. Si les deux prérequis
convergent, alors la première variation régularisée de la mesure de Varifold
converge vers la courbure moyenne. La vitesse de convergence de cette approche
ainsi que sa précision en pratique restent à explorer.
%
\subsection{Estimation de courbure sur des surfaces digitales}
%
%BC [Esbelin2011], MDSS [Coeurjolly2001;deVieilleville2007] et MDCA [Roussillon2011].
Comme vu dans le \RefChapitre{sec:notions}, en géométrie digitale nous
considérons la convergence asymptotique comme un critère essentiel pour un
estimateur \cite{Klette2000}. Lorsque la forme est discrétisée
sur une grille avec une résolution $h$ tendant vers zéro, la quantité estimée
devrait converger vers la valeur attendue.


\cauthors{Coeurjolly}{Coeurjolly_ChapEstimateur} ont proposé un état de l'art
complet pour l'estimation de tangentes et de la courbure en dimension 2. Nous
allons détailler les méthodes les plus représentatives.
%
\paragraph{Reconnaissance d'objets digitaux}
%
Nous allons nous intéresser dans un premier temps aux méthodes reconnaissant des
objets digitaux sur le bord de l'objet, afin d'en extraire des informations
différentielles.


\noindent\textbf{Segments digitaux maximaux.\quad}
% The curvature k is defined from the length l and the width w of a DSS as follow: $1/k=(l \ast l)/(8 \ast w)+w/2$
% Most centered Maximal Segment
\cauthors{Coeurjolly}{Coeurjolly2001} ont montré une façon simple d'estimer
la courbure à la surface d'un objet digital à l'aide des segments maximaux.
Cette méthode, que nous appellerons \MDSS par la suite, propose de calculer la
longueur des segments maximaux du bord $\Boundary \DigShape$ de $\DigShape
\subset \Z^2$. En effet, il existe une relation entre la corde d'un cercle et
le rayon de ce cercle (voir \RefFigure{fig:mdss-chord}).

\begin{figure}[ht]{\small
    \begin{center}
      {\begin{overpic}[width=10cm]{figures/MDSS1}
          \put(-5,-4){$R + \frac{1}{2}$}
          \put(10,-4){$R - \frac{1}{2}$}
          \put(90,-4){$R$}
      \end{overpic}}
    \end{center}}
    \caption[Relation entre la corde d'un cercle et son rayon.]{Illustration d'un cercle d'épaisseur 1 et de la corde de ce cercle (Figure~3 de \cite{Coeurjolly2001}). \label{fig:mdss-chord}}
\end{figure}

L'idée ici est de considérer le segment maximal comme une corde à une distance
$h$ (la taille de la grille) du cercle osculateur de la surface.
Alors, si $l$ est la longueur du MDSS au point $\vp \in \Z^2$, nous avons :
%
\begin{equation}
  %
    \frac{1}{\Curv(\vp,\DigShape)} = \frac{(l \cdot l)}{8} + \frac{1}{2} \,.
  %
\end{equation}
%
Une version plus robuste est proposée en ajoutant l'épaisseur du MDSS $w$ :
%
\begin{equation}
  %
    \frac{1}{\Curv(\vp,\DigShape)} = \frac{(l \cdot l)}{8 \cdot w} + \frac{w}{2} \,.
  %
\end{equation}
%
Cependant, aucune preuve de convergence n'est apportée pour cet estimateur.
% => ça c'est pour les demi tangentes, erreur dans le bouquin ?
% La méthode repose sur l'hypothèse que les segments maximaux
% d'un cercle euclidien se comportent comme des cordes de hauteur $h$ (le pas de
% discrétisation) et de longueur $\Theta(h^{-\frac{1}{2}})$. Cependant, quelques
% années plus tard, \cauthors{de Vieilleville}{deVieilleville2007} remarquent que
% bien que la borne supérieure du plus long MDSS est en $O(h^{-\frac{1}{2}})$, leur
% longueur en moyenne est proche de $\Theta(h^{-\frac{1}{3}})$ (entre
% $\Theta(h^{-\frac{1}{3}})$ et $\Theta(h^{-\frac{1}{3}} \log
% \left(\frac{1}{h}\right))$, voir le \RefLemma{lem:law-length-MDSS}). Cette borne
% supérieure a pour conséquence de réfuter la preuve de convergence avancée dans
% \cite{Coeurjolly2001}.
%
%
% \textbf{Demi-tangentes digitales}\\
% %
% \cauthors{de Vieilleville}{deVieilleville2007} s'intéressent à la reconnaissance
% des segments maximaux digitaux que nous avons défini dans le chapitre précédent
% (voir \RefSection{sec:segments}). Ainsi, le principe est simple : pour une forme
% $\Shape \subset \R^2$, pour chaque point $\vp$ du bord digital $\BdZ{\DigShape}$
% avec $\DigShape = \DSh$, ils récupèrent le segment digital maximal (\MDSS) dont
% son centre est le plus proche du point $\vp$. Alors, en s'appuyant sur des
% travaux de \cauthors{Coeurjolly}{Coeurjolly2001}, en calculant l'inverse du
% rayon du cercle circonscrit contenant $\vp$ et les deux extrémités du \MDSS le
% plus proche que nous aurons mis à l'échelle avec le pas de discrétisation $h$,
% nous obtenons une estimation de la courbure.
% %
% \\
% %
% Cependant, les auteurs annoncent ne pas pouvoir prouver la convergence
% asymptotique de cet estimateur de courbure à cause d'une propriété sur les
% demi-tangentes sur le bord digital qui ne peut pas être vérifiée, la convergence
% de l'estimateur reposant sur celle-ci.
% %


\noindent\textbf{Analyse des arcs de cercle digitaux maximaux.\quad}
% Most centered Digital Circular Arc
\cauthors{Roussillon}{Roussillon2011} proposent non plus d'étudier les segments
maximaux digitaux, mais de reconnaître des arcs de cercle maximaux digitaux
(\MDCA, voir \RefDefinition{def:digital-circular-arc} pour la définition
formelle). La courbure d'une courbe étant définie comme l'inverse du rayon du
cercle osculateur, il paraît logique de s'intéresser à reconnaître des arcs de
cercle digitaux sur la forme digitale.

\begin{figure}[ht]{
    \begin{center}
    \includegraphics[height=6cm]{images/Notions/MDCA}
    \end{center}}
    \caption[Illustration des arcs de cercle digitaux maximaux sur une ellipse.]
    {Illustration des arcs de cercle digitaux maximaux sur une ellipse discrétisée (Figure~1
    de \cite{Roussillon2011}).\label{fig:mdca-curv-figure}}
\end{figure}

Le principe de l'estimateur est simple : pour une forme $\Shape \subset \R^2$
avec un champ de courbure continu, ils calculent les $(A_i)_{i \in 1,n}$ \MDCA
($n$ étant le nombre total de \MDCA) du bord digital $\BdZ{\DigShape}$ avec
$\DigShape = \DSh$. Ils calculent également les $(V_i)_{i \in 1,n}$, une
partition du bord digital permettant de faire correspondre tous les surfels vers
le \MDCA dont le centre est le plus proche du surfel. Chaque surfel est alors relié à son \MDCA le plus
représentatif (voir \RefFigure{fig:mdca-curv-figure}, les surfels bleus sont
reliés au \MDCA bleu). Alors :
%
\begin{equation}
  %
    \forall i \in 1,\dotsc,n, \quad \forall \ve \in V_i, \quad \forall \vp \in \ve,
    \quad \CurvH{}_{MDCA}(\DigShape,\vp,h) \EqDef \Curv(A_i,h) \,.
  %
\end{equation}
%
Les auteurs ont prouvé la convergence asymptotique en $O(h^{\min(1-2a,b)})$ de
cet estimateur sur des formes convexes de $\R^2$ dont le champ de courbure est
continu, strictement positif et borné supérieurement, si $0 < b \le a <
\frac{1}{2}$ correspondent aux bornes inférieures et supérieures des longueurs des
\MDCA (\respp $\Omega(h^a)$ et $O(h^b)$). Cette dernière hypothèse n'est pas
prouvée à ce jour.


Le principal avantage de ces deux méthodes est qu'elles ne nécessitent aucun
paramètre. Cependant, ces méthodes de reconnaissance d'objets sont assez
sensibles lorsque les données sont bruitées. C'est ce que nous verrons dans la
section comparative (\RefSection{sec:comparaison-courbure}).
%
\paragraph{Convolutions binomiales}
%
\cauthors{Malgouyres}{Malgouyres2008} ont proposé une méthode pour estimer les
dérivées sur le principe de convolutions binomiales (nous la nommerons \BC par
la suite). Ils définissent un opérateur $\Psi_K$ qui modifie la fonction $F : \Z
\rightarrow \Z$ par la convolution avec un noyau $K : \Z \rightarrow \Z$. Par
exemple, l'opérateur de différence finie arrière est $\Psi_\delta F$ où le noyau
$\delta$ est défini comme :
%
\begin{equation}
  %
    \delta(a) =
      \begin{cases}
        1   & \text{si } a = 0,\\
        -1  & \text{si } a = 1,\\
        0   & \text{sinon}.\\
      \end{cases}
  %
\end{equation}
%
Les auteurs donnent également un noyau de lissage défini (pour $n \in \N$) par :
%
\begin{equation}
  %
  H_n(a) =
    \begin{cases}
      \left(\!
        \begin{array}{c}
          n \\
          a + \frac{n}{2}
        \end{array}
        \!\right)   & \text{si } n \text{ est pair et } a \in \{ -\frac{n}{2} , \cdots, \frac{n}{2} \},\\
        \left(\!
          \begin{array}{c}
            n \\
            a + \frac{n + 1}{2}
          \end{array}
          \!\right)   & \text{si } n \text{ est impair et } a \in \{ -\frac{n + 1}{2}, \cdots, \frac{n - 1}{2} \},\\
      \quad\quad 0   & \text{sinon}.\\
    \end{cases}
  %
\end{equation}
%
Alors, le noyau de dérivées $D_n$ est défini par :
%
\begin{equation}
  D_n \EqDef \delta \ast H_n \,,
\end{equation}
%
où $\ast$ est le produit de convolution. L'estimateur de dérivé pour une fonction discrète $F$ est :
%
\begin{equation}
  \frac{1}{2^n}\Psi_{D_n}F \,.
\end{equation}
%
Les auteurs proposent également des dérivées d'ordre supérieur avec une
expression similaire :
%
\begin{align}
  D_n^2 &= \delta \ast \delta \ast D_n \\
  D_n^k &= \underbrace{\delta \ast \cdots \ast \delta}_{k \text{ fois}} \ast D_n \,.
\end{align}
%
La courbure est alors estimée de la sorte :
%
\begin{equation}
  %
  \CurvH{n}_{BC}(\vp) \EqDef \frac{D_n^2(\vp_1) \ast D_n(\vp_2) - D_n^2(\vp_2) \ast D_n(\vp_1)}{(D_n(\vp_1)^2 + D_n(\vp_2)^2)^\frac{3}{2}} \,.
  %
\end{equation}


Dans le cas où $n = \lfloor h^{2(\alpha -3)/3} \rfloor$ et $\alpha \in ]0,1]$ est un paramètre
associé au niveau de bruit (plus $\alpha$ est proche de $0$, plus l'estimateur
sera robuste au bruit), \cauthors{Esbelin}{Esbelin2011} annoncent une convergence asymptotique en
$O(h^{{(\frac{2}{3})}^k})$ des dérivées du $k$-ième ordre pour des formes $C^3$ (donc en
$O(h^{4/9})$ pour les dérivées du second ordre). Nous verrons dans la partie
comparative que nous n'observons expérimentalement pas la même vitesse de
convergence (\RefSection{sec:comparaison-courbure}).
%
\paragraph{Correspondance de surface polynomiale}
%
\cauthors{Provot}{Provot2011} proposent quant à eux de s'intéresser aux
\anglais{Digital Level Layer} (\DLL) pour estimer les dérivées d'une fonction
digitale à l'aide de la programmation linéaire ou avec des outils de la
\AlgorithmicGeometry. Cette approche permet d'approcher les $k$ dérivées de
$f^{(k)}(\vx)$. Ainsi, à l'instar de \JetFitting, ils proposent une nouvelle
méthode basée sur les polynômes de Taylor.
%
Le principe de cette méthode est de faire ajuster un polynôme sur les valeurs de
la fonction digitale. Ils étendent alors les valeurs de la fonction $F(\vx)$
dans l'intervalle $[F(\vx) - R; F(\vx) - R]$. Il suffit alors de trouver
un polynôme $P(\vx)$ respectant :
%
\begin{equation}
    F(\vx) - R \le P(\vx) \le F(\vx) + R \,,
\end{equation}
%
dans le voisinage de $\vx_0$, où $P(\vx) = \sum_{i=1}^{k} a_i\Shape^i$, $a_i$
étant les coefficients entiers du polynôme $P(\vx)$. Ils introduisent alors la notion de
rugosité (\anglais{roughness}) de la fonction, \cad le $R$ minimum nécessaire
pour trouver un polynôme s'ajustant sur les données de la fonction $F$ avec une
erreur inférieure à $R$ (voir la \RefFigure{fig:dll-roughness}).

\begin{figure}[ht]{
    \begin{center}
    \includegraphics[width=14cm]{images/Curvature/DLL_roughness}
    \end{center}}
    %
    \caption[Illustration du paramètre $R$ de rugosité de \DLL.]{\emph{De gauche à
    droite : } Fonction $F$, $F$ étendu avec un intervalle le plus petit
    possible pour lequel il existe un polynôme de degré 2 ou plus qui passe au
    travers de chaque point étendu (Figure~1 de
    \cite{Provot2011}).\label{fig:dll-roughness}}
    %
\end{figure}

Ils montrent que nous pouvons calculer la rugosité comme l'épaisseur
verticale (\anglais{vertical thickness}) d'un ensemble $S = \{((\vx^i)_{1 \le i
\le k} \; F(\vx)) \text{ pour } \vx \in \Shape \}$ de dimension $k + 1$ (voir la
\RefFigure{fig:dll-roughness2}). Ils utilisent alors l'algorithme de détection
de collision Gilbert-Johnson-Keerthi (GJK) \cite{Gilbert1988} pour calculer
rapidement l'épaisseur verticale de l'ensemble $S$ (l'épaisseur de la séparation
entre deux hyperplans).

\begin{figure}[ht]{
    \begin{center}
    \includegraphics[width=14cm]{images/Curvature/DLL_roughness2}
    \end{center}}
    %
    \caption[Illustration de la notion d'épaisseur du \DLL.]{Illustration de la
    notion d'épaisseur du \DLL. Les trois bandes s'ajustent sur les points de
    $S$ (en prenant en compte une direction $\vec{d}$, vertical dans le cas présent).
    La bande bleue possède alors l'épaisseur minimale de toutes les bandes
    (Figure~10 de \cite{ProvotIPOL2014}).\label{fig:dll-roughness2}}
    %
\end{figure}

Enfin, ils définissent la $k$-ième dérivée de $F$ à l'origine comme :
%
\begin{equation}
    F^{(k)}(\mathbf{0}) \EqDef k!a_k \,,
\end{equation}
%
si $P(\vx)$ respecte les contraintes de rugosité de ses bornes.

Ils obtiennent alors une preuve de convergence en $O(h^{\frac{1}{k+1}})$ sur
l'estimation des dérivées du $k$-ième ordre pour une fonction réelle $f : \R
\rightarrow \R$ de classe $C^{k+1}$ où les dérivées d'ordre $k+1$ sont bornées
dans le voisinage du point $\vx$. L'inconvénient de cette méthode est qu'elle
dépend énormément du paramètre de rugosité maximale. De plus, cette approche
n'est applicable qu'à des fonctions discrètes, et pas directement à des bords
d'objets discrets.
% ils ne fournissent
%pas de moyen d'extraire la courbure de ces dérivées.
%
%
% Un \DLL est un sous-ensemble de points
% $\vp$ de $\Z^d$ vérifiant la double inégalité suivante :
% %
% \begin{equation}
%     -\delta \le F(\vp) \le \delta
% \end{equation}
% %
% où $F : \Shape \rightarrow \R$ avec $\Shape \subset \R$. L'idée derrière cette
% méthode est de rajouter une contrainte d'épaisseur (ce qu'ils appellent le
% paramètre de rugosité, voir \RefFigure{fig:dll-roughness, fig:dll-roughness2}) au lieu de faire de l'ajustement de polynôme avec de l'erreur aux moindres carrés comme \JetFitting.
% %
% \begin{figure}[ht]{
%     \begin{center}
%     \includegraphics[width=14cm]{images/Curvature/DLL_roughness}
%     \end{center}}
%     %
%     \caption[Illustration des arcs de cercle digitaux maximaux sur une
%     ellipse.]{\emph{De gauche à droite : } Fonction $F$, $F$ étendu avec un
%     intervalle le plus petit possible pour lequel il existe un polynome de degré
%     1 ou plus qui passe  au travers de chaque point étendu (Figure~1 de
%     \cite{Provot2011}).\label{fig:dll-roughness}}
%     %
% \end{figure}
 %
% De plus, cela permet de considérer la forme digitale directement, et ainsi de
% reconnaitre des structures digitales grâce à l'algorithme de détection de
% collision Gilbert-Johnson-Keerthi (GJK) \cite{Gilbert1988}. Ainsi, il permet de décomposer la surface en DSS, DCA et coniques.
% %
% \\
% %
% % La première question que nous pouvons nous poser, c'est pourquoi définir une
% % nouvelle primitive digitale correspondant à des courbures algébriques ou à des
% % surfaces euclidiennes ? Les auteurs justifient ce choix par le fait que les
% % autres primitives digitales ont de chacunes des avantages et des inconvénients,
% % et qu'il est difficile de déterminer laquelle des approches est la plus adaptée.
% % Par exemple les primitives basées sur l'approche analytique peuvent perdre des
% % informations topologiques de la surface. Les primitives basées sur l'approche
% % topologique ou morphologique quant à elles ne possèdent pas de caractérisation
% % analytique. Les \DLL proposent une façon unifiée et efficace pour résoudre
% % toutes ces contraintes.
%
% \begin{figure}[ht]{
%     \begin{center}
%     \includegraphics[width=14cm]{images/Curvature/DLL_roughness2}
%     \end{center}}
%     %
%     \caption[Illustration de la notion d'épaisseur du \DLL.]{Illustration de la
%     notion d'épaisseur du \DLL. Les trois bandes s'ajustent sur les points de
%     $S$ (en prenant en compte une direction $\vec{d}$, vertical dans le cas présent).
%     La bande bleue possède alors l'épaisseur minimale de toutes les bandes
%     (Figure~10 de \cite{ProvotIPOL2014}).\label{fig:dll-roughness2}}
%     %
% \end{figure}

% Ils obtiennent alors une preuve de convergence en $O(h^{\frac{1}{k+1}})$ sur
% l'estimation des dérivées du $k$-ième ordre pour une fonction réelle $f : \R
% \rightarrow \R$ de classe $C^{k+1}$ où les dérivées d'ordre $k+1$ sont bornées
% dans le voisinnage du point $\vx$.
%Alors l'estimation de la courbure converge
%asymptotiquement en $O(h^\frac{1}{3})$
% \begin{table}[]
% \centering
% \caption{My caption}
% \label{my-label}
% \begin{tabular}{@{}lp{0.15cm}p{0.15cm}p{0.15cm}p{0.15cm}p{0.15cm}p{4cm}p{2cm}r@{}}
% \hline
% Estimateur & \multicolumn{5}{c}{Courbure} & Famille de formes & Vitesse de convergence & Référence \\ \cline{2-6}
% & $\Curv$ & $\MeanCurv$ & $\GaussCurv$ & $\PrincCurv{i}$ & $\NormalDir$ &  &  & \\ \hline
%
% Gauss-Bonnet & \svgNope & \svgNope & \svgYes & \svgNope & \svgNope & Surface paramétrique lisse de valence $6$ & $O(\delta^\alpha)$ avec $\alpha \ge 3$ & \cite{Xu2006} \\
%
% Cycles normaux & \svgNope & \svgYes & \svgYes & \svgNope & \svgNope & Surface lisse $\epsilon lfs$ & $O(\epsilon)$ & \cite{CohenSteiner2003} \\
%
%
% \MDCA     & \svgYes & \svgNope & \svgNope & \svgNope & \svgNope & ??? & \svgNope & \cite{Coeurjolly2001} \\
% \II       & \svgYes & \svgYes & \svgYes & \svgYes & \svgYes & $\Shapes^{3-SC}$ avec $R = kh^\frac{1}{3}$ & $O(h^\frac{1}{3})$ & \cite{CVIU2014} \\
% \hline
% \end{tabular}
% \end{table}

\paragraph{Tableau récapitulatif de convergence d'estimateurs digitaux de courbure}

\begin{table}[ht]
\centering
\caption{Convergence asymptotique connue d'estimateurs de courbure sur des données digitales.}
\label{tab:curv-comp}
\begin{tabular}{@{}p{1.9cm}lllr@{}}
\toprule
 & & \multicolumn{2}{c}{Vitesse de convergence} &            \\ \cmidrule(r){3-4}
Estimateur & Famille de formes & Borne sup. & Observée & Référence \\ \midrule

$\hat{\Curv}^{MDCA}$ & $\Shapes^{1-SC}$ & ? & $O(h^\frac{1}{3})$ & \cite{Roussillon2011} \\
$\hat{\Curv}^{BC}$ & $\Shapes^{3-SC}$ & $O(h^\frac{4}{9})$ & $\approx O(h^{0.154})$ & \cite{Malgouyres2008} \\
$\hat{\Curv}^{MDSS}$ & $\Shapes^{1-SC}$ & ? & \svgNope & \cite{Coeurjolly2001} \\
$\hat{\Curv}^{II}$ & $\Shapes^{3-SC}$ avec $R = kh^\frac{1}{3}$ & $O(h^\frac{1}{3})$ & $O(h^\frac{1}{3})$ & ici \\
$\hat{\Curv}^{*II}$ & $\Shapes^{3-SC}$ & $O\left(h^\frac{1}{3} \log^2 \left(\frac{1}{3}\right)\right)$ & $O(h^\frac{1}{3})$ & ici \\
\midrule
% $\hat{\MeanCurv}^{JetFitting}$ &  & ? & $O(h^\frac{1}{3})$ & \cite{Cazals2005} \\
$\hat{\MeanCurv}^{II}$ & $\Shapes^{3-SC}$ avec $R = kh^\frac{1}{3}$ & $O(h^\frac{1}{3})$ & $O(h^\frac{1}{3})$ & ici \\
$\hat{\MeanCurv}^{*II}$ & $\Shapes^{3-SC}$ & ? & $O(h^\frac{1}{3})$ & ici \\
\midrule
% $\hat{\PrincCurv{i}}^{JetFitting}$ &  & ? & $O(h^\frac{1}{3})$ & \cite{Cazals2005} \\
$\hat{\PrincCurv{i}}^{II}$ & $\Shapes^{3-SC}$ avec $R = kh^\frac{1}{3}$ & $O(h^\frac{1}{3})$ & $O(h^\frac{1}{3})$ & ici \\
$\hat{\PrincCurv{i}}^{*II}$ & $\Shapes^{3-SC}$ & ? & $O(h^\frac{1}{3})$ & ici \\

\bottomrule
\end{tabular}
\end{table}


Ainsi, nous avons quelques estimateurs qui convergent asymptotiquement en
dimension 2 (voir \RefTable{tab:curv-comp}), mais qui peuvent être sensibles au
bruit de la forme ou dont la preuve de convergence repose sur les paramètres
choisis. De plus pour l'estimation de courbure le long de courbes 2D, la
convergence asymptotique d'estimateurs ne nécessitant aucun paramètre représente
toujours un défi (des résultats de convergence d'estimateurs sans paramètre ont
été obtenus pour l'estimation de longueur \cite{Coeurjolly2004} et des normales
\cite{deVieilleville2007}). Nous allons en proposer dans ce chapitre. Enfin, il
n'existe aucun estimateur de courbure en dimension 3 \comJeremy{dont la
convergence asymptotique soit établie
théoriquement~\cite{Lenoir1997,Fourey2008}. Nous allons proposer un nouvel
estimateur de courbure 3D dans ce chapitre et établir des résultats de
convergence asymptotique.}

%% FIXME
%% content/chapter-estimators.tex:971: Package varioref Error: \vref at page boundary 45-46 (may loop).
%% content/chapter-estimators.tex:971: leading text: dans le \RefSection{sec:pottmann-principle}
%% --------------

\section{Estimation de courbure par intégration volumique locale d'une boule}
\label{sec:estimators:volume}
%
Nous allons désormais nous intéresser aux travaux réalisés au cours de cette
thèse. Nous allons dans un premier temps décrire l'estimation de quantités
différentielles que proposent \cauthors{Pottmann}{Pottmann2007,Pottmann2009},
dans le \RefSection{sec:pottmann-principle}. Nous allons ensuite proposer une
version digitale de ces estimateurs dans les \RefSections{sec:ii-2d}{sec:ii-3d}
en dimension 2 et 3. La principale contribution de ces travaux est de démontrer
la convergence asymptotique de ces estimateurs sur des données digitales. Ces
travaux ont été publiés dans \cite{DGCI2013,CVIU2014}.

En \GeometryProcessing, le calcul d'invariants par intégration a été largement
étudié dans le but de définir des estimateurs de quantités différentielles.
\cauthors{Pottmann}{Pottmann2007,Pottmann2009} en ont d'ailleurs fait un
panorama assez complet. L'idée principale de cette approche est de déplacer un support
sur les points $\vx$ du bord $\dS$ d'une forme $\Shape$ et de calculer
l'intégrale de l'intersection entre $\Shape$ et ce support. Différents supports
peuvent être considérés (sphère euclidienne, boule euclidienne, etc.) ainsi que
différentes fonctions d'intégration (linéaire, gaussienne, etc.). Nous
n'utiliserons par la suite que les invariants volumiques par intégration définis
de la sorte :
%
\begin{definition}
  \label{def:Volume}
  %
  Pour une forme $\Shape \in \Shapes$ et un rayon $R \in \R^{+*}$, \colorize{l'intégrale
  volumique} $V_R(\vx)$ au point $\vx \in \dS$ est donné par :
  %
  \begin{equation}
    V_R(\vx) \EqDef \int_{\Ball{R}{\vx}} \chi(\vp)d\vp\,,
  \end{equation}
  %
  où $\Ball{R}{\vx}$ est une boule euclidienne de rayon $R$ et de centre $\vx$,
  et $\chi(\cdot)$ est la fonction caractéristique de $\Shape$ (valant $1$ pour
  tous les points de $\Shape$, $0$ sinon). En dimension 2, nous noterons cette
  quantité $A_R(\vx)$.
  %
\end{definition}
%
\subsection{Analyse euclidienne de l'intégration volumique de la boule et relation à la courbure}
\label{sec:pottmann-principle}
%
La relation entre $A_R(\vx)$ et la courbure (\resp $V_R(\vx)$ et la courbure
moyenne) au point $\vx$ pour des formes de $\R^2$ (\respp $\R^3$) a été étudiée
de nombreuses fois dans la littérature
\cite{Bullard1995,Pottmann2007,Pottmann2009}. Nous pouvons la formaliser ainsi :
%
\begin{lemma}{\textbf{\cite{Pottmann2009}}}\\
\label{lem:pottmann-2d}
  %
  Pour une forme suffisamment lisse $\Shape$ de $\R^2$, $\vx \in \dS$, nous
  avons :
  %
  \begin{equation}
    \label{eq:volume2d}
    A_R(\vx) = \frac{\pi}{2} R^2 - \frac{\Curv(\Shape,\vx)}{3}R^3 + O(R^4)\,.
  \end{equation}
  %
  où $\Curv(\Shape,\vx)$ est la courbure de $\dS$ au point $\vx$.
  %
  \\
  %
  Pour une forme suffisamment lisse $\Shape$ de $\R^3$, $\vx \in \dS$, nous
  avons :
  %
  \begin{equation}
    \label{eq:volume3d}
    V_R(\vx) = \frac{2\pi}{3} R^3 - \frac{\pi \MeanCurv(\Shape,\vx)}{4}R^4 + O(R^5)\,.
  \end{equation}
  %
  où $\MeanCurv(\Shape,\vx)$ est la courbure moyenne de $\dS$ au point $\vx$.
  %
\end{lemma}
%
De tels résultats sont obtenus par approximation de Taylor au point $\vx$ de la
surface $\dS$, approchée sous la forme de la fonction paramétrique $z=f(x,y)$ ($y=f(x)$ en
2D). Alors, des \RefEquations{eq:volume2d}{eq:volume3d} avec un
rayon $R$ fixé, nous pouvons définir les estimateurs locaux de courbure en 2D,
$\CurvT{R}(\vx)$, et de courbure moyenne en 3D, $\MeanCurvT{R}(\vx)$ :
%
\begin{definition}{\fakeTitle{Estimateurs de courbure en 2D $\CurvT{R}(\Shape,\vx)$ et de courbure moyenne en 3D $\MeanCurvT{R}(\Shape,\vx)$ \cite{Pottmann2007}}}
  \label{def:pottmann-2d-3d-mean}
  %
  Pour une forme $\Shape \subset \R^2$, l'estimateur de courbure en 2D $\CurvT{R}$
  au point $\vx \in \dS$ est défini par :
  %
  \begin{equation}
    \label{eq:pottmann-2d}
    \CurvT{R}(\Shape,\vx) \EqDef \frac{3\pi}{2 R} - \frac{3 A_R(\vx)}{R^3} .
  \end{equation}
  %
  \\
  %
  Pour une forme $\Shape \subset \R^3$, l'estimateur de courbure moyenne en 3D
  $\MeanCurvT{R}$ au point $\vx \in \dS$ est défini par :
  %
  \begin{equation}
    \label{eq:pottmann-3d-mean}
    \MeanCurvT{R}(\Shape,\vx) \EqDef \frac{8}{3 R} - \frac{4 V_R(\vx)}{\pi R^4}.
  \end{equation}
  %
\end{definition}
%
De plus, lorsque le rayon $R$ de la boule tend vers zéro, les valeurs de ces
deux estimateurs vont converger vers celles attendues. Plus formellement,
\cauthors{Pottmann}{Pottmann2007} (et les travaux précédents) ont démontré la
convergence des estimateurs de courbure en 2D et de courbure moyenne en 3D :
%
\begin{theorem}{\fakeTitle{Convergence des estimateurs de courbure en 2D $\CurvT{R}(\Shape,\vx)$ et de courbure moyenne en 3D $\MeanCurvT{R}(\Shape,\vx)$ \cite{Pottmann2007}}}
  \label{theo:pottmann-2d-3d-mean-conv}
  %
  \begin{equation}
    \CurvT{R}(\Shape,\vx) = \Curv(\Shape,\vx) + O(R),
    \quad\MeanCurvT{R}(\Shape,\vx) = \MeanCurv(\Shape,\vx) + O(R).
  \end{equation}
  %
\end{theorem}
%
De la même manière, des informations directionnelles comme les courbures
principales (et donc la courbure gaussienne) peuvent être extraites du calcul
par intégration. En effet, à la place de calculer la mesure de $\Ball{R}{\vx}$
dans la \RefDefinition{def:pottmann-2d-3d-mean}, nous allons calculer sa matrice
de covariance :
%
\begin{definition}
  \label{def:cov-matrix}
  Pour un ensemble non vide $Y \subset \R^d$, la matrice de covariance de $Y$
  est définie par :
  \begin{equation}
    \Cov(Y) \EqDef \int_Y (\vp-\overline{Y})(\vp-\overline{Y})^T d\vp = \int_Y \vp \vp^T d\vp - \Vol(Y)\overline{Y}\overline{Y}^T,
  \end{equation}
  où $\overline{Y}$ est le barycentre de $Y$ et $\Vol(Y)$ son volume.
\end{definition}
%
Pour les entiers $p, q, s \ge 0$, nous pouvons écrire la définition des $
pqs$-moments $\Mom{pqs}(Y)$ de $Y$ ($x_i$ est la $i$-ème composante de $\vx$) :
%
\begin{equation}
  \label{eq:moments}
  \Mom{pqs}(Y) \EqDef \iiint_Y x_1^p x_2^q x_3^s dx_1 dx_2 dx_3\,.
\end{equation}
%
Il apparaît alors clairement que le volume $\Vol(Y)$ est le $0$-moment
$\Mom{000}(Y)$, et que le barycentre $\overline{Y}$ est donné par les $1$-moments
normalisés par le $0$-moment, c'est à dire :
%
\begin{equation}
  \overline{Y} \EqDef \frac{( \Mom{100}(Y), \Mom{010}(Y), \Mom{001}(Y) )^T}{\Mom{000}(Y)}
\end{equation}

Nous allons maintenant définir les ordres de grandeur des moments. Ces résultats nous seront
utile par la suite.
%
\begin{lemma}\label{lem:moment-ball}
  %
  Soit $\Ball{R}{\vy}$ une boule de rayon $R$ et de centre $\vy$. Alors, pour un
  ensemble non vide $Y \subset \Ball{R}{\vy}$, nous avons :
  %
  \begin{align}
    \Mom{000}(Y) =& O(R^3), \label{eq:ball-moment-0}\\
    \Mom{100}(Y) =& O(R^3(\|\vy\|_\infty + R)), \label{eq:ball-moment-1}\\
    \Mom{200}(Y) =& O(R^3(\|\vy\|^2_\infty + R\|\vy\|_\infty + R^2)), \label{eq:ball-moment-2}\\
    \Mom{100}(Y)/\Mom{000}(Y) =& O(\|\vy\|_\infty + R), \label{eq:ball-moment-1bis}
  \end{align}
  %
  Les autres moments du même ordre ont respectivement les mêmes bornes.
  %
\end{lemma}
\begin{proof}
  %
  \noindent\textbf{Moment d'ordre zéro.\quad}
  %
  L'\RefEquation{eq:ball-moment-0} peut être facilement démontrée puisque le
  moment d'ordre zéro est le volume de $Y$, de ce fait ne peut pas dépasser le
  volume de la boule qui est $\frac{4}{3}\pi R^3$.


  \noindent\textbf{Moments du premier ordre.\quad}
  %
  Nous changeons les variables de $(x_1',x_2',x_3')=(x_1,x_2,x_3)-\vy$ dans
  l'expression suivante :
  %
  \begin{align}
    \Mom{100}(Y) &= \iiint_Y x_1 dx_1 dx_2 dx_3 = \iiint_{Y-\vy} (x_1'+y_1) dx_1'dx_2'dx_3' \\
                 &= y_1 \Vol(Y) + \Mom{100}(Y-\vy).
  \end{align}
  %
  $(Y-\vy)$ correspond au translaté de $Y$ de centre $\vy$ en $(0,0,0)$. Dans le
  premier terme, $\Vol(Y)$ est borné par le volume de la boule de rayon $R$.
  Avec la propriété d'additivité des intégrales, le second terme est maximisé
  par le $100$-moment de la demi-boule $\HalfBall{R}{0}$ centrée en $0$ à valeur
  de $x_1$ positives. Alors, en utilisant les coordonnées sphériques, nous
  obtenons :
  %
  \begin{align}
    \Mom{100}(\HalfBall{R}{0})
    &= \int_{0}^{R} \int_{\frac{-\pi}{2}}^{\frac{\pi}{2}} \int_{\frac{-\pi}{2}}^{\frac{\pi}{2}} (\rho\cos\phi\cos\theta)(\rho^2 \cos\phi)  \, d\theta d\phi d\rho \\
    &= \left[\frac{\rho^4}{4} \right]_{0}^{R}  \left [\sin\theta\right ]^\frac{\pi}{2}_{-\frac{\pi}{2}}\frac{1}{2} [\phi + \sin(\phi)\cos(\phi)]^\frac{\pi}{2}_{-\frac{\pi}{2}} \\
    &= \frac{\pi}{4}R^4, \label{eq:exact-half-ball-moment-1}
  \end{align}
  %
  Les autres équations sont prouvées de la même façon.
  %
\end{proof}


Pour plus de simplicité dans les formules, nous noterons $A$ l'ensemble
euclidien $\Ball{R}{\vx} \cap \Shape$. La matrice de covariance $\Cov(A)$ de
$A$ peut alors être réécrite comme :
\begin{align}
  \Cov(A) &=  \left\lbrack
    \begin{array}{ccc}
      \Mom{200}(A) & \Mom{110}(A) & \Mom{101}(A)\\
      \Mom{110}(A) & \Mom{020}(A) & \Mom{011}(A)\\
      \Mom{101}(A) & \Mom{011}(A) & \Mom{002}(A)
    \end{array}
    \right\rbrack\quad\quad\quad\quad\quad\quad \nonumber \\
    &\quad\quad\quad\quad\quad\quad- \frac{1}{\Mom{000}(A)}
    \left\lbrack
    \begin{array}{c}
      \Mom{100}(A) \\
      \Mom{010}(A) \\
      \Mom{001}(A) \\
    \end{array}
    \right\rbrack
    \otimes
    \left\lbrack
    \begin{array}{c}
      \Mom{100}(A) \\
      \Mom{010}(A) \\
      \Mom{001}(A) \\
    \end{array}
    \right\rbrack^T.
\label{eq:defCov}
\end{align}
%
où $\otimes$ désigne le produit de tensoriel dans l'espace des vecteurs.
%
\cauthors{Pottmann}{Pottmann2007} ont démontré que les valeurs propres et les
vecteurs propres de $\Cov(A)$ fournissaient des informations sur les courbures
principales et directions principales de courbure :
%
\begin{lemma}[\cite{Pottmann2007}, Théorème~2]
 \label{lem:pottmann-3d}
 %
 Pour une forme $\Shape \in \Shapes$ suffisamment lisse, les valeurs propres $\lambda_1$,
 $\lambda_2$, $\lambda_3$ de $\Cov(A)$, où $ A \EqDef \Ball{R}{\vx} \cap
 \Shape$ et $\vx \in \Bd{\Shape}$, $\lambda_1 \ge \lambda_2 \ge
 \lambda_3$, nous avons l'approximation de Taylor suivante :
 %
 \begin{align}
   \lambda_1& = \frac{2\pi}{15}R^5 - \frac{\pi}{48}(3\PrincCurv{1}(\Shape,\vx) + \PrincCurv{2}(\Shape,\vx))R^6 + O(R^7)\,,\\
   \lambda_2& = \frac{2\pi}{15}R^5 - \frac{\pi}{48}(\PrincCurv{1}(\Shape,\vx) + 3\PrincCurv{2}(\Shape,\vx))R^6 + O(R^7)\,,\\
   \lambda_3& = \frac{19\pi}{480}R^5 - \frac{9\pi}{512}(\PrincCurv{1}(\Shape,\vx) + \PrincCurv{2}(\Shape,\vx))R^6 + O(R^7)\,,
 \end{align}
 %
 où $\PrincCurv{1}(\Shape,\vx)$ et $\PrincCurv{2}(\Shape,\vx)$ désignent les
 courbures principales de  $\Bd{\Shape}$ au point $\vx$.\footnote{Il y a une
 erreur typographique dans l'expression de $\lambda_1$ dans~\cite{Pottmann2007}, corrigée ici.}
 %
\end{lemma}
%
De même qu'avec l'\RefEquation{def:pottmann-2d-3d-mean}, nous pouvons définir
les estimateurs locaux de courbures principales $\PrincCurvT{1}{R}$ et
$\PrincCurvT{2}{R}$ et de courbure gaussienne $\GaussCurvT{R}$ comme des
fonctions de $\{\lambda_i\}_{1,2,3}$ et de $R$ :
%
\begin{align}
  \label{eq:pottmann-3d-princ}
  \PrincCurvT{1}{R}(\Shape,\vx)& \EqDef \frac{6}{\pi R^6}(\lambda_2 - 3\lambda_1) + \frac{8}{5R}\,,\\
  \PrincCurvT{2}{R}(\Shape,\vx)& \EqDef \frac{6}{\pi R^6}(\lambda_1 - 3\lambda_2) + \frac{8}{5R}\,,\\
  \GaussCurvT{R}(\Shape,\vx)& \EqDef \PrincCurvT{1}{R} \cdot \PrincCurvT{2}{R}.
\end{align}
%
D'après le \RefLemma{lem:pottmann-3d}, tous ces estimateurs approchent la
quantité attendue lorsque le rayon $R$ tend vers $0$ :
%
\begin{align}
  \label{eq:pottmann-3d-princ-conv}
  \PrincCurvT{1}{R}(\Shape,\vx)& = \PrincCurv{1}(\Shape,\vx) + O(R)\,,\\
  \PrincCurvT{2}{R}(\Shape,\vx)& = \PrincCurv{2}(\Shape,\vx) + O(R)\,,\\
  \GaussCurvT{R}(\Shape,\vx)& = \GaussCurv(\Shape,\vx) + O(R^2) (+ O(R)) \,.
\end{align}
%
Il est important à noter que pour la courbure gaussienne, la constante derrière le terme
le grand « $O$ » de $O(R)$ est dépendant de la courbure moyenne
$\MeanCurv(\Shape, \vx)$.

Lorsque nous nous intéressons à des formes digitales $\DSh$ (voir
\RefSection{sec:digitization}), l'implémentation de ces estimateurs devient
directe (voir \RefFigure{fig:2d-curv-estimator}) : choisir un rayon $R$, centrer
une boule euclidienne (ou digitale) sur des points de $\Bd{\Body{\DSh}{h}}$
(\cad des éléments de la surface digitale), calculer la quantité par
dénombrement de points digitaux (aire, volume ou matrice de covariance) et enfin
estimer l'information de courbure $\CurvT{R}$, $\MeanCurvT{R}$,
$\PrincCurvT{1}{R}$ ou $\PrincCurvT{2}{R}$.

\begin{figure}[ht]
  \begin{center}
    \input{figures/tikz/2d-curv-estimator.tikz}
  \end{center}
  %
  \caption[Illustration de l'estimateur digital de courbure en 2D par
  intégration $\CurvH{R}$.]{Illustration de l'estimateur digital de courbure en
  dimension 2 par intégration $\CurvH{R}$ avec deux pas de discrétisation $h =
  1$ et $h = 0.5$ sur un disque de rayon $7.5$. La sphère orange a un rayon
  valant $3$. Sur la figure de gauche, on dénombre $12$ points à l'intersection
  entre la sphère et la forme; sur la figure de droite on dénombre $50$ points.
  Alors la courbure estimée sur la figure de gauche vaut
  $\CurvH{3}(\DigF{\Shape}{1}, \vx, 1) = 3\pi/(2\times3) -
  3\times12\times1^2/3^3 \approx 0.237$ et celle de la figure de droite vaut
  $\CurvH{3}(\DigF{\Shape}{0.5}, \vx, 0.5) = 3\pi/(2\times3) -
  3\times50\times{0.5}^2/3^3 \approx 0.182$. La vraie valeur de la courbure est
  $1/7.5 \approx 0.133$.
  %
  \label{fig:2d-curv-estimator}}
%
\end{figure}

Cependant, quelques problèmes surviennent avec cette approche : comment bien
estimer l'aire, le volume ou la matrice de covariance sur des formes digitales ?
Pouvons nous apporter des preuves de convergence de ces estimateurs sur des
données digitales ? Que veut dire « $R$ tend vers zéro » quand la taille de la
grille digitale influe également sur ce rayon ? Dans une premier temps, nous allons
nous intéresser à l'aire et au volume digital, puis aux moments
digitaux.

\subsection{Analyse digitale de l'aire et du volume de la boule et relation à la courbure}
\label{sec:ii-2d}

\begin{figure}[ht]
  \begin{center}
    \input{figures/tikz/2d-area-2.tikz}
  \end{center}
  \caption
  %
  {Illustration de la dépendance du rayon pour une échelle $h$ donnée. Le disque de gauche
  a un rayon de $3$ et celui de droite a un rayon de $1.5$, soit deux fois moins
  que celui de droite. Le nombre de points digitaux suite à la discrétisation de
  Gauss des disques avec un pas de $h=1$ est de $29$ pour le disque de gauche et
  de $6$ pour le disque de droite. L'aire digitale estimée du disque de gauche
  est alors de $1^2 \times 29 = 29$, l'aire digitale estimée du disque de droite
  est de $1^2 \times 6 = 6$. Il est alors évident que $29 \ne 4 \times
  6$ alors que $\pi \times 3^2 = 4 \times (\pi \times (1.5)^2)$.\label{fig:2d-area-2}}
  %
\end{figure}

Nous souhaitons estimer la quantité $A_R(\vx) = \Area(\Ball{R}{\vx}\cap\Shape)$ de la
\RefDefinition{def:Volume} au pas de discrétisation $h$, où $\Ball{R}{\vx}$ est une
boule de rayon $R$ centrée au point $\vx$. Nous ne pouvons pas directement
utiliser les résultats des
\RefEquations{eq:AreaByCountingConv}{eq:VolumeByCountingConv} pour estimer cette
aire : le terme d'erreur en grand « O » cache le fait que la constante impliquée
dépend de la taille de la forme, son échelle et sa courbure maximale. Un moyen
simple de comprendre cet effet est d'estimer l'aire digitale d'une forme $\Shape$ et
celle de la même forme doublée en taille (que nous nommerons
$\Shape'$). Alors que pour des formes euclidiennes, nous avons :
%
\begin{equation}
  4 \times \Area(\Shape) = \Area(\Shape') \,,
\end{equation}
%
leurs aires digitales estimées ne suivent pas la même propriété :
%
\begin{equation}
  4 \times \AreaC(\DSh,h) \ne \AreaC(\DigF{\Shape'}{h},h) \,,
\end{equation}
%
la discrétisation de $\Shape'$ la rendant plus détaillée et donc son estimation
plus précise. Cet effet est illustré avec la \RefFigure{fig:2d-area-2}. C'est
un problème gênant avec les invariants par intégration car les boules impliquées
ont un rayon $R$ qui doit tendre vers $0$ lorsque $h$ tend également vers $0$.
Nous devons alors normaliser notre estimation d'aire et de volume pour que le
terme d'erreur ne soit plus influencé par l'échelle de la boule. Nous estimons
alors l'aire de l'intersection entre la forme et la boule de rayon $R$ en se
ramenant à la boule unité :
%
\begin{align}
  \AreaC(\DigF{\Ball{R}{\vx} \cap \Shape}{h}, h) & \EqDef h^2 \MCard\left( \left( \frac{1}{h} \cdot \left( \Ball{R}{\vx} \cap \Shape \right) \right) \cap \Z^2 \right), \\
   & = h^2 \MCard\left( \left(\frac{R}{h} \cdot \left( \Ball{1}{\frac{1}{R} \cdot \vx} \cap \frac{1}{R} \cdot \Shape\right) \right) \cap \Z^2 \right), \\
   & = R^2 \frac{h^2}{R^2} \MCard\left( \left(\frac{R}{h} \cdot \left( \Ball{1}{\frac{1}{R} \cdot \vx} \cap \frac{1}{R} \cdot \Shape\right) \right) \cap \Z^2 \right), \\
   & = R^2 \AreaC\left( \DigF{\Ball{1}{\frac{1}{R} \cdot \vx} \cap \frac{1}{R} \cdot \Shape}{h/R}, h/R \right), \label{eq:Area_unitary_ball}
\end{align}
%
Alors, en insérant l'\RefEquation{eq:AreaByCountingConv} dans le terme de droite
de \ref{eq:Area_unitary_ball}\footnote{Le terme $\beta$ est celui défini dans le \RefSection{sec:AreaByCounting}.} :
\begin{equation}
  \label{eq:Area_unitary_ball_shape}
  \AreaC(\DigF{\Ball{R}{\vx} \cap \Shape}{h}, h) = R^2 \left(\Area( \Ball{1}{\frac{1}{R} \cdot \vx} \cap \frac{1}{R} \cdot \Shape) + O( (h/R)^\beta) \right).
\end{equation}
%
Notons $SB(R)$ l'ensemble $\Ball{1}{\frac{1}{R} \cdot \vx} \cap \frac{1}{R}
\cdot \Shape$. La constante $K_1$ associée au terme d'erreur grand « O » dépend
uniquement de la courbure maximale de $\BT SB(R)$. La courbure n'est pas définie
aux intersections des deux bords de la forme et de la boule (dans le
sous-ensemble $\BT \Ball{1}{\frac{1}{R} \cdot \vx} \cap \frac{1}{R} \cdot \dS$)
mais son influence dans l'estimation d'aire est négligeable car elle est de l'ordre
de $O(h^2)$. La partie restante de $\BT SB(R)$ a une courbure maximale qui est
égale à $1$ pour un rayon $R$ suffisamment petit. En effet, puisque $\Shape$ a
une courbure bornée, son dilaté $\frac{1}{R} \cdot \dS$ devient plat au point
$\frac{1}{R} \cdot \vx$ quand $R$ tend vers $0$, alors la courbure maximale est
introduite par $\BT \Ball{1}{\vx}$. Il existe un rayon $R_0$ tel qu'une
constante $K_1$ intervienne pour un rayon arbitraire $R < R_0$. En développant
le terme d'erreur en grand « O » avec ce $K_1$, et en insérant dans
l'\RefEquation{eq:Area_unitary_ball_shape} la relation :
%
\begin{equation}
  A_R(\vx) = \Area( \Ball{R}{\vx} \cap \Shape ) = R^2 \Area( \Ball{1}{\frac{1}{R} \cdot \vx} \cap \frac{1}{R} \cdot \Shape),
\end{equation}
%
nous obtenons le théorème suivant :
%
\begin{theorem}{\fakeTitle{Convergence de l'estimation digitale de l'aire à l'intersection d'un forme et d'une boule}}
  \label{thm:convergence-area-intersection}
  %
  Soit $\Shape$ une forme convexe finie de $\R^2$. Alors, \colorize{l'estimation
  digitale de l'aire} au point $\vx \in \R^2$ à l'intersection de $\Shape$ et
  d'une boule $\Ball{R}{\vx}$ de rayon $R$ centrée en $\vx$ converge vers son
  aire euclidienne pour la discrétisation de Gauss au pas de discrétisation $h$,
  avec $0 < h \le R \le R_0$. Plus précisément, nous avons :
  %
  \begin{equation}\label{eq:convergence-area-intersection}
    | \AreaC(\DigF{\Ball{R}{\vx} \cap \Shape}{h}, h) - A_R(\vx) | \le K_1 h^\beta R^{2-\beta} \,,
  \end{equation}
  %
  où $K_1$ est une constante indépendante de $\Shape$.
\end{theorem}
%
La convergence de la précédente relation tient du fait que $h \le R \le R_0$ et
est valide quand $\vx$ est n'importe quel point de $\R^2$ (pas seulement un
point de $\dS$). De plus, la constante $K_1$ est indépendante de la forme
$\Shape$ (mais dépendante de $R_0$).

Le même raisonnement est valide en dimension 3 : la courbure n'est alors pas
définie dans le sous-ensemble $\BT \Ball{1}{\frac{1}{R} \cdot \vx} \cap \frac{1}{R}
\cdot \dS$, qui tend vers le cercle unité lorsque $R \rightarrow 0$. L'erreur
introduite dans l'estimation de volume est alors le nombre de voxels intersectés
($\approx 2\pi/h$), ce qui est négligeable. La courbure maximale est également
$1$ pour un rayon $R$ suffisamment petit. Nous obtenons alors :
%
\begin{theorem}{\fakeTitle{Convergence de l'estimation digitale du volume à l'intersection d'un forme et d'une boule}}
  \label{thm:convergence-volume-intersection}
  %
  Soit $\Shape$ une forme convexe finie de $\R^3$. Alors, \colorize{l'estimation
  digitale du volume} au point $\vx \in \R^3$ à l'intersection de $\Shape$ et
  d'une boule $\Ball{R}{\vx}$ de rayon $R$ centrée en $\vx$ converge vers son
  volume euclidien pour la discrétisation de Gauss au pas de discrétisation $h$,
  avec $0 < h \le R \le R_0$. Plus précisément, nous avons :
  %
  \begin{equation}\label{eq:convergence-volume-intersection}
    | \VolC(\DigF{\Ball{R}{\vx} \cap \Shape}{h}, h) - V_R(\vx) | \le K_1 h^\gamma R^{3-\gamma} \,,
  \end{equation}
  %
  où $K_1$ est une constante indépendante de $\Shape$.
\end{theorem}
%
\subsubsection{Estimateurs digitaux de courbure en 2D et de courbure moyenne en 3D par intégration}
%
Nous allons définir des estimateurs digitaux de courbure en 2D $\CurvH{R}$ et de
courbure moyenne en 3D $\MeanCurvH{R}$ dans le même esprit que la
\RefDefinition{def:pottmann-2d-3d-mean}, dont le calcul consiste uniquement à
compter le nombre de points digitaux dans un voisinage du point d'intérêt :
%
\begin{definition}{\fakeTitle{Estimateur digital de courbure en 2D par intégration}}\label{def:digital-2d-curvature}
  %
  Pour tout rayon $R$ positif, nous définissons l'estimateur digital de courbure
  2D par intégration $\CurvH{R}$ d'une forme digitale $\DigShape \subset \Z^2$
  en tout point $\vx \in \R^2$ et pour le pas de discrétisation $h > 0$ comme :
  %
  \begin{equation}
    \forall 0 < h < R,\quad \CurvH{R}(\DigShape,\vx,h) \EqDef \frac{3\pi}{2R} - \frac{3\AreaC(\Ball{R/h}{\vx/h} \cap \DigShape, h)}{R^3}\,,
  \end{equation}
  %
  où $\frac{R}{h}$ est le rayon digital de la boule.
\end{definition}
%
\begin{definition}{\fakeTitle{Estimateur digital de courbure moyenne en 3D par intégration}}\label{def:digital-3d-mean-curvature}
  %
  Pour tout rayon $R$ positif, nous définissons l'estimateur digital de courbure
  moyenne en 3D par intégration $\MeanCurvH{R}$ d'une forme digitale $\DigShape
  \subset \Z^3$ en tout point $\vx \in \R^3$ et pour le pas de discrétisation $h >
  0$ comme :
  %
  \begin{equation}
    \forall 0 < h < R,\quad \MeanCurvH{R}(\DigShape,\vx,h) \EqDef \frac{8}{3R} - \frac{4\VolC(\Ball{R/h}{\vx/h} \cap \DigShape, h)}{\pi R^4}\,.
  \end{equation}
  %
\end{definition}
%
Comme le montre la \RefFigure{fig:2d-curv-estimator}, ces estimateurs placent
une boule de rayon euclidien $R$ autour du point d'intérêt $\vx$ et comptent le
nombre de points digitaux de $\DigShape = \Body{\DSh}{h}$, de l'intersection de
cette boule et de la forme. Un calcul linéaire simple est alors appliqué sur
cette quantité afin d'approcher la courbure. L'intuition est, comme le montre
également la \RefFigure{fig:2d-curv-estimator}, que si la discrétisation
s'affine nous aurons une meilleure estimation de la courbure. Le prochain
paragraphe va le démontrer.
%
\subsubsection{Convergence des estimateurs digitaux de courbure en 2D et de courbure moyenne en 3D par intégration}
%
Nous allons ici montrer que les estimateurs de courbure en 2D $\CurvH{R}$ et de
courbure moyenne en 3D $\MeanCurvH{R}$ convergent vers la valeur attendue de
courbure pour tous les points $\vx$ le long du bord $\dS$ de l'objet $\Shape$ si
la forme respecte certaines contraintes.
%
\begin{theorem}{\fakeTitle{Convergence de l'estimateur digital de courbure en 2D $\CurvH{R}$ le long de $\dS$ \cite{DGCI2013,CVIU2014}}}
  \label{thm:convergence-curv-2d}
  %
  Soit $\Shape$ une forme convexe de $\R^2$ telle que son bord $\dS$ est $C^2$ et
  sa courbure bornée. Alors, l'estimateur digital de courbure en 2D $\CurvH{R}$ en
  tout point $\vx$ de $\dS$ converge vers la courbure
  $\Curv(\Shape,\vx)$ de $\Shape$ au point $\vx$ pour la discrétisation de
  Gauss, avec une vitesse de convergence d'au moins $O(h^{\frac{1}{3}})$ lorsque
  $R = O(h^{\frac{1}{3}})$. Plus précisément, nous avons :
  %
  \begin{equation}
    \forall 0 < h < h_0,
    \quad \left | \CurvH{R}(\DSh, \vx, h) - \Curv(\Shape, \vx) \right|
                          \le O \left(h^{\frac{1}{3}}\right). %% O(R) + K_1 R h.
  \end{equation}
  %
\end{theorem}
\begin{proof}
  %
  En utilisant les relations sur les invariants par intégration définis
  précédemment (\RefDefinition{def:digital-2d-curvature}), ainsi que la relation
  $\DigF{\Ball{R}{\vx} \cap \Shape}{h}=\Ball{R/h}{\frac{1}{h} \cdot \vx} \cap
  \DSh$, nous obtenons pour $R < R_0$ :
  %
  \begin{equation}
    | \CurvH{R}(\DSh,\vx,h) - \Curv(\Shape,\vx) |
    = \left|\frac{3\pi}{2R}-\frac{3\AreaC(\Ball{R/h}{\vx / h} \cap \DSh, h)}{R^3} - \Curv(\Shape,\vx) \right|,
  \end{equation}
  %
  en utilisant la relation entre l'estimation digitale de l'aire et l'aire
  euclidienne du \RefTheorem{thm:convergence-area-intersection}, nous obtenons :
  %
  \begin{align}\label{eq-curvhat-error-bound-prelim}
    | \CurvH{R}(\DSh,\vx,h) - \Curv(\Shape,\vx) |
    &\le \left|\frac{3\pi}{2R}-\frac{3\Area(\Ball{R}{\vx} \cap \Shape)}{R^3} - \Curv(\Shape,\vx) \right| + 3K_1 \frac{h^\beta}{R^{1+\beta}} \nonumber \\
    &\le \left|\CurvT{R}(\Shape,\vx) - \Curv(\Shape,\vx) \right| + 3K_1 \frac{h^\beta}{R^{1+\beta}},
  \end{align}
  %
  ainsi, en utilisant le \RefTheorem{theo:pottmann-2d-3d-mean-conv}, nous pouvons
  écrire que :
  %
  \begin{equation}\label{eq-curvhat-error-bound-prelim2}
    | \CurvH{R}(\DSh,\vx,h) - \Curv(\Shape,\vx) |
    \le O(R) + 3K_1 \frac{h^\beta}{R^{1+\beta}}.
  \end{equation}
  %
  Nous obtenons alors deux termes d'erreur dépendants du rayon $R$ de la boule
  qui sont contradictoires : lorsque $R$ diminue, $O(R)$ diminue mais $3K_1
  \frac{h^\beta}{R^{1+\beta}}$ augmente, et vice-versa. Nous voulons alors
  guider le rayon $R$ afin d'obtenir un optimal de notre terme d'erreur. Nous
  proposons alors de définir $R = k h^{\alpha}$ et de choisir $k$ (une
  constante) et $\alpha$ afin de minimiser les bornes d'erreur. Nous notons
  $K_2$ la constante derrière le grand « O » de $O(R)$, nous obtenons :
  %
  \begin{equation}\label{eq-curvhat-error-bound-prelim3}
    | \CurvH{R}(\DSh,\vx,h) - \Curv(\Shape,\vx) |
    \le K_2kh^\alpha + \frac{3K_1}{k^{1+\beta}}h^{\beta - \alpha(1+\beta)}.
  \end{equation}
  %
  Alors, l'erreur minimale est atteinte lorsque les exposants sont égaux : nous
  devons résoudre $\alpha = \beta - \alpha(1+\beta)$.
  L'\RefEquation{eq:AreaByCountingConv} nous informe que $\beta = 1$ dans le cas
  général\footnote{Il est à noter que si le bord de la forme est $C^3$ sans
  courbure nulle, $\beta$ est alors égal à $\frac{15}{11} - \epsilon$ et donc
  $\alpha = \frac{15}{37} - \epsilon \approx 0.405$. Voir
  \RefSection{sec:AreaByCounting}.}, et donc $\alpha = \frac{1}{3}$. Donc,
  lorsque $R=kh^{\frac{1}{3}}$ :
  %
  \begin{align}\label{eq-curvhat-error-bound-prelim4}
    | \CurvH{R}(\DSh,\vx,h) - \Curv(\Shape,\vx) |
    &\le K_2kh^\frac{1}{3} + \frac{3K_1}{k^2}h^{\frac{1}{3}}\\
    &\le O(h^\frac{1}{3}).
  \end{align}
  %
\end{proof}

Nous obtenons des résultats similaires pour l'estimateur digital de courbure
moyenne sur des formes en 3D :
%
\begin{theorem}{\fakeTitle{Convergence de l'estimateur digital de courbure moyenne en 3D $\MeanCurvH{R}$ le long de $\dS$ \cite{DGCI2013,CVIU2014}}}
  \label{thm:convergence-curv-3d}
  %
  Soit $\Shape$ une forme convexe de $\R^3$ telle que son bord $\dS$ est $C^3$ et
  sa courbure bornée. Alors, l'estimateur digital de courbure moyenne en 3D
  $\MeanCurvH{R}$ en tout point $\vx$ de $\dS$ converge vers la
  courbure $\MeanCurv(\Shape,\vx)$ de $\Shape$ au point $\vx$ pour la
  discrétisation de Gauss, avec une vitesse de convergence d'au moins
  $O(h^{\frac{1}{3}})$ lorsque $R = O(h^{\frac{1}{3}})$. Plus précisément, nous
  avons :
  %
  \begin{equation}
    \forall 0 < h < h_0,
    \quad \left | \MeanCurvH{R}(\DSh, \vx, h) - \MeanCurv(\Shape, \vx) \right|
                          \le O \left(h^{\frac{1}{3}}\right). %% O(R) + K_1 R h.
  \end{equation}
  %
\end{theorem}
\begin{proof}
  %
  La preuve suit exactement les mêmes étapes que la preuve du
  \RefTheorem{thm:convergence-curv-2d}. Ainsi, la
  \RefDefinition{def:digital-3d-mean-curvature} et la relation
  $\DigF{\Ball{R}{\vx} \cap \Shape}{h}=\Ball{R/h}{\frac{1}{h} \cdot \vx} \cap \DSh$ nous
  permettent d'obtenir pour $R < R_0$ :
  %
  \begin{equation}
    | \MeanCurvH{R}(\DSh,\vx,h) - \MeanCurv(\Shape,\vx) |
    = \left|\frac{8}{3R}-\frac{4\VolC(\Ball{R/h}{\vx / h} \cap \DSh, h)}{\pi R^4} - \MeanCurv(\Shape,\vx) \right|,
  \end{equation}
  %
  en utilisant la relation entre l'estimation digitale du volume et le volume
  euclidien du \RefTheorem{thm:convergence-volume-intersection}, nous obtenons :
  %
  \begin{align}\label{eq-meancurvhat-error-bound-prelim}
    | \MeanCurvH{R}(\DSh,\vx,h) - \MeanCurv(\Shape,\vx) |
    &\le \left|\frac{8}{3R}-\frac{4\Vol(\Ball{R}{\vx} \cap \Shape)}{\pi R^4} - \MeanCurv(\Shape,\vx) \right| + \frac{4 K_1}{\pi} \frac{h^\gamma}{R^{1+\gamma}} \nonumber \\
    &\le \left|\MeanCurvT{R}(\Shape,\vx) - \MeanCurv(\Shape,\vx) \right| + \frac{4 K_1}{\pi} \frac{h^\gamma}{R^{1+\gamma}},
  \end{align}
  %
  ainsi, en utilisant le \RefTheorem{theo:pottmann-2d-3d-mean-conv}, nous pouvons
  écrire que :
  %
  \begin{equation}\label{eq-meancurvhat-error-bound-prelim2}
    | \MeanCurvH{R}(\DSh,\vx,h) - \MeanCurv(\Shape,\vx) |
    \le O(R) + \frac{4 K_1}{\pi} \frac{h^\gamma}{R^{1+\gamma}}.
  \end{equation}

  Comme précédemment, les deux termes d'erreurs sont dépendants du rayon $R$ de
  la boule et ont un effet antagoniste. Nous proposons de définir $R = k h^{\alpha}$,
  $k$ étant une constante, afin de trouver le $\alpha$ optimal pour minimiser
  l'erreur. Nous notons $K_2$ la constante derrière le grand « O » de $O(R)$,
  nous obtenons :
  %
  \begin{equation}\label{eq-meancurvhat-error-bound-prelim3}
    | \MeanCurvH{R}(\DSh,\vx,h) - \MeanCurv(\Shape,\vx) |
    \le K_2kh^\alpha + \frac{3K_1}{k^{1+\gamma}}h^{\gamma - \alpha(1+\gamma)}.
  \end{equation}
  %
  Et comme précédemment, cela revient à résoudre $\alpha = \gamma -
  \alpha(1+\gamma)$. L'\RefEquation{eq:VolumeByCountingConv} nous informe que
  $\gamma = 1$ dans le cas général\footnote{Il est à noter que si le bord de la
  forme est lisse, $\gamma$ est alors égal à $\frac{243}{158}$ et donc $\alpha =
  \frac{243}{559} \approx 0.435$. Voir \RefSection{sec:AreaByCounting}.}, et
  donc $\alpha = \frac{1}{3}$, comme en 2D. Donc, lorsque $R=kh^{\frac{1}{3}}$ :
  %
  \begin{align}\label{eq-meancurvhat-error-bound-prelim4}
    | \MeanCurvH{R}(\DSh,\vx,h) - \MeanCurv(\Shape,\vx) |
    &\le K_2kh^\frac{1}{3} + \frac{4K_1}{\pi k^2}h^{\frac{1}{3}}\\
    &\le O(h^\frac{1}{3}).
  \end{align}
  %
\end{proof}

Cependant, ces théorèmes ne sont valides que pour tout point $\vx$ de $\dS$.
Lorsque nous traitons des bords des surfaces digitales, une nouvelle
approximation intervient dû à la discrétisation. Le prochain paragraphe adaptera
ces estimateurs sur des contours digitaux de surface.
%
\subsubsection{Convergence asymptotique des estimateurs digitaux de courbure en
2D et de courbure moyenne en 3D sur le bord des surfaces digitales}

\begin{figure}[ht]
  \begin{center}
    \input{figures/tikz/2d-curv-estimator2.tikz}
  \end{center}
  \caption
  %
  {Illustration de la différence entre l'estimation digitale de courbure sur le
  bord euclidien $\dS$ de l'objet $\Shape$ (\emph{à gauche}) et l'estimation
  digitale de courbure sur le bord digital $\partial \Body{\DSh}{h}$ du
  plongement euclidien $\Body{\DSh}{h}$ de la discrétisation de Gauss de
  $\Shape$ (\emph{à droite}), au pas de discrétisation $h$. Nous pouvons
  remarquer que nous n'obtenons pas le même nombre de points digitaux à
  l'intersection entre la boule et la
  forme.\label{fig:2d-curv-estimator-xhat}}
  %
\end{figure}

Comme nous venons de le dire, nous ne connaissons pas exactement la position de
$\vx$ lorsque nous considérons des données digitales. Les seules informations
dont nous disposons sont les points digitaux $\vxH$ de $\Bd{\Body{\DSh}{h}}$.
Nous devons alors considérer l'erreur possible de positionnement de l'endroit où
est estimé la courbure pour proposer des théorèmes de convergence
asymptotiques (voir la \RefFigure{fig:2d-curv-estimator-xhat}).
%
Nous aurons besoin de la définition de la projection inverse
(\RefDefinition{def:projection}), nous devons alors injecter cette approximation
dans le calcul d'aire et de volume, pour ensuite la diffuser aux calculs de
courbures. Ainsi, nous pouvons prouver la convergence asymptotique des
estimateurs digitaux de courbure sur le bord discrétisé d'un objet suffisamment
lisse.
%
\begin{theorem}{\fakeTitle{\cite{DGCI2013,CVIU2014}}} \label{thm:multigrid-convergence-curv}
%
Soit $\Shape$ une forme convexe de $\R^2$ telle que son bord $\dS$ est $C^3$ à
courbure bornée positive. Alors, l'estimateur digital de courbure $\CurvH{R}$ converge
asymptotiquement vers la courbure $\Curv$ pour la discrétisation de Gauss sur
des formes $\Shape$, avec une vitesse de convergence d'au moins
$O(h^\frac{1}{3})$ lorsque $R = kh^\frac{1}{3}$. Plus précisément :
%
\begin{align}
  \forall 0 < h \le h_0,\,\, & \forall \vx \in \Bd{\Shape},\,\,
  \forall \vxH \in \Bd{\Body{\DSh}{h}} \text{~avec~} \| \vxH -\vx\|_\infty \le h, \nonumber \\
  & \left| \CurvH{R}(\DSh, \vxH, h) - \Curv(\Shape, \vx) \right| \le O\left(h^{\frac{1}{3}}\right)\,.
\end{align}
%
\end{theorem}
%
\begin{proof}
%
Notons $t \EqDef ||\vx-\vxH||_2$ la distance d'un point $\vxH$ de
$\Bd{\Body{\DSh}{h}}$ à un point $\vx$ de $\dS$. En 3D, le
\RefTheoremFake{7}{Pottmann2009} permet de quantifier cette erreur :
%
\begin{equation}\label{eq:volume-shift-error}
  | V_r(\vxH) - V_r(\vx) | = R^2 \pi t (1 + O(R^2)+O(t)).
\end{equation}
%
De la même façon en 2D, nous obtenons :
%
\begin{equation}\label{eq:area-shift-error}
  | A_r(\vxH) - A_r(\vx) | = 2R t (1 + O(R^2)+O(t)).
\end{equation}
%
Nous pouvons alors réécrire l'\RefEquation{eq:convergence-area-intersection}
pour $\vxH$ :
%
\begin{equation}
  | \AreaC(\DigF{\Ball{R}{\vxH} \cap \Shape}{h}, h) - A_R(\vxH) | \le K_1 h^\beta R^{2-\beta},
\end{equation}
%
ce qui implique, avec l'\RefEquation{eq:area-shift-error} :
%
\begin{equation}\label{eq:shift-B-R-error-bound-2D}
  | \AreaC(\DigF{\Ball{R}{\vxH} \cap \Shape}{h}, h) - A_R(\vx) |  \le K_1 h^\beta R^{2-\beta} +  2 R t (1 + O(R^2) + O(t)).
\end{equation}
%
Ensuite, dans le but d'obtenir un estimateur de courbure, nous suivons le même
raisonnement que pour la preuve du \RefTheorem{thm:convergence-curv-2d} mais en
utilisant l'\RefEquation{eq:shift-B-R-error-bound-2D}, ce qui nous donne :
%
\begin{equation}\label{eq:shift-curvhat-error-bound-2D}
  | \CurvH{R}(\DSh,\vxH,h) - \Curv(\Shape,\vx) | \le O(R) + 3 K_1 \frac{h^\beta}{R^{1+\beta}} + \frac{6 t}{R^2}(1 + O(R^2) + O(t)).
\end{equation}
%
Puisque nous sommes sur des données digitales, nous savons que $t \le
\frac{\sqrt{2}}{2}h$ d'après la \RefDefinition{def:projection} de la projection
inverse. Dans certains cas, nous pouvons espérer une meilleure localisation de
$\vxH$ vers $\vx$ \cite{deVieilleville2006}. Nous noterons alors $t =
O(h^{\alpha'})$, avec $\alpha' \ge 1$. Nous pouvons réécrire la précédente
équation afin d'obtenir une borne d'erreur uniquement dépendante de $h$, avec
$R=kh^{\alpha}$ :
%
\begin{equation} \label{eq:shift-curvhat-error-bound-h-only}
  | \CurvH{R}(\DSh,\vxH,h) - \Curv(\Shape,\vx) |
  \le O(h^\alpha) + O(h^{\beta-\alpha(1+\beta)}) + O(h^{\alpha'-2\alpha})
    + O(h^{\alpha'}) + O(h^{2\alpha'-2\alpha}).
\end{equation}
%
Ainsi, comme pour le \RefTheorem{thm:convergence-curv-2d}, nous devons trouver
le meilleur $\alpha$ possible, à la différence ici qu'il dépend de $\beta$ et
$\alpha'$.
%
Lorsque $\alpha$ augmente, $\beta-\alpha(1+\beta)$ et $\alpha'-2\alpha$ sont les
erreurs qui diminuent le plus, il faut alors résoudre $\alpha =
\beta-\alpha(1+\beta)$ et $\alpha = \alpha'-2\alpha$ afin de trouver la valeur
de $\alpha$ optimale\footnote{Une erreur s'est glissée dans ce calcul dans
\cite{DGCI2013}.}.
%
\begin{equation}
\begin{aligned}[c]
  \alpha &= \beta-\alpha(1+\beta)\\
  \alpha &= \frac{\beta}{2+\beta}
\end{aligned}
\qquad
\begin{aligned}[c]
  \alpha &= \alpha' - 2\alpha\\
  \alpha &= \frac{\alpha'}{3}
\end{aligned}
\end{equation}
%
Si $\alpha' \ge \frac{3 \beta}{2 + \beta}$, alors $\alpha = \frac{\beta}{2 +
\beta}$, sinon $\alpha = \frac{\alpha'}{3}$.
%
Si le point $\vxH$ est sur le bord digital $\Bd{\Body{\DSh}{h}}$, nous savons
que $\alpha'=1$ grâce à la relation $t \le \frac{\sqrt{2}}{2}h$, $\beta = 1$
dans le cas général, alors nous obtenons $\alpha = \frac{\alpha'}{3} =
\frac{1}{3}$. Donc, lorsque $R = kh^{\frac{1}{3}}$ :
%
\begin{equation}
  | \CurvH{R}(\DSh,\vxH) - \Curv(\Shape,\vx) | \le O(h^{\frac{1}{3}})
\end{equation}
%
\end{proof}


De la même façon, en 3D pour la courbure moyenne, nous pouvons prouver la
convergence asymptotique de l'estimateur $\MeanCurvH{R}$ sur le bord digital
d'un objet suffisamment lisse :
%
\begin{theorem}{\fakeTitle{\cite{DGCI2013,CVIU2014}}} \label{thm:multigrid-convergence-curv-mean}
%
Soit $\Shape$ une forme convexe de $\R^3$ telle que son bord $\dS$ est $C^3$ à
courbure bornée positive. Alors, l'estimateur digital de courbure $\MeanCurvH{R}$ converge
asymptotiquement vers la courbure $\MeanCurv$ pour la discrétisation de Gauss sur
des formes $\Shape$, avec une vitesse de convergence d'au moins
$O(h^\frac{1}{3})$ lorsque $R = kh^\frac{1}{3}$. Plus précisément :
%
\begin{align}
  \forall 0 < h \le h_0,\,\, & \forall \vx \in \Bd{\Shape},\,\,
  \forall \vxH \in \Bd{\Body{\DSh}{h}} \text{~avec~} \| \vxH -\vx\|_\infty \le h, \nonumber \\
  & \left| \MeanCurvH{R}(\DSh, \vxH, h) - \MeanCurv(\Shape, \vx) \right| \le O\left(h^{\frac{1}{3}}\right)\,.
\end{align}
%
\end{theorem}
%
\begin{proof}
%
Notons $t \EqDef ||\vx-\vxH||_2$ la distance d'un point $\vxH$ de
$\Bd{\Body{\DSh}{h}}$ à un point $\vx$ de $\dS$. En 3D, le
\RefTheoremFake{7}{Pottmann2009} permet de quantifier cette erreur :
%
\begin{equation}\label{eq:volume-shift-error-2}
  | V_r(\vxH) - V_r(\vx) | = R^2 \pi t (1 + O(R^2)+O(t)).
\end{equation}
%
Nous pouvons alors réécrire l'\RefEquation{eq:convergence-volume-intersection}
pour $\vxH$ :
%
\begin{equation}
  | \VolC(\DigF{\Ball{R}{\vxH} \cap \Shape}{h}, h) - V_R(\vxH) | \le K_1 h^\gamma R^{3-\gamma},
\end{equation}
%
ce qui implique, avec l'\RefEquation{eq:volume-shift-error-2} :
%
\begin{equation}\label{eq:shift-B-R-error-bound-3D}
  | \VolC(\DigF{\Ball{R}{\vxH} \cap \Shape}{h}, h) - V_R(\vx) |  \le K_1 h^\gamma R^{3-\gamma} +  R^2 \pi t (1 + O(R^2) + O(t)).
\end{equation}
%
Ensuite, nous suivons le même raisonnement que pour la preuve du
\RefTheorem{thm:convergence-curv-3d} mais en utilisant
l'\RefEquation{eq:shift-B-R-error-bound-3D}, ce qui nous donne :
%
\begin{equation}\label{eq:shift-curvhat-error-bound-3D}
  | \MeanCurvH{R}(\DSh,\vxH,h) - \MeanCurv(\Shape,\vx) | \le O(R) + \frac{4 K_1}{\pi} \frac{h^\gamma}{R^{1+\gamma}} + \frac{4 t}{R^2}(1 + O(R^2) + O(t)).
\end{equation}
%
À nouveau, nous savons que $t \le \frac{\sqrt{3}}{2}h$ \cite{Lachaud2015}. Nous noterons alors
$t = O(h^{\alpha'})$, avec $\alpha' \ge 1$. Nous pouvons réécrire la
précédente équation afin d'obtenir une borne d'erreur uniquement dépendante de
$h$, avec $R=kh^{\alpha}$ :
%
\begin{equation} \label{eq:shift-meancurvhat-error-bound-h-only}
  | \MeanCurvH{R}(\DSh,\vxH,h) - \MeanCurv(\Shape,\vx) |
  \le O(h^\alpha) + O(h^{\gamma-\alpha(1+\gamma)}) + O(h^{\alpha'-2\alpha})
    + O(h^{\alpha'}) + O(h^{2\alpha'-2\alpha}).
\end{equation}
%
Ainsi, comme pour le \RefTheorem{thm:convergence-curv-3d}, nous devons trouver
le meilleur $\alpha$ possible. Les exposants étant les mêmes qu'en dimension 2, nous
obtenons $\alpha = \frac{\gamma}{2 + \gamma}$ si $\alpha' \ge \frac{3 \gamma}{1 +
\gamma}$, sinon $\alpha = \frac{\alpha'}{3}$.
%
Si le point $\vxH$ est sur le bord digital $\Bd{\Body{\DSh}{h}}$, nous
savons que $\alpha'=1$ grâce à la relation $t \le \frac{\sqrt{3}}{2}h$, $\gamma =
1$ dans le cas général, alors nous obtenons $\alpha = \frac{\alpha'}{3} =
\frac{1}{3}$. Donc, lorsque $R = kh^{\frac{1}{3}}$ :
%
\begin{equation}
  | \MeanCurvH{R}(\DSh,\vxH) - \MeanCurv(\Shape,\vx) | \le O(h^{\frac{1}{3}})
\end{equation}
%
\end{proof}

Dans les prochains paragraphes, nous allons nous intéresser à calculer les
courbures principales sur des objets digitaux, en calculant des matrices de
covariance sur la surface de l'objet. Comme pour la courbure en dimension 2 et
la courbure moyenne en dimension 3, nous chercherons à prouver la convergence
asymptotiques de ces estimateurs.

\subsection{Analyse digitale de la matrice de covariance et
relation aux courbures principales en 3D}
\label{sec:ii-3d}
%
Nous souhaitons maintenant calculer les moments sur l'intersection entre la forme
$\Shape$ et la boule, \cad à $\Ball{R}{\vx} \cap \Shape$, dont la taille décroît
avec le pas de discrétisation $h$. Nous ne pouvons pas utiliser directement les résultats
de l'\RefEquation{eq:MomentsByCounting-conv} car les constantes cachées dans le
terme en grand « O » dépendent de la taille de la forme, de son échelle et de la
courbure maximale. Nous devons normaliser l'estimation des moments de telle sorte
que le terme d'erreur ne soit plus influencé par l'échelle. Comme en 2D, cela
revient à se rapporter à la sphère unité. Pour plus de lisibilité, nous noterons
$\sigma = p + q + s$ :
%
\begin{align} \label{eq:MomentsOnUnitaryBall}
%
  \DMom{pqs}{h}(\DigF{\Ball{R}{\vx} \cap \Shape}{h},h) &= h^{3+\sigma}\Mom{pqs}\left
  ( (\frac{1}{h} \cdot \Ball{R}{\vx} \cap \Shape) \cap \Z^3 \right) \nonumber \\
  &= h^{3+{\sigma}} \Mom{pqs}\left( \frac{R}{h}\cdot(
  \Ball{1}{\frac{1}{R} \cdot \vx} \cap \frac{1}{R} \cdot \Shape) \cap \Z^3 \right) \nonumber \\
  &= R^{3+{\sigma}} \left( \frac{h}{R} \right)^{3+{\sigma}} \hspace{-0.3cm} \Mom{pqs} \left( \DigF{ \Ball{1}{\frac{1}{R}\cdot \vx}\cap\frac{1}{R}\cdot \Shape}{h/R} \right) \nonumber\\
  &= R^{3+{\sigma}}\DMom{pqs}{h}\left( \DigF{ \Ball{1}{\frac{1}{R}\cdot \vx}\cap\frac{1}{R}\cdot \Shape}{h/R}, {\frac{h}{R}} \right).
%
\end{align}
%
Ainsi, la forme $\Ball{1}{\frac{1}{R} \cdot \vx}\cap\frac{1}{R}\cdot \Shape$
tend vers la demi-boule de rayon $1$ lorsque $R$ décroît. Par conséquent, nous
pouvons appliquer l'\RefEquation{eq:MomentsByCounting-conv} à
l'\RefEquation{eq:MomentsOnUnitaryBall} et considérer à présent que la constante
impliquée dans le terme d'erreur n'est plus liée à $R$ ni à $h$. Comme $\Mom{pqs}(R \cdot Y) = R^{3+p+q+s} \Mom{pqs}(Y)$, nous
pouvons écrire :
%
\begin{align} \label{eq:MomentsOnUnitaryBall-conv}
%
  \DMom{pqs}{h}(\DigF{\Ball{R}{\vx} \cap \Shape}{h},h) &= R^{3+\sigma}
  \Mom{pqs}\left( \Ball{1}{\frac{1}{R}\cdot \vx} \cap \frac{1}{R}\cdot \Shape
  \right) + R^{3+\sigma} O \left( \frac{h}{R} \right)^{\mu_{{\sigma}}}  \nonumber \\
   &= \Mom{pqs}( \Ball{R}{\vx} \cap \Shape ) + O( R^{3+{\sigma}-\mu_\sigma} h^{\mu_\sigma}).
%
\end{align}
%
Alors, l'\RefEquation{eq:MomentsOnUnitaryBall-conv} est le résultat de la
convergence asymptotique des moments digitaux des sous-ensembles $\Ball{R}{\vx}
\cap \Shape$ lorsque $R$ et $h$ décroissent.
%
\subsubsection{Matrice de covariance digitale et estimateurs digitaux de courbures principales}
%
De la même façon que pour la matrice de covariance, nous définissons la matrice
de covariance digitale au pas de discrétisation $h$ pour tout sous-ensemble
$\DigShape \subset \Z^3$ en fonction des moments digitaux d'ordre $0$, $1$ et $2$ :
%
\begin{definition}{\fakeTitle{Matrice de covariance digitale}} \label{def:DigCovMatrix-def}
%
  Pour tout sous ensemble $\DigShape \subset \Z^3$, sa matrice de covariance
  digitale $\DCov{h}$ au pas de discrétisation $h$ est :
%
  \begin{align}
%
    \DCov{h}(Z) \EqDef &\left\lbrack
        \begin{array}{ccc}
          \DMom{200}{h}(\DigShape) & \DMom{110}{h}(\DigShape) & \DMom{101}{h}(\DigShape)\\
          \DMom{110}{h}(\DigShape) & \DMom{020}{h}(\DigShape) & \DMom{011}{h}(\DigShape)\\
          \DMom{101}{h}(\DigShape) & \DMom{011}{h}(\DigShape) & \DMom{002}{h}(\DigShape)
        \end{array}
        \right\rbrack
        - \frac{1}{\DMom{000}{h}(\DigShape)}
        \left\lbrack
        \begin{array}{c}
          \DMom{100}{h}(\DigShape) \\
          \DMom{010}{h}(\DigShape) \\
          \DMom{001}{h}(\DigShape) \\
        \end{array}
        \right\rbrack
        \otimes
        \left\lbrack
        \begin{array}{c}
          \DMom{100}{h}(\DigShape) \\
          \DMom{010}{h}(\DigShape) \\
          \DMom{001}{h}(\DigShape) \\
        \end{array}
        \right\rbrack^T.
%
  \end{align}
%
\end{definition}
%
En suivant l'approximation de Taylor du \RefLemma{lem:pottmann-3d}, nous pouvons
définir les estimateurs digitaux de courbures principales à partir de la
diagonalisation de la matrice de covariance digitale :
%
\begin{definition}
  \label{def:principal-curv-estimators}
%
  Soit $\DigShape \subset \Z^3$ une forme digitale et $h > 0$ un pas de
  discrétisation. Pour $R \ge h$, nous définissons les estimateurs digitaux par
  intégration de courbures principales $\PrincCurvH{1}{R}$ et
  $\PrincCurvH{2}{R}$ de $\DigShape$ au point $\vy \in \R^3$ et au pas de
  discrétisation $h$, et les estimateurs digitaux par intégration de leurs
  directions principales de courbure respectives $\PrincDirH{1}{R}$ et
  $\PrincDirH{2}{R}$, ainsi que l'estimateur digital de normale par intégration
  $\NormalDirH{R}$ comme :
%
\begin{align}
  \PrincCurvH{1}{R}(\DigShape,\vy,h)  &\EqDef \frac{6}{\pi R^6}(\hat{\lambda}_2 - 3\hat{\lambda}_1) + \frac{8}{5R},
  &\PrincDirH{1}{R}(\DigShape,\vy,h) &\EqDef \hat{\nu}_1 \\
  \PrincCurvH{2}{R}(\DigShape,\vy,h) &\EqDef \frac{6}{\pi R^6}(\hat{\lambda}_1 - 3\hat{\lambda}_2) + \frac{8}{5R},
  &\PrincDirH{2}{R}(\DigShape,\vy,h) &\EqDef \hat{\nu}_2\\
  & &\NormalDirH{R}(\DigShape,\vy,h) &\EqDef\hat{\nu}_3\,,
\end{align}
%
où $\hat{\lambda}_1 \ge \hat{\lambda}_2 \ge \hat{\lambda}_3$ sont les valeurs
propres de $\DCov{h}(\Ball{R/h}{\vy / h} \cap \DigShape)$, et $\hat{\nu}_1,
\hat{\nu}_2, \hat{\nu}_3$ sont leurs vecteurs propres correspondantes.
%
\end{definition}
%
Dans les paragraphes suivants nous allons montrer la convergence asymptotique de
ces estimateurs. La démonstration dépend de la convergence des moments digitaux,
de la stabilité des moments face à des petits déplacements provoqués par la
discrétisation, ainsi que de la théorie de perturbation de matrice.
%
\subsubsection{Propriétés des matrices de covariance et de moments}

Dans un premier temps, nous allons étudier les propriétés des matrices de
covariance et des moments face aux perturbations. Nous pouvons
établir l'invariance des matrices de covariances :
%
\begin{lemma} \label{lem:covariance-translation-invariant}
%
  Invariance à la translation pour les matrices de covariance :
%
  \begin{itemize}
    \item $\forall Y \subset \R^3$, $\forall \vv \in \R^3$, $\Cov(Y + \vv) = \Cov(Y)$.
    \item $\forall Z \subset \Z^3$, $\forall \vv \in \Z^3$, $\forall h > 0$, $\DCov{h}(Z + \vv) = \DCov{h}(Z)$.
  \end{itemize}
%
\end{lemma}
\begin{proof}
  La preuve est directe par additivité des moments géométriques.
\end{proof}

Nous devons également étudier comment les moments sont perturbés par une erreur de positionnement $\vt$ de la boule :
%
\begin{lemma} \label{lem:moments-ball-position-error}
%
  Pour tout point $\vx \in \R^3$, un rayon $R$
  positif, pour tout vecteur $\vt$ avec pour norme $t \EqDef \| \vt \|_2 \le R$,
  nous avons :
%
  \begin{align}
    \left| \Mom{000}(\Ball{R}{\vx + \vt} \cap \Shape) - \Mom{000}(\Ball{R}{\vx} \cap \Shape) \right|
    & \le O(t R^2), \label{eq:zero-moments-ball-position-error} \\
    \left| \Mom{100}(\Ball{R}{\vx + \vt} \cap \Shape) - \Mom{100}(\Ball{R}{\vx} \cap \Shape) \right|
    & \le O(t R^3) + O( \|\vx\|_\infty t R^2 ), \label{eq:first-moments-ball-position-error}\\
    \left| \Mom{200}(\Ball{R}{\vx + \vt} \cap \Shape) - \Mom{200}(\Ball{R}{\vx} \cap \Shape) \right|
    & \le O(t R^4) + O( \|\vx\|_\infty t R^3 ) + O( \|\vx\|^2_\infty t R^2 ). \label{eq:second-moments-ball-position-error}
  \end{align}
%
  Les bornes sont valides pour tous les moments ayant respectivement le même
  ordre. Ces relations restent vraies si $\Ball{R}{\vx+\vt}$ et
  $\Ball{R}{\vx}$ intersectent le même ensemble $\Shape$.
%
\end{lemma}
\begin{proof}

\begin{figure}[ht]
  \begin{center}
    \setlength{\tabcolsep}{0.0pt}
    \begin{tabular}{@{}c c@{}}
      \begin{overpic}[width=6.5cm]{figures/momentsPerturbation}
        \put(51,27){$\vx$}
        \put(40,40){$\vx+\vt$}
        \put(5,70){$\Ball{R}{\vx+\vt}$}
        \put(86,30){$\Ball{R}{\vx}$}
        \put(60,0){\tiny $(\Ball{R}{\vx} \setminus \Ball{R}{\vx+\vt} )\cap \Shape$}
        \put(22,18){\tiny$(\Ball{R}{\vx+\vt} \setminus \Ball{R}{\vx} )\cap \Shape$}
      \end{overpic}
      &
      \begin{overpic}[width=6.5cm]{figures/momentsPerturbation2}
        \put(51,32){$\vx$}
        \put(40,45){$\vx+\vt$}
        \put(5,73){$\Ball{R}{\vx+\vt}$}
        \put(90,33){$\Ball{R}{\vx}$}
      \end{overpic}
    \end{tabular}
    \caption[Illustration du \RefLemma{lem:moments-ball-position-error}.]{Illustration du \RefLemma{lem:moments-ball-position-error}.}
      \label{fig:moments-perturb}
  \end{center}
\end{figure}

En décomposant $\Shape$ en fonction de deux boules de rayon R, centrées en $\vx$
et $\vx + \vt$ (voir \RefFigure{fig:moments-perturb}), nous avons :
%
\begin{align}
  \Delta(\vt) \EqDef \Mom{pqs}&(\Ball{R}{\vx+\vt} \cap \Shape ) - \Mom{pqs}(\Ball{R}{\vx} \cap \Shape ) \\
    &= \Mom{pqs}( (\Ball{R}{\vx+\vt} \setminus \Ball{R}{\vx}) \cap \Shape ) \\
    & \quad - \Mom{pqs}( (\Ball{R}{\vx} \setminus \Ball{R}{\vx+\vt}) \cap \Shape )
\end{align}
%
Comme :
%
\begin{equation}
  \emptyset \neq Y_1 \subset Y_2 \subset \R^3 \Implies \sup_{ Y \subset Y_1} \left | \Mom{pqs}(Y) \right| \le \sup_{Y \subset Y_2} \left| \Mom{pqs}(Y)\right |,
\end{equation}
%
et avec la propriété d'additivité des intégrales, nous avons :
%
\begin{align}
  |\Delta(\vt)| &\EqDef |\Mom{pqs}( (\Ball{R}{\vx+\vt} \ominus \Ball{R}{\vx}) \cap \Shape )| \\
  &\le \sup_{Y \subset (\Ball{R}{\vx+\vt} \ominus \Ball{R}{\vx}) \cap \Shape} |\Mom{pqs}(Y)|\\
  &\le \sup_{Y \subset \Ball{R}{\vx+\vt} \ominus \Ball{R}{\vx}} |\Mom{pqs}(Y)|\\
  &\le \sup_{Y \subset \Ball{R+t}{\vx} - \Ball{R-t}{\vx}} |\Mom{pqs}(Y)|.
\end{align}
%
Pour une meilleure lisibilité, nous noterons $S_{R,t}(\vx) \EqDef
\Ball{R+t}{\vx} \setminus \Ball{R-t}{\vx}$. Nous couvrons alors la différence
entre les deux boules centrées en des points différents ($\Ball{R}{\vx+\vt}
\ominus \Ball{R}{\vx}$) par la différence entre deux boules de même centre mais
de rayons différents ($S_{R,t}(\vx)$). \comJeremy{La
\RefFigure{fig:moments-perturb} illustre la différence des deux boules, en
bleu.} Cela correspond à un anneau en 2D de largeur $2t$. Il s'agit évidemment
d'une borne supérieure épaisse, mais les ordres de perturbations sont les mêmes.


\noindent\textbf{Moment d'ordre zéro.\quad}
%
Pour le moment d'ordre zéro, nous utilisons simplement le volume de la boule :
%
\begin{align}
  \sup_{Y \subset S_{R,t}(\vx)} |\Mom{000}(Y)|
  &= \Mom{000}( S_{R,t}(\vx) ) \\
  &= \frac{4\pi}{3}(R+t)^3 - \frac{4\pi}{3}(R-t)^3 \\
  &= \frac{4\pi}{3}(6R^2t+2t^3)=O(tR^2).
\end{align}


\noindent\textbf{Moments du premier ordre.\quad}
%
Pour les moments du premier ordre, nous translatons la forme à l'origine du
repère et nous utilisons le résultat précédent ainsi que le fait que le
$100$-moment centré est maximisé pour la demi-boule à valeur de $x_1$ positive
(que nous noterons $\HalfBall{R}{\vx}$)\footnote{Rappelons que $x_i$ est la
$i$-ème composante de $\vx$.} :
%
\begin{align}
  \sup_{Y \subset S_{R,t}(\vx)} |\Mom{100}(Y)|
  &\le \sup_{Y \subset S_{R,t}(\mathbf{0})} |\Mom{100}(Y)| + |x_1|\cdot\Mom{000}(Y)\nonumber \\
  &= \Mom{100}( \HalfBall{R+t}{\mathbf{0}} - \HalfBall{R-t}{\mathbf{0}} ) + O(|x_1|tR^2) \nonumber \\
  &= 2\pi(R^3t +Rt^3) + O( (\| \vx \|_\infty) tR^2) \nonumber \\
  &= O(tR^3) + O(\| \vx \|_\infty tR^2).
\end{align}


\noindent\textbf{Moments du second ordre.\quad}
%
Pour les moments du second ordre, nous translatons également la forme à
l'origine du repère, puis nous utilisons les deux résultats précédents ainsi que
le fait que le $200$-moment est maximisé pour la boule :
%
\begin{align}
  \sup_{Y \subset S_{R,t}(\vx)} |\Mom{200}(Y)| &
  \le \sup_{Y \subset S_{R,t}(\mathbf{0})} |\Mom{200}(Y)| + 2|x_1|\cdot |\Mom{100}(Y)| + x_1^2\cdot\Mom{000}(Y) \nonumber \\
  & = \Mom{200}(S_{R,t}(\mathbf{0})) + \|\vx\|_\infty (O(tR^3) + O(\|\vx\|_\infty tR^2))+  \|\vx\|_\infty^2 O(tR^2) \nonumber \\
  & = O(tR^4) +O(\|\vx\|_\infty tR^3) +O(\|\vx\|_\infty^2 tR^2)
\end{align}
%
Les autres moments sont prouvés de la même façon.
%
\end{proof}

\noindent Dans le prochain paragraphe, nous allons injecter ces erreurs de
positionnement des moments dans le calcul de la matrice de covariance afin de
voir comment ils influent sur la convergence de la matrice.

\subsubsection{Convergence asymptotique des matrices de covariances digitales}
%
Avec le \RefLemma{lem:covariance-translation-invariant} et le
\RefLemma{lem:moments-ball-position-error}, nous pouvons facilement prouver la
convergence asymptotique des matrices de covariances digitales. Le théorème
suivant établit la convergence lorsque la matrice de covariance est calculée sur
le même point $\vy$, et le suivant établira sa convergence asymptotique sur le
bord digital de la forme.
%
\begin{theorem}{\fakeTitle{Convergence de la matrice de covariance digitale \cite{CVIU2014}}}
  \label{thm:conv-cov-matrix}
%
  Soit $\Shape$ une forme convexe de $\R^3$ à reach positif, dont le bord est
  $C^3$ et peut être décomposé en nombre fini de parties monotones. Alors, il
  existe une constante $h_0$ telle que pour tout pas de discrétisation $0 < h <
  h_0$, pour tout $\vy \in \R^3$, pour un rayon $R \ge h$, nous
  avons\footnote{$\mu_i \ge 1$ d'après
  l'\RefEquation{eq:MomentsByCounting-conv}} :
%
  \begin{equation}
    \left \| \DCov{h}( \Ball{R/h}{\vy/h} \cap  \DSh ) - \Cov( \Ball{R}{\vy} \cap \Shape )\right \| \le \sum_{i=0}^2 O(R^{5-\mu_i}h^{\mu_i}).
  \end{equation}
%
  La constante cachée derrière le terme en grand « O » ne dépend pas de la
  taille de la forme ni de sa géométrie. $\|\cdot\|$ est la norme spectrale des
  matrices.
%
\end{theorem}
\begin{proof}
%
  Afin de faciliter la lecture, nous noterons $A \EqDef \Ball{R}{\vy} \cap
  \Shape$ et $A_h \EqDef \Ball{R/h}{\vy/h} \cap \DSh = \DigF{\Ball{R}{\vy} \cap
  \Shape}{h}$ (au sens de la discrétisation de Gauss). Nous commençons par
  translater les ensembles $A$ et $A_h$ centrés en $\vy$ vers l'origine du
  repère. Nous devons alors utiliser un vecteur qui tient compte de la
  discrétisation puisque nous décalons $A_h$ par le vecteur $\Rounded{\vy}{h}$
  (vecteur de nombres entiers le plus proche de $\frac{\vy}{h}$) et nous
  décalons $A$ par le vecteur $h\Rounded{\vy}{h}$. Nous noterons alors
  $\tilde{A} \EqDef A -
  h\Rounded{\vy}{h}$ et $\tilde{A}_h \EqDef A_h - \Rounded{\vy}{h}$. Avec ces deux définitions et la propriété d'invariance des
  matrices de covariances digitales
  (\RefLemma{lem:covariance-translation-invariant}), nous obtenons :
%
  \begin{align}
    \DCov{h}( \Ball{R/h}{\vy/h} \cap \DSh )
    &= \DCov{h}( A_h )\\
    &= \DCov{h}\left( \tilde{A}_h + \Rounded{\vy}{h}\right )\\
    &= \DCov{h}( \tilde{A}_h )\\
    &= \DCov{h}( \DigF{\tilde{A}}{h} ). \label{eq:DigitalCovarianceMatrix-conv-proof1}
  \end{align}
%
  La dernière égalité vient du fait que $\tilde{A}_h = \DigF{A}{h} -
  \Rounded{\vy}{h} = \DigF{A - h\Rounded{\vy}{h}}{h} = \DigF{\tilde{A}}{h}$.
%
  Nous faisons de même avec la matrice de covariance continue :
%
  \begin{align}
    \Cov( \Ball{R}{\vy} \cap \Shape )
    &= \Cov( A ) \\
    &= \Cov\left( \tilde{A} + h\Rounded{\vy}{h} \right) \\
    &= \Cov( \tilde{A} ). \label{eq:DigitalCovarianceMatrix-conv-proof2}
  \end{align}
%
  Alors, nous pouvons réécrire l'erreur d'estimation de la matrice de covariance
  digitale comme :
%
  \begin{align}
    \| \DCov{h}( \Ball{R/h}{\vy/h} \cap \DSh ) - \Cov( \Ball{R}{\vx} \cap \Shape ) \|
    = \| \DCov{h}( \DigF{\tilde{A}}{h} ) - \Cov( \tilde{A} ) \|. \label{eq:DigitalCovarianceMatrix-conv-proof3}
  \end{align}
%
  Détaillons maintenant $\DCov{h}( \DigF{\tilde{A}}{h} )$ :
%
  \begin{equation}
    \DCov{h}( \DigF{\tilde{A}}{h} )
    = \left [
    \begin{array}{ccc}
      \DMom{200}{h}(\tilde{A}_h) & &\\
      & \ddots & \\
    \end{array}
    \right ]% \nonumber \\ &
    - \frac{1}{\DMom{000}{h}(\tilde{A}_h)}
    \left [ \begin{array}{c}
      \DMom{100}{h}(\tilde{A}_h) \\ \vdots
    \end{array} \right ]
    \otimes
    \left [ \begin{array}{c}
      \DMom{100}{h}(\tilde{A}_h) \\ \vdots
    \end{array} \right ]^T.
  \end{equation}
%
  En utilisant les résultats de l'\RefEquation{eq:MomentsOnUnitaryBall-conv},
  nous obtenons :
%
\begin{align}
  \DCov{h}( \DigF{\tilde{A}}{h} )
  &= \left [
  \begin{array}{ccc}
    \Mom{200}(\tilde{A}) + O(R^{5 - \mu_2} h^{\mu_2}) & &\\
    & \ddots & \\
  \end{array}
  \right ] \nonumber \\
  & \quad- \frac{1}{\Mom{000}(\tilde{A}) + O(R^{3 - \mu_0} h^{\mu_0})}
  \left [ \begin{array}{c}
    (\Mom{100}(\tilde{A}) + O(R^{4 - \mu_1} h^{\mu_1}))^2 \\ \vdots
  \end{array} \right ].
\end{align}
%
  Il est à noter que les constantes dans les termes en grand « O » sont
  indépendantes de $\Shape$ grâce à la normalisation. La précédente équation
  nous donne $\Cov( \tilde{A} )$ et des termes d'erreurs sur les moments. Nous
  allons alors chercher les bornes supérieures de ces moments dans le calcul de
  $\DCov{h}( \DigF{\tilde{A}}{h} )$ en sachant que le rayon $R$ est supérieur à
  $h$. Puisque $\tilde{A}$ n'est pas vide et est proche de l'origine, nous
  pouvons alors utiliser les
  \RefEquations{eq:ball-moment-1}{eq:ball-moment-1bis} du
  \RefLemma{lem:moment-ball} pour l'ensemble $\tilde{A} \subset \Ball{R}{t}$,
  pour les points $\vt = \vx - h\left[\frac{\vx}{h}\right]$, en sachant que $||\vt||_\infty
  \le \frac{h}{2}$. Nous obtenons alors :
  %
  \begin{equation}
    \DCov{h}( \DigF{\tilde{A}}{h} ) = \Cov(\tilde{A}) + O(R^{5-{\mu_2}}h^{\mu_2}) + O(R^{5-{\mu_0}}h^{\mu_0}) + O(R^{5-{\mu_1}}h^{\mu_1}).
  \end{equation}
  %
  Puisque $\Cov(\tilde{A})=\Cov(A-h\Rounded{x}{h})=\Cov(A)$ grâce à la propriété d'invariance à la translation des matrice de covariance du \RefLemma{lem:covariance-translation-invariant} :
  %
  \begin{equation}
    \DCov{h}( \DigF{\tilde{A}}{h} ) = \Cov(A) + O(R^{5-{\mu_2}}h^{\mu_2}) + O(R^{5-{\mu_0}}h^{\mu_0}) + O(R^{5-{\mu_1}}h^{\mu_1}).
  \end{equation}
%
\end{proof}


Comme pour le calcul de la courbure en 2D ou courbure moyenne en 3D, afin d'obtenir un
résultat de convergence asymptotique, nous devons placer notre estimateur
digital sur le bord digital de la forme, ajoutant ainsi une erreur de positionnement.
Cela équivaut à estimer les moments à une position $\vx + \vt$, et pour cela
nous allons nous servir des précédents lemmes.
%
\begin{theorem}{\fakeTitle{Convergence asymptotique de la matrice de covariance digitale \cite{CVIU2014}}}
\label{thm:multigrid-conv-cov-matrix}
%
  Soit $\Shape$ une forme convexe de $\R^3$ à reach positif, dont le bord est
  $C^3$ et peut être décomposé en nombre fini de parties monotones. Alors, il
  existe une constante $h_0$ telle que pour tout pas de discrétisation $0 < h <
  h_0$, pour tout $\vx \in \R^3$, pour un rayon $R \ge h$, nous
  avons\footnote{$\mu_i \ge 1$ d'après
  l'\RefEquation{eq:MomentsByCounting-conv}} :
%
  \begin{align}
    &\forall 0 < h < h_0, \quad \forall \vx \in \Bd{\Shape}, \quad
    \forall \hat{\vx} \in \Bd{\Body{\DSh}{h}} \text{~avec~} \| \hat{\vx} -\vx\|_\infty \le h, \nonumber \\
    &\left\| \DCov{h}( \Ball{R/h}{\hat{\vx}/h} \cap  \DSh ) - \Cov( \Ball{R}{\vx} \cap \Shape )\right \|\le O(\|\vx-\hat{\vx}\| R^4) + \sum_{i=0}^2 O(R^{5-\mu_i}h^{\mu_i}),
  \end{align}
  %
  La constante cachée derrière le terme en grand « O » ne dépend pas de la
  taille de la forme ni de sa géométrie. $\|\cdot\|$ est la norme spectrale des
  matrices.
  %
\end{theorem}
\begin{proof}
  %
  %Pour plus de facilité de lecture, nous allons noter $A \EqDef \Ball{R}{\vx}
  %\cap \Shape$ et $A_h \EqDef \Ball{R/h}{\hat{\vx}/h} \cap \DSh = \DigF{\Ball{R}{\hat{\vx}}
  %\cap \Shape}{h}$.
  Le fait que $\| \vx - \hat{\vx} \|_\infty \le h \le R$ implique que
  $\Ball{R}{\vx} \cap \Shape$ et $\Ball{R}{\hat{\vx}} \cap \Shape$ ne sont pas
  vides. Nous séparons alors la différence des matrices en deux parties :
  %
  \begin{align}
  &\| \DCov{h}( \Ball{R/h}{\hat{\vx}/h} \cap  \DigF{\Shape}{h} ) - \Cov( \Ball{R}{\vx} \cap \Shape ) \| \\ \nonumber
  & \quad \le  \| \DCov{h}( \Ball{R/h}{\hat{\vx}/h} \cap  \DigF{\Shape}{h} ) - \Cov( \Ball{R}{\hat{\vx}} \cap \Shape ) \|
  + \| \Cov( \Ball{R}{\hat{\vx}} \cap \Shape ) - \Cov( \Ball{R}{\vx} \cap \Shape ) \|.
 \end{align}
  %
  Nous avons borné le premier terme par $\sum_{i=0}^2 O(R^{5-\mu_i}h^{\mu_i})$
  dans le \RefTheorem{thm:conv-cov-matrix} si nous l'appliquons au point
  $\hat{\vx}$. Pour le second terme, nous définissons $\vt \EqDef \hat{\vx} -
  \vx$, $t \EqDef \| \vt \|_2$, and $\Shape' \EqDef \Shape - \vx$. Nous
  utilisons alors la propriété d'invariance à la translation des matrices de
  covariance (\RefLemma{lem:covariance-translation-invariant}) afin de déplacer
  les matrices à l'origine du repère :
  %
  \begin{align}
    &\| \Cov( \Ball{R}{\hat{\vx}} \cap \Shape ) - \Cov( \Ball{R}{\vx} \cap \Shape )\|
    = \| \Cov( \Ball{R}{\vx + \vt} \cap \Shape ) - \Cov( \Ball{R}{\vx} \cap \Shape )\| \\
    & \quad = \| \Cov( \Ball{R}{\vx + \vt} \cap \Shape - \vx ) - \Cov( \Ball{R}{\vx} \cap \Shape - \vx )\| \\
    & \quad = \| \Cov( (\Ball{R}{\vx + \vt} - \vx) \cap (\Shape - \vx) ) - \Cov( (\Ball{R}{\vx} - \vx) \cap (\Shape - \vx) )\| \\
    & \quad = \| \Cov( \Ball{R}{\vt} \cap (\Shape - \vx) ) - \Cov( \Ball{R}{\mathbf{0}} \cap (\Shape - \vx) )\| \\
    & \quad = \| \Cov( \Ball{R}{\vt} \cap \Shape' ) - \Cov( \Ball{R}{\mathbf{0}} \cap \Shape' )\|.
  \end{align}
  %
  Nous appliquons maintenant le \RefLemma{lem:moments-ball-position-error} pour
  les différents moments qui interviennent dans la matrice de covariance $\Cov$.
  Pour une facilité de lecture, nous noterons $Y_{\vt}$ l'ensemble
  $\Ball{R}{\vt} \cap \Shape'$ et $Y_{\mathbf{0}}$ l'ensemble
  $\Ball{R}{\mathbf{0}} \cap \Shape'$ :
  %
  \begin{align}
   \| \Cov( Y_{\vt} ) - \Cov(Y_{\mathbf{0}})\|
   &= \left\| \left\lbrack \begin{array}{cc}
       \Mom{200}(Y_{\vt}) - \Mom{200}(Y_{\mathbf{0}}) & \\
       & \ddots
     \end{array} \right\rbrack
   \right. \nonumber\\
  & - \frac{1}{\Mom{000}(Y_{\vt})}
   \left\lbrack \begin{array}{c}
     \Mom{100}(Y_{\vt}) \\
     \vdots
   \end{array} \right\rbrack
   \otimes
   \left\lbrack \begin{array}{c}
     \Mom{100}(Y_{\vt}) \\
     \vdots
   \end{array} \right\rbrack^T
   \nonumber\\
  & \left.
   + \frac{1}{\Mom{000}(Y_{\mathbf{0}})}
   \left\lbrack \begin{array}{c}
     \Mom{100}(Y_{\mathbf{0}}) \\
     \vdots
   \end{array} \right\rbrack
   \otimes
   \left\lbrack \begin{array}{c}
     \Mom{100}(Y_{\mathbf{0}}) \\
     \vdots
   \end{array} \right\rbrack^T
   \right\|.
  \end{align}

  La matrice $\Cov( Y_{\vt} ) - \Cov(Y_{\mathbf{0}})$ contient alors la
  différence des moments géométriques du second ordre comme par exemple $
  \Mom{200}(Y_{\vt}) - \Mom{200}(Y_{\mathbf{0}})$ et des quantités de la forme
  de $\Delta \EqDef \frac{\Mom{100}(Y_{\vt})^2}{\Mom{000}(Y_{\vt})} -
  \frac{\Mom{100}(Y_{\mathbf{0}})^2}{\Mom{000}(Y_{\mathbf{0}})}$ si nous
  décomposons les termes de droite. D'après le
  \RefLemma{lem:moments-ball-position-error}, nous savons que les erreurs des
  moments du second ordre sont bornées en $O(t R^4)$. Pour borner $\Delta$, nous
  savons déjà $|\Mom{000}(Y_{\mathbf{t}}) - \Mom{000}(Y_{\mathbf{0}})| = \pi
  R^2(t+O(t^2)+O(tR^2))$ en utilisant le \RefTheoremFake{7}{Pottmann2009}.
  Alors, nous obtenons :
  %
  \begin{align}
    \Delta &= \frac{\Mom{100}(Y_{\vt})^2}{\Mom{000}(Y_{\mathbf{0}}) + O(tR^2)} -
    \frac{\Mom{100}(Y_{\mathbf{0}})^2}{\Mom{000}(Y_{\mathbf{0}})}\\
    &= O(tR^2)\frac{\Mom{100}(Y_{\vt})^2}{\Mom{000}(Y_{\mathbf{0}})^2} +
    \frac{\Mom{100}(Y_{\vt})^2 - \Mom{100}(Y_{\mathbf{0}})^2}{\Mom{000}(Y_{\mathbf{0}})}\,.
  \end{align}
  %
  Puisque $\frac{a}{b+O(x)}=\frac{a}{b}+\frac{a}{b^2}O(x)$, $a^2-b^2=(a-b)(a+b)$
  et en utilisant de nouveau le \RefLemma{lem:moments-ball-position-error}, nous
  avons :
  %
  \begin{align*}
    \Delta
    &= O(tR^4) + (\Mom{100}(Y_{\vt}) + \Mom{100}(Y_{\mathbf{0}}))\frac{\Mom{100}(Y_{\vt}) - \Mom{100}(Y_{\mathbf{0}})}{\Mom{000}(Y_{\mathbf{0}})} \\
    &= O(tR^4) + (O(tR^3)+O(R^4))\frac{\Mom{100}(Y_{\vt}) - \Mom{100}(Y_{\mathbf{0}})}{\Mom{000}(Y_{\mathbf{0}})}
  \end{align*}
  %
  Puisque $t < R$ nous obtenons $\Mom{000}(Y_{\mathbf{0}}) = \Theta(R^3)$, si nous
  utilisons le \RefLemma{lem:moments-ball-position-error}, nous avons :
  %
  \begin{equation}
    \Delta = O(tR^4) + (O(tR^3)+O(R^4))\frac{O(tR^3)}{\Mom{000}(Y_{\mathbf{0}})} = O(tR^4).
  \end{equation}
  %
  Les mêmes bornes sont trouvées pour tous les termes de la matrice. Lorsqu'on
  les rassemble toutes, nous obtenons le résultat du théorème.
  %
  \end{proof}

\subsubsection{Théorie de perturbation de matrices}
%
Nous venons de montrer que la matrice de covariance digitale tend vers la
matrice de covariance continue lorsque nous la calculons à partir de
l'intersection de la forme avec une boule. Nous avons défini des estimateurs de
courbures principales et de directions principales de courbure à partir des
valeurs propres et des vecteurs propres de la matrice de covariance digitale
dans le \RefSection{def:principal-curv-estimators}. Afin de pouvoir obtenir des
preuves de convergence asymptotique, il est nécessaire de comprendre comment une
erreur sur une matrice peut perturber sa décomposition. C'est pour cela que nous
allons dans un premier temps présenter un résultat de la théorie de perturbation
de matrices \cite{Bauer1960,Stewart1990,Bhatia1997}.


Soit $M$ et $M'$ deux matrice symétriques, nous voulons alors quantifier la
différence entre les valeurs propres (\respp les vecteurs propres) de $M$ et les
valeurs propres (\resp les vecteurs propres) de $M'$ comme des fonctions de la
norme de $M - M'$. Par exemple, si $M'$ est la matrice de covariance sur des
données bruitées et $M$ la matrice de covariance sur des données non bruitées,
les résultats de perturbation de matrice pourront nous indiquer les bornes
d'erreurs des valeurs propres (et ainsi celles de l'estimation des courbures
principales).
%
\begin{theorem}{\fakeTitle{Inégalité de Lidskii-Weyl}}
  \label{thm:lidskii-weyl}
  %
  Si $\lambda_i(M)$ désigne la $i$-ième valeur propre ordonnée d'un matrice
  symétrique $M$ et $\lambda_i(M')$ la $i$-ième valeur propre ordonnée d'un matrice
  symétrique $M'$, alors :
  %
  \begin{equation}
    \max_{i}| \lambda_i(M) - \lambda_i(M')| \le \|M-M'\| .
  \end{equation}
  %
\end{theorem}
%
Dans notre contexte où les valeurs propres sont utilisées pour estimer les
courbures principales, nous pouvons espérer une convergence de ces valeurs. Le
prochain paragraphe va répondre à cette interrogation.
%
\subsubsection{Convergence asymptotique des estimateurs digitaux de courbures principales}
%
La convergence asymptotique des matrices de covariances digitales et les
résultats de stabilité de la théorie de perturbation des matrices induisent la
convergence asymptotique des estimateurs de courbures principales par
intégration :
%
\begin{theorem}{\fakeTitle{Convergence asymptotique des estimateurs digitaux de courbures principales $\PrincCurvH{1}{R}$ et $\PrincCurvH{2}{R}$ \cite{CVIU2014}}}
\label{thm:multigrid-convergence-curv-k1k2}
%
  Soit $\Shape$ une forme convexe de $\R^3$ à reach positif telle que son bord
  $\Bd{X}$ est $C^3$ et peut être décomposée en nombre fini de parties monotones.
  Alors, pour la discrétisation de Gauss, les estimateurs digitaux de courbures
  principales $\PrincCurvH{1}{R}$ et $\PrincCurvH{2}{R}$ convergent
  asymptotiquement vers les courbures principales $\PrincCurv{1}$ et
  $\PrincCurv{2}$ pour des constantes positives $h_0,
  k$ telles que, pour tout pas de discrétisation $h \le h_0$, en définissant le
  rayon de la boule $R = kh^{\frac{1}{3}}$, nous avons :
  %
  \begin{align}
     \forall \vx \in \Bd{\Shape}, \quad \forall \hat{\vx} \in \Bd{\Body{\DSh}{h}}, \text{~avec~} \| \hat{\vx} -\vx\|_\infty \le h,\nonumber\\
     | \PrincCurvH{1}{R}(\DSh,\hat{\vx},h) - \PrincCurv{1}(\Shape,\vx) | \le O(h^{\frac{1}{3}}) ,\\
     | \PrincCurvH{2}{R}(\DSh,\hat{\vx},h) - \PrincCurv{2}(\Shape,\vx) | \le O(h^{\frac{1}{3}}) .
  \end{align}
  %
\end{theorem}
\begin{proof}
  %
  Nous allons prouver la convergence asymptotique pour l'estimateur digital de la
  première courbure principale. Le raisonnement est le même pour la seconde
  courbure principale. D'après la \RefDefinition{def:principal-curv-estimators},
  $\hat{\lambda}_1$ et $\hat{\lambda}_2$ sont les deux plus grandes valeurs
  propres de $\DCov{h}(\Ball{R/h}{\vx / h} \cap \DigShape)$ avec
  $Z=\DigF{X}{h})$. Nous désignons $M' \EqDef \DCov{h}(\Ball{R/h}{\hat{\vx}/h}
  \cap \DSh)$ et $M \EqDef \Cov(\Ball{R}{\vx} \cap \Shape )$. Comme $M$ et $M'$
  sont symétriques par définition, le \RefTheorem{thm:lidskii-weyl} implique que
  les valeurs propres de $M$ et $M'$ sont proches, avec les mêmes bornes. Il est à
  noter que puisque les bornes d'erreurs tendent vers zéro lorsque $h$ tend vers
  zéro, l'ordre des valeurs propres des deux matrices est le même pour un $h$
  suffisamment petit. Alors, nous pouvons écrire :
  %
  \begin{align}
    &\PrincCurvH{1}{R}(\DSh,\hat{\vx},h) = \frac{6}{\pi R^6}(\hat{\lambda}_2 - 3\hat{\lambda}_1) + \frac{8}{5R} \\
    &\quad = \frac{6}{\pi R^6} \left( \lambda_2 - 3\lambda_1 + O(\|\vx-\hat{\vx}\| R^4) + \sum_{i=0}^2 O(R^{5-\mu_i}h^{\mu_i}) \right) + \frac{8}{5R}.
  \end{align}
  %
  Alors, en substituant avec l'approximation de Taylor du
  \RefLemma{lem:pottmann-3d}, nous obtenons :
  %
  \begin{equation} \label{eq:error-term-conv-mean-curv-est}
    \PrincCurvH{1}{R}(\DSh,\hat{\vx},h) = \PrincCurv{1}(\Shape,\vx) + O(R) + O( h/R^2) + \sum_{i=0}^2 O(h^{\mu_i}/R^{1+\mu_i}) ,
  \end{equation}
  %
  avec $\mu_i \ge 1$ pour la forme $\Shape$ (voir
  \RefEquation{eq:MomentsByCounting-conv}). En donnant pour valeur $R =
  kh^\alpha$, $k$ étant une constante, la valeur optimale pour $\alpha$
  minimisant le plus les erreurs est $\frac{1}{3}$. Ainsi, nous obtenons :
  %
  \begin{equation}
    \PrincCurvH{1}{R}(\DSh,\hat{\vx},h) = \PrincCurv{1}(\Shape,\vx) + O(h^{\frac{1}{3}}).
  \end{equation}
  %
\end{proof}

Ces bornes d'erreurs peuvent être améliorées sur les
voisinages où la courbure gaussienne n'est pas nulle. Alors, les constantes
$\mu_i$ se rapprochent de $1.5$. De plus, une meilleure estimation de la
distance entre $\vx$ et $\hat{\vx}$ permettrait d'améliorer les vitesses de
convergence.


Des résultats de convergence asymptotique sur l'estimation digitale des
directions principales de courbure et de la normale peuvent être également
apportés. Les détails sont expliqués dans \cite{ChapterIICurvature}. La preuve
est un peu plus compliquée que pour les courbures principales car des problèmes
interviennent aux points ombilic de la surface, \cad lorsque les deux courbures
principales sont égales. De ce fait, pour tous ces points, les deux valeurs
propres de la matrice de covariance coïncident et leurs vecteurs propres
couvrent un plan. Cependant, nous pouvons écrire :
%
\begin{theorem}{\fakeTitle{Convergence asymptotique des estimateurs digitaux de
directions principales de courbure par intégration $\PrincDirH{1}{R}$ et
$\PrincDirH{1}{R}$, et de l'estimateur digital des normales par intégration
\NormalDirH{R} \cite{ChapterIICurvature}.}}
\label{thm:multigrid-conv-principal-directions-and-normal}
  %
  Soit $\Shape$ une forme convexe de $\R^3$ telle que son bord $\Bd{\Shape}$ a un
  reach supérieur à $\rho$ et est $C^3$.
  %
  \\
  %
  Alors, pour la discrétisation de Gauss, les estimateurs digitaux de directions
  principales de courbure par intégration convergent asymptotiquement vers les
  directions principales de courbure $\PrincDir{1}$ et $\PrincDir{2}$ sur des
  formes $\Shape$ pour des pas de discrétisation $h$ et rayon $R$ donnant des
  courbures principales distinctes. L'estimateur digital des normales par
  intégration converge également vers la normale $\NormalDir$. Plus précisément,
  en donnant $R=k h^{\frac{1}{3}}$ avec $k$ une constante positive arbitraire,
  nous avons :
  %
  \begin{align}
    &\exists h_\Shape \in \R^+,\,\, \forall h \in \R,\,\, 0 < h < h_\Shape,\,\,
     \,\,\nonumber\quad \quad\quad \quad\\
    &\forall \vx \in \Bd{\Shape}, \quad \forall \hat{\vx} \in \Bd{\Body{\DSh}{h}}  \text{~avec~} \| \hat{\vx} -\vx\|_\infty
    \le h, \nonumber\quad \quad\quad \quad \\
    &\quad \quad\quad \quad\| \PrincDirH{1}{R}( \DSh, \hat{\vx}, h ) - \PrincDir{1}(\Shape, \vx) \|
    \le \frac{1}{|\PrincCurv{1}(\Shape,\vx)-\PrincCurv{2}(\Shape,\vx)|} O(h^{\frac{1}{3}})\,, \\
    &\quad \quad\quad \quad\| \PrincDirH{2}{R}( \DSh, \hat{\vx}, h ) - \PrincDir{2}(\Shape, \vx) \|
    \le \frac{1}{|\PrincCurv{1}(\Shape,\vx)-\PrincCurv{2}(\Shape,\vx)|} O(h^{\frac{1}{3}})\,, \\
    &\quad \quad\quad \quad\| \NormalDirH{R}( \DSh, \hat{\vx}, h ) - \NormalDir(\Shape, \vx) \|
    \le O(h^{\frac{2}{3}})\,.
  \end{align}
\end{theorem}
%
Les vecteurs propres issus de la décomposition de la matrice de covariance ne
sont pas orientés. Afin de pouvoir comparer les directions principales de
courbure, celles-ci doivent être orientées dans la même direction afin d'éviter
les erreurs angulaires provoqués par cette absence d'orientation. La convergence
asymptotique est alors valide si nous avons au préalable orienté les vecteurs dans
la même direction.


La démonstration complète est donnée dans \cite{ChapterIICurvature}. L'intuition
de la preuve est d'analyser l'écart entre deux valeurs propres successives («
\anglais{eigengap} ») afin de pouvoir borner l'erreur d'estimation
des vecteurs propres. Cet écart joue un rôle important dans la variation des
vecteurs propres puisque plus sa valeur est petite, plus il y a de risques en
cas de perturbation de la surface d'obtenir des rotations. Enfin, l'utilisation
du \RefTheoremFake{3}{Pottmann2007} sur la proximité des vecteurs propres de la
matrice de covariance et des directions de courbures principales permet de
conclure à une convergence asymptotique.
%
\subsection{Résultats sur l'estimation de la courbure par intégration volumique de la boule}
%
Avant de conclure sur cette méthode, nous allons montrer quelques résultats des
estimateurs. Une comparaison expérimentale complète sera proposée dans le
\RefSection{sec:comparaison-courbure}.

Les estimateurs sont définis sur des formes $\Shape$ de la famille $\Shapes$ des
formes convexes à bord $C^3$ et à courbure bornée positive, cependant nous
voulons tester également sur d'autres formes ne rentrant pas dans ce cadre. Nous
avons choisi l'objet digital \textsc{OctaFlower} de dimension $256^3$. L'unique
paramètre de nos estimateurs est le rayon de la boule : nous avons choisi $R=15$
pour toutes les images des
\RefFigures{fig:digital-II-octa}{fig:digital-II-octa-2}. La carte des couleurs
pour les estimateurs de courbure moyenne, gaussienne et courbures principales va
du bleu (valeur minimale) au jaune (valeur maximale).

\begin{figure}[ht]
\begin{center}
  \includegraphics[width=.40\linewidth]{images/Curvature/Octa256_Mean_R_15_0001}
  \includegraphics[width=0.1cm,height=5.5cm]{images/YMTB6W}
  \includegraphics[width=.40\linewidth]{images/Curvature/Octa256_Gauss_R_15_0001}
  \includegraphics[width=0.1cm,height=5.5cm]{images/YMTB6W}\\
  \includegraphics[width=.40\linewidth]{images/Curvature/Octa256_k1_R_15_0001}
  \includegraphics[width=0.1cm,height=5.5cm]{images/YMTB6W}
  \includegraphics[width=.40\linewidth]{images/Curvature/Octa256_k2_R_15_0001}
  \includegraphics[width=0.1cm,height=5.5cm]{images/YMTB6W}\\
\end{center}\vspace{-0.5cm}
  \caption[Estimateurs différentiels digitaux par intégration sur une surface digitale (\textsc{OctaFlower} de dimension $256^3$.]{Estimateurs différentiels digitaux par intégration sur une surface digitale (\textsc{OctaFlower} de dimension $256^3$) avec $R = 15$. \emph{De gauche à droite, de haut en bas :} courbure moyenne, courbure gaussienne, première et seconde courbures principales. \label{fig:digital-II-octa}}
\end{figure}

\begin{figure}[ht]
\begin{center}
  \includegraphics[width=.2645\linewidth]{images/Curvature/Octa256_dir1_R_15_0001}
  \includegraphics[width=.2645\linewidth]{images/Curvature/Octa256_dir2_R_15_0001}
  \includegraphics[width=.2645\linewidth]{images/Curvature/octa-256-normal}\\
  \includegraphics[width=.40\linewidth]{images/Curvature/octa-256-prindir1-zoom}
  \includegraphics[width=.40\linewidth]{images/Curvature/octa-256-prindir2-zoom}
\end{center}\vspace{-0.5cm}
  \caption[Estimateurs différentiels digitaux par intégration sur une surface digitale (\textsc{OctaFlower} de dimension $256^3$.]{Estimateurs différentiels digitaux par intégration sur une surface digitale (\textsc{OctaFlower} de dimension $256^3$) avec $R = 15$. \emph{De gauche à droite, de haut en bas :} première et seconde direction principale de courbure et normale (zoom sur la dernière ligne). \label{fig:digital-II-octa-2}}
\end{figure}

\section{Estimation sans paramètre de la courbure par intégration volumique de la boule}
\label{sec:curvature:parameter-free}
%
Dans ce paragraphe, nous allons proposer une version sans paramètre des
estimateurs de courbure du \RefSection{sec:estimators:volume}. Ces travaux ont
été publiés dans \cite{DGCI2014}. Dans le contexte de la convergence
asymptotique, la notion d'échelle est explicitement donnée grâce au paramètre de
discrétisation $h$. C'est d'ailleurs pour cela que les rayons de boules doivent
être en relation avec $h$ ($R$ est en $O(h^\frac{1}{3})$). Cependant, pour un
objet digital donné, ce choix de rayon nécessite d'étudier au préalable la forme
dont nous souhaitons estimer la courbure, et la qualité d'estimation dépend de
ce paramètre. La \RefFigureN{fig:digital-II-octa-scale} montre le résultat de
l'estimation de la courbure moyenne et de la courbure gaussienne avec différents
rayons $R$ pour la forme \textsc{OctaFlower}.

\begin{figure}[ht]
\begin{center}
  \includegraphics[width=.30\linewidth]{images/Curvature/Octa256_Mean_R_15_0001}
  \includegraphics[width=.30\linewidth]{images/Curvature/Octa256_Mean_R_25_0001}
  \includegraphics[width=.30\linewidth]{images/Curvature/Octa256_Mean_R_30_0001}\\
  \includegraphics[width=.30\linewidth]{images/Curvature/Octa256_Gauss_R_15_0001}
  \includegraphics[width=.30\linewidth]{images/Curvature/Octa256_Gauss_R_25_0001}
  \includegraphics[width=.30\linewidth]{images/Curvature/Octa256_Gauss_R_30_0001}
\end{center}\vspace{-0.5cm}
  \caption{Estimateurs différentiels digitaux par intégration sur une surface digitale (\textsc{OctaFlower} de dimension $256^3$) avec trois rayons différents : $R = 15, 25, 30$. \emph{De haut en bas}, courbure moyenne, courbure gaussienne. \label{fig:digital-II-octa-scale}}
\end{figure}

Nous voulons alors automatiquement détecter ce paramètre de rayon pour des
objets digitaux $\DigShape$ de dimension $2$ et $3$ tout en garantissant les
propriétés de convergence asymptotique si ceux-ci respectent les conditions de
convergence des estimateurs de courbure du \RefSection{sec:estimators:volume}.
La technique est simple : nous devons extraire les informations géométriques de
la forme afin de pouvoir retrouver quel est le paramètre asymptotique $h$ pour
la forme donnée. Pour ce faire, nous utiliserons les segments maximaux digitaux décris
dans le \RefSection{sec:segments}.
%
%\subsection{Propriétés sur les segments maximaux}
%
\subsection{Estimateur digital de courbure sans paramètre en 2D}

Dans le \RefSection{sec:segments} nous avons rappelé les propriétés des
longueurs des segments maximaux sur les bords d'objets digitaux
(\RefLemma{lem:law-length-MDSS}). Nous allons tirer parti de ces propriétés afin
de définir un nouvel estimateur de courbure sur des objets digitaux
$\DigShape \subset \Z^2$. Nous allons reprendre l'estimateur
digital de courbure défini dans la \RefDefinition{def:digital-2d-curvature}.
Celui-ci nécessite un rayon $R$ qui doit être en $O(h^{\frac{1}{3}})$ afin
d'avoir une convergence asymptotique de la courbure. Nous allons exploiter les
primitives digitales décritent dans le \RefSection{sec:segments}, en particulier
des segments maximaux de droite digitale et de leurs lois asymptotiques.

Nommons $L_D({\DigShape})$ la longueur moyenne des segments maximaux sur
$\DigShape$ et $\rho(\DigShape) = L_D^2({\DigShape})$ le carré de cette
longueur.  Alors, définissons un estimateur digital de courbure sans
paramètre en 2D $\CurvHS$ pour tout pointel $\vp \in \BdZ{\DigShape}$ (le
\RefSection{sec:notions-base} introduit la notion de pointel et plus
généralement la topologie de Khalimsky qui sera utilisée ici) :
%
\begin{definition}
  \label{def:curvature-estimator-2d-pf}
  %
  Soit une forme $\DigShape \subset \Z^2$. L'estimateur digital de courbure
  sans paramètre en 2D $\CurvHS$ au pointel $\vp \in \BdZ{\DigShape}$ est défini
  comme :
  %
  \begin{equation}
    \CurvHS( \DigShape, \vp) \EqDef \frac{3\pi}{2 \rho(\DigShape)} - \frac{3 A(\DigShape,\vp)}{{\rho(\DigShape)}^3}
  \end{equation}
  %
  où $\rho(\DigShape) =L_D^2(\DigShape)$ et $A(\DigShape,\vp) =
  Card(\Ball{\rho(\DigShape)}{\vp} \cap \DigShape)$.
  %
\end{definition}
%
Pour paraphraser cette définition, nous devons dans un premier temps calculer la
moyenne des longueurs des segments discret sur le bord $\BdZ{\DigShape}$ de
$\DigShape$. Nous élevons au carré cette longueur moyenne afin de donner $\rho$.
Ainsi, l'estimateur $\CurvHS$ est une fonction du nombre de points
digitaux\footnote{C'est un abus de langage désignant le centre de la cellule de
Khalimsky, \cad le spel, d'après le choix de plongement dont nous avons
parlé dans le \RefSection{sec:notions-base}.} de $\DigShape$ intersecté avec une
boule de rayon $\rho$ centrée au pointel $\vp$. Cette version de l'estimateur de
la \RefDefinition{def:digital-2d-curvature} s'adapte alors à l’échelle de la
forme.


Si nous souhaitons rendre compatible l'estimateur dans le cadre de la
convergence asymptotique, afin qu'il puisse se comparer avec des données de
l'espace euclidien, nous devons y appliquer un facteur d'échelle du pas de
discrétisation $h$. En effet, comme nous l'avons dit précédemment, l'estimateur ne contient pas d'information d'échelle. De plus, pour $\DigShape =
\DSh$, avec $\rho(\DigShape) = L^2_D(\DigShape)$, nous avons (d'après le
\RefLemma{lem:law-length-MDSS}) :
%
\begin{align}
  \Theta(h^{-\frac{1}{3}}) \le L_D( \DigShape ) \le \Theta\left(h^{-\frac{1}{3}} \log \left(\frac{1}{h}\right)\right) \\
  \Theta(h^{-\frac{2}{3}}) \le L^2_D( \DigShape ) \le \Theta\left(h^{-\frac{2}{3}} \log^2 \left(\frac{1}{h}\right)\right) \\
  \Theta(h^{\frac{1}{3}}) \le hL^2_D( \DigShape ) \le \Theta\left(h^{\frac{1}{3}} \log^2 \left(\frac{1}{h}\right)\right) \label{eq:bounds-length-MDSS}.
\end{align}
%


Ainsi, à l'exception du terme en $\log^2(\cdot)$, $h \rho(\DigShape)$ semble être
un candidat idéal comme paramètre de rayon $R$, puisqu'il suit
$\Theta(h^\frac{1}{3})$, pour obtenir un estimateur digital de
courbure sans paramètre.
%
\begin{theorem}{\fakeTitle{Convergence asymptotique de l'estimateur digital de courbure sans paramètre $\CurvHS$}}
\label{thm:curvature-estimator-2d-pf-conv}
  %
  Soit $\Shape$ une forme convexe de $\R^2$ telle que son bord $\Bd{\Shape}$ est
  $C^3$ et sa courbure bornée non nulle. Soit $\DigShape = \DSh$. Alors, avec une
  constante positive $h_0$, nous avons :
  %
  \begin{align}
    &\forall 0 < h \le h_0, \quad \forall \vx \in \dS, \quad \forall \vp \in \BdZ{\DigShape} \text{~avec~} \| h\vp -\vx\|_\infty \le h, \nonumber\\
    &\left | \frac{1}{h}\CurvHS(\DigShape,\vp) - \Curv(\Shape,\vx) \right| \le O\left( h^{\frac{1}{3}} \log^2 \left(\frac{1}{h}\right)\right) .
    %
  \end{align}
  %
\end{theorem}
%
Il est à noter que $\vp \in \BdZ{\DigShape}$ implique que $h\vp \in
\Bd{\Body{\DSh}{h}}$.


\begin{proof}
  %
  Dans un premier temps, nous allons décomposer
  $\frac{1}{h}\CurvHS(\DigShape,\vp)$ :
  %
  \begin{align}
    \frac{1}{h}\CurvHS(\DigShape,\vp) &= \frac{3\pi}{2 h\rho(\DigShape)} - \frac{3 A(\DigShape,\vp)}{h{\rho(\DigShape)}^3} \\
    &= \frac{3\pi}{2 h\rho(\DigShape)} - \frac{3~ Card(\Ball{\rho(\DigShape)}{\vp} \cap \DSh)}{h{\rho(\DigShape)}^3} \\
    &= \frac{3\pi}{2 h\rho(\DigShape)} - \frac{3~ Card( \Ball{(h\rho(\DigShape))/h}{\frac{1}{h} \cdot (h\vp)} \cap \DSh, h)}{h{\rho(\DigShape)}^3} \\
    &= \CurvH{R}(\DSh,\hat{\vx},h)\quad \text{(avec $R \EqDef h\rho(\DigShape)$ et $\hat{\vx} \EqDef h\vp$)}
  \end{align}
  %
  Il suffit alors de borner simplement $| \CurvH{R}(\DSh,\hat{\vx},h) -
  \Curv(\DSh,\vx) |$ en tenant compte du comportement asymptotique de $R \EqDef
  h\rho(\DigShape)$. D'après l'\RefEquation{eq:bounds-length-MDSS}, $r$ est
  compris entre deux bornes :
  \begin{itemize}
    %
    \item Si $R = \Theta(h^{\frac{1}{3}})$, nous sommes exactement dans les
    hypothèses du \RefTheorem{thm:convergence-curv-2d}, et le terme d'erreur
    est alors en $O( h^\frac{1}{3} )$.
    %
    \item Si $R = \Theta(h^{\frac{1}{3}} \log^2 \left(\frac{1}{h}\right))$, nous
    décomposons le terme d'erreur du \RefTheorem{thm:convergence-curv-2d} en
    utilisant l'\RefEquation{eq:shift-curvhat-error-bound-h-only}\footnote{En
    tenant compte que $\alpha' = 1$ et $\beta = 1$ dans le cas général} :
    %
    \begin{align}
      |\CurvH{R}(\DSh,\hat{\vx},h) - \Curv(\Shape,\vx)| & \le O(h\rho(\DigShape)) + O\left(\frac{h}{(h\rho(\DigShape))^{2}}\right) \nonumber\\
      &\quad + O\left(\frac{h}{(h\rho(\DigShape))^{2}}\right)( 1 + O((h\rho(\DigShape))^2) + O(h)) \\
      & \le O(h^{\frac{1}{3}} \log^2 (\frac{1}{h})) + O\left(\frac{h^\frac{1}{3}}{\log^4 (\frac{1}{h})}\right) + O( h ) \nonumber\\
      &\quad + O\left(\frac{h^\frac{4}{3}}{\log^4 (\frac{1}{h})}\right)
    \end{align}
    %
    Alors le terme d'erreur dominant de cette expression est $O(h^{\frac{1}{3}}
    \log^2 (\frac{1}{h}))$.
    %
  \end{itemize}
  %
  Ainsi, nous allons borner par le terme d'erreur dominant des deux
  possibilités. Puisque
  $\frac{1}{h}\CurvHS(\DigShape,\vp)=\CurvH{R}(\DSh,\hat{\vx},h)$, nous pouvons
  conclure que son erreur est en $O(h^{\frac{1}{3}} \log^2 (\frac{1}{h}))$.
  %
\end{proof}


De ce fait, nous avons un estimateur digital de courbure convergent, ne nécessitant aucun paramètre de rayon. Le paramètre d'échelle $h$ est présent
seulement pour déterminer l'unité utilisée pour mesurer les courbures comme
l'explique la \RefFigure{fig:2d-parameter-free-explained}.

\begin{figure}[ht]
  \begin{center}
    \input{figures/tikz/2d-parameter-free-explained.tikz}
  \end{center}
  \caption[Illustration de la dépendance du paramètre d'échelle $h$.]
  %
  {Illustration de la dépendance du paramètre d'échelle $h$. L'objet euclidien
  de gauche donne le même objet digital que l'objet euclidien de droite alors
  qu'ils sont à des échelles différentes. La courbure à la surface de l'objet
  dépend alors de $h$.\label{fig:2d-parameter-free-explained}}
  %
\end{figure}

Nous utilisons un unique rayon pour estimer la courbure sur tout le bord de
l'objet. En sachant que nous pouvons extraire des faisceaux de segments maximaux
en chaque point du bord de l'objet, pouvons-nous modifier notre estimateur afin
de la taille de la boule s'adapte localement à la géométrie de l'objet ? En
étant adaptatif, nous pouvons espérer obtenir de meilleures vitesses de
convergence en moyenne (erreur en norme $l_2$) de l'estimateur. Le paragraphe
suivant traite cette question.

\subsection{Estimateur digital local de courbure sans paramètre en 2D}

Nous allons noter $\rho(\DigShape,\vp)$ la longueur moyenne du faisceau de
segments maximaux au point $\vp$ élevé au carré. Nous définissons alors une
version locale de l'estimateur de la
\RefDefinition{def:curvature-estimator-2d-pf} :
%
\begin{definition}
  %
  Soit une forme $\DigShape \subset \Z^2$. L'estimateur digital local sans
  paramètre de courbure en 2D au pointel $\vp \in \BdZ{\DigShape}$ est défini par :
  %
  \begin{equation}
    \CurvHSL(\DigShape, \vp) \EqDef \frac{3\pi}{2 \rho(\DigShape,\vp)} - \frac{3 A'(\DigShape,\vp)}{{\rho(\DigShape,\vp)}^3}
  \end{equation}
  %
  où $A'(\DigShape, \vp) = Card(\Ball{\rho(\DigShape,\vp)}{\vp} \cap
  \DigShape)$.
  %
\end{definition}
%
Pour résumer, nous calculons le faisceau de segments maximaux en tout point
$\vp$ du bord de $\DigShape$, puis nous calculons la moyenne de leurs longueurs.
Cette longueur au carré nous donne la taille à utiliser pour la boule localement
pour le point $\vp$. La conséquence de ceci est d'avoir une plus grosse boule
sur les parties à faible courbure et de diminuer la taille de la boule sur les
zones fortement courbées. La \RefFigure{fig:curvature-pf-radii} montre la
différence entre $\CurvHS$ et $\CurvHSL$ : dans sa version locale, le rayon est
adapté localement à la géométrie de la forme.

\begin{figure}[ht]{
  \begin{center}
    \includegraphics[width=.95\linewidth]{images/Curvature/ScaleSpace_Flower_Global}
    \includegraphics[width=.95\linewidth]{images/Curvature/ScaleSpace_Flower_Local}
    %\includegraphics[width=.95\linewidth]{images/Curvature/ScaleSpace_Flower_Local_5}
  \end{center}}
  %
  \caption{Valeur de courbure estimée en fonction du rayon $R$ sur le bord de
  l'objet \textsc{AccFlower} allant du bleu (valeur minimale) au jaune (valeur
  maximale). \emph{En abscisse :} position sur la surface de l'objet, \emph{en
  ordonnée :} différents rayons $R$. \emph{En noir sur l'image du haut :} rayon
  choisi par l'estimateur $\CurvHS$. \emph{En noir sur l'image du bas :}
  différents rayons choisis par l'estimateur $\CurvHSL$ en fonction de la
  géométrie locale de la forme.
  %
  \label{fig:curvature-pf-radii}}
  %
\end{figure}

Dans cette version locale, quelques segments maximaux peuvent avoir une longueur
trop importante, ce qui nous empêche d'obtenir des preuves de convergence
asymptotique. En effet, si, dans le faisceau de segments maximaux, les longueurs
sont globalement dans l'intervalle $\Theta(h^{-\frac{1}{3}}) \le L_D(
{\DigShape} ) \le \Theta(h^{-\frac{1}{3}} \log \left(\frac{1}{h}\right))$, un
bon comportement asymptotique peut être envisagé pour cet estimateur local.
Cependant, des problèmes interviennent avec les segments maximaux les plus longs : à
cause de leur borne supérieur en $O(h^{-\frac{1}{2}})$ aucune convergence ne peut
être attendue. Néanmoins, l'évaluation expérimentale (voir
\RefSection{sec:comparaison-courbure}) montre de bonnes propriétés de
convergence asymptotique et nous pouvons observer que la version locale
$\CurvHSL$ est plus performante que $\CurvHS$.
%
\subsection{Estimateur digital de courbures sans paramètre en 3D}
%
Intéressons-nous maintenant à la version en 3D de ces estimateurs sans paramètre.
Nous devons adapter l'utilisation des longueurs des segments maximaux à des
objets $\DigShape \subset \Z^3$. Comme le montre la
\RefFigure{fig:3d-dig-object-slices}, nous proposons d'utiliser des coupes de
l’objet afin d'extraire des contours 2D, et ainsi pouvoir calculer les segments
maximaux.

\begin{figure}[ht]
    \begin{center}
      \includegraphics[width=10cm]{images/Curvature/ctopo3dSurfelCut}
    \end{center}
    \caption[Coupes sur un objet digital 3D.]{Coupes sur un objet digital 3D dans deux directions alignées sur les axes.}
    \label{fig:3d-dig-object-slices}
\end{figure}

\noindent Formalisons cela avec une proposition sur les variétés lisses :
%
\begin{proposition}
\label{prop:slices-3d}
  %
  Soit un objet $\Shape \subset \R^3$ avec un bord $C^2$ sur lequel la valeur
  absolue des courbures principales (notées respectivement $\PrincCurv{1}$ et
  $\PrincCurv{2}$) est bornée par une constante $K$. La normale de $\dS$ au
  point $\vx$ est notée $\vec{\NormalDir}(\vx)$. Pour tout $\vx \in \dS$, on
  considère le plan $\pi_{\vec{\ve}}(\vx)$ contenant $\vx$ et perpendiculaire à
  un vecteur canonique $\vec{\ve}$ de la base de $\R^3$ (\emph{i.e.}
  $\vec{\ve}\in\{ \vec{\vx}_1, \vec{\vx}_2, \vec{\vx}_3 \}$).
  %
  \\
  %
  Soit $\dS_{\vec{\ve}}(\vx)$ le bord de l'intersection $\Shape
  \cap \pi_{\vec{\ve}}(\vx)$. Alors, il y a \textbf{au moins} deux ensembles parmi
  $\dS_{\vec{\vx}_1}(\vx)$, $\dS_{\vec{\vx}_2}(\vx)$ et
  $\dS_{\vec{\vx}_3}(\vx)$ qui sont localement des courbes dont la
  courbure est bornée par $\sqrt{2}K$ en valeur absolue.
  %
\end{proposition}
\begin{proof}
  %
  Afin d'alléger la lecture, nous avons retiré les $(\vx)$ de toutes les
  notations puisqu'il ne peut y avoir d’ambiguïté. La
  \RefFigure{fig:slices-3d-proof} illustre les notations utilisées.
  %
  \begin{figure}[ht]
    \begin{center}
      \begin{overpic}[width=10cm]{figures/Slice_3D}
      \put(40.5,48){$\pi_{\vec{\vx}_2}$}
      \put(95,50){$\pi_{\vec{\vx}_2}$}

      \put(53,19.5){$\dS$}

      \put(33,26){$\vx$}
      \put(80,30){$\vx$}

      \put(5,22.5){$\textcolor{blue}{\gamma_{\PrincCurv{2}}}$}
      \put(36,10){$\textcolor{blue}{\gamma_{\PrincCurv{1}}}$}

      \put(69,27){$\gamma$}
      \put(23,15){$\gamma$}

      \definecolor{darkgreen}{rgb}{0.,0.7,0.}

      \put(29,8){\textcolor{darkgreen}{$\chi$}}
      \put(31,43){\textcolor{darkgreen}{$\pi_{\vec{\NormalDir} \times \vec{\vt}}$}}

      \put(24,33){$\textcolor{red}{\vec{\NormalDir}}$}

      \put(33,35){$\textcolor{darkgreen}{\vec{\vt}(0)}$}
      \put(85,30){$\textcolor{darkgreen}{\vec{\vt}(0)}$}
      %\put(80,24){$\textcolor{black}{\vec{\vu}}$}

      \put(52,48){$\textcolor{gray}{\vec{\vx}_1}$}
      \put(51.5,44){$\textcolor{gray}{\vec{\vx}_2}$}
      \put(47,51.5){$\textcolor{gray}{\vec{\vx}_3}$}
      \end{overpic}
    \end{center}
    %
    \caption{Notations pour la \RefProposition{prop:slices-3d}.}
    %
    \label{fig:slices-3d-proof}
  \end{figure}
  %
  Tout d'abord, si un des plans $\pi_{\vec{\vx}_1}$, $\pi_{\vec{\vx}_2}$ ou
  $\pi_{\vec{\vx}_3}$ est le plan tangent à $\vx$, les deux autres sont des plans
  normaux de $\dS$ au point $\vx$, \cad qu'ils contiennent la normale
  $\vec{\NormalDir}$. Dans ce cas, le théorème d'\cauthor{Euler}{Euler1760} sur
  la courbure des surfaces nous dit que toutes les courbes définies par
  l'intersection entre le plan normal et la surface $\dS$ ont une courbure
  $\Curv$ égale à :
  %
  \begin{equation}
    \PrincCurv{1} \cos^2 \theta + \PrincCurv{2} \sin^2 \theta ,
  \end{equation}
  %
  $\theta$ étant un angle. Il est alors immédiat que :
  %
  \begin{equation}
    |\Curv| \in \lbrack \min(|\PrincCurv{1}|,|\PrincCurv{2}|), \max(|\PrincCurv{1}|,|\PrincCurv{2}|) \rbrack ,
  \end{equation}
  %
  et est de ce fait borné par $K$ sur ces deux plans.
  %
  Ensuite, pour tout $\vec{\ve} \in \{ \vec{\vx}_1, \vec{\vx}_2, \vec{\vx}_3 \}$,
  l'ensemble $\dS_{\vec{\ve}}$ est localement une courbe 3D qui croise $\vx$ sur
  la surface $\dS$. Il y a au moins un $\vec{\ve} \in \{ \vec{\vx}_1,
  \vec{\vx}_2, \vec{\vx}_3 \}$ dont $\vec{\NormalDir} \cdot \vec{\ve} \ge
  \frac{\sqrt{2}}{2}$. En effet, notons la normale $\vec{\NormalDir} = a
  \vec{\vx}_1 + b \vec{\vx}_2 + c \vec{\vx}_3$ et supposons que, par exemple,
  $\vec{\NormalDir} \cdot \vec{\vx}_1 \ge \frac{\sqrt{2}}{2}$, alors :
  %
  \begin{equation}
    b^2 + c^2 = 1 - a^2 = 1 - (\vec{\NormalDir} \cdot \vec{\vx}_1)^2 < \frac{1}{2} .
  \end{equation}
  %
  De ce fait, $b = \vec{\NormalDir} \cdot \vec{\vx}_2$ et $c = \vec{\NormalDir}
  \cdot \vec{\vx}_3$ sont plus petits que $\frac{\sqrt{2}}{2}$. Dans la suite,
  nous considérons seulement un vecteur $\vec{\ve} \in \{ \vec{\vx}_1,
  \vec{\vx}_2, \vec{\vx}_3 \}$ dont $\vec{\NormalDir} \cdot \vec{\ve} \le
  \frac{\sqrt{2}}{2}$.
  %
  \\
  %
  Soit $\chi$ une courbe définie comme l'intersection entre $\dS$ et le plan
  $\pi_{\vec{\NormalDir} \times \vec{\vt}}$ contenant $\vec{\NormalDir}$ et la
  tangente $\vec{\vt}$ au point $\vx$ de $\dS_{\vec{\ve}}$. D'après le
  Théorème de Meusnier \cite{Berger1992}, nous avons la relation suivante entre la
  courbure $\Curv_\chi$ de $\chi$ et la courbure $\Curv$ de $\dS_{\vec{\ve}}$ au
  point $\vx$ :
  %
  \begin{equation}
    \Curv_\chi = \Curv \cdot \cos \alpha
  \end{equation}
  %
  où $\alpha$ est l'angle entre les plans $\pi_{\vec{\ve}}$ et
  $\pi_{\vec{\NormalDir} \times \vec{\vt}}$. Puisque $\cos \alpha =
  \vec{\NormalDir} \cdot \vec{\ve}$ et $\frac{\sqrt{2}}{2} \leq \cos \alpha \leq
  1 $, nous avons au final :
  %
  \begin{equation}
    %
    |\Curv_\chi| \le |\Curv| \le \sqrt{2}|\Curv_\chi|.
    %
  \end{equation}
  %
  À nouveau, puisque $|\Curv_\chi|$ est compris entre $\lbrack
  \min(|\PrincCurv{1}|,|\PrincCurv{2}|),
  \max(|\PrincCurv{1}|,|\PrincCurv{2}|)\rbrack$,  il est borné par $K$ et ainsi
  nous obtenons le résultat final.
  %
\end{proof}

Nous allons définir une version sans paramètre de l'estimateur de courbure
moyenne $\MeanCurvH{R}$ (voir \RefDefinition{def:digital-3d-mean-curvature}).
Comme pour la version 2D sans paramètre, nous allons automatiquement calculer le
rayon $R$ en fonction de la géométrie de l'objet. Le rayon $\rho'(\DigShape)$ de
l'estimateur $\MeanCurvHS$ sera alors le carré de la moyenne de \emph{certaines}
longueurs de segments maximaux. Nous allons détailler plus précisément cela
dans les définitions suivantes (plus particulièrement à la \RefDefinition{def:rho-dig-3d}) :
%
\begin{definition}
  %
  Pour un objet digital $\DigShape \subset \Z^3$, l'estimateur digital de
  courbure moyenne sans paramètre $\MeanCurvHS$ au pointel $\vp \in
  \BdZ{\DigShape}$ est défini par :
  %
  \begin{equation}
    %
    \MeanCurvHS( \DigShape, \vp ) \EqDef \frac{8}{3 {\rho'(\DigShape)}} - \frac{4 V_{{\rho'(\DigShape)}}(\DigShape, \vp)}{\pi{{\rho'(\DigShape)}}^4} ,
    %
    \label{def:curvature-estimator-3d-mean-pf}
  \end{equation}
  %
  où $V_{\rho'(\DigShape)}(\DigShape, \vp) = Card(\Ball{\rho'(\DigShape)}{\vp} \cap \DigShape)$.
  %
\end{definition}
%
De plus, nous pouvons estimer les courbures principales, directions principales
et la normale de la même manière :
%
\begin{definition}
  %
  Soit $\DigShape$ une forme digitale de $\Z^3$, nous définissons les estimateurs
  digitaux sans paramètre de courbures principales $\PrincCurvHS{1}$ et
  $\PrincCurvHS{2}$, de directions principales de courbure $\PrincDirHS{1}$ et
  $\PrincDirHS{2}$ et de normale $\NormalDirHS$ au pointel $\vp \in
  \BdZ{\DigShape}$ de $\DigShape$ comme :
  %
  \begin{align}
      %
      \PrincCurvHS{1}(\DigShape, \vp)  &\EqDef \frac{6}{\pi (\rho'(\DigShape))^6}(\hat{\lambda}^{*}_2 - 3\hat{\lambda}^{*}_1) + \frac{8}{5 \rho'(\DigShape)},
      &\PrincDirHS{1}(\DigShape, \vp) &\EqDef \hat{\nu}^{*}_1 \\
      \PrincCurvHS{2}(\DigShape, \vp) &\EqDef \frac{6}{\pi (\rho'(\DigShape))^6}(\hat{\lambda}^{*}_1 - 3\hat{\lambda}^{*}_2) + \frac{8}{5 \rho'(\DigShape)},
      &\PrincDirHS{2}(\DigShape, \vp) &\EqDef \hat{\nu}^{*}_2\\
      & &\NormalDirHS(\DigShape, \vp) &\EqDef\hat{\nu}^{*}_3 \,,
      %
  \end{align}
  %
  où $\hat{\lambda}^{*}_1$ et $\hat{\lambda}^{*}_2$ sont les deux plus grandes
  valeurs propres de la matrice de covariance de $\Ball{{\rho'(\DigShape)}}{\vp}
  \cap \DigShape$, et $ \hat{\nu}^{*}_1$, $\hat{\nu}^{*}_2$ et $\hat{\nu}^{*}_3$
  ses vecteurs propres.
  %
  \label{def:curvature-estimator-3d-k1k2-pf}
\end{definition}
%
Précisons maintenant à quoi correspond exactement le paramètre
$\rho'(\DigShape)$. Nous apportons une définition pour sa version globale
$\rho'(\DigShape)$ ainsi que pour sa version locale $\rho'(\DigShape, \vp)$ pour
$\vp \in \BdZ{\DigShape}$ :
%
\begin{definition}
  \label{def:rho-dig-3d}
  %
  Pour un objet digital $\DigShape$, chaque surfel $\vp \in \BdZ{\DigShape}$ est
  orthogonal à deux coupes $\pi_{\vec{\ve}_1}(\vp)$ et $\pi_{\vec{\ve}_2}(\vp)$.
  Pour toutes les coupes $\pi_{\vec{\ve}_i}(\vp) \cap \DigShape$, le faisceau de
  segments maximaux recouvrant $\vp$ détermine un ensemble de nombres entiers
  $l_i(\vp)$, construit à partir des longueurs de ces segments maximaux. Enfin,
  nous désignons par $M(\vp)$ la coupe au point $\vp$ contenant le segment
  maximal le plus long, \cad la coupe $i$ pour laquelle l'ensemble $l_i(\vp)$
  contient le plus grand nombre entier. Alors, nous définissons :
  %
  \begin{itemize}
    %
    \item $\rho'(\DigShape)$ comme le carré de la moyenne des longueurs des segments maximaux pour toutes les coupes $\pi_{\vec{\ve}_i(\vp)} \cap \DigShape$ de $\DigShape$;
    %
    \item $\rho'(\DigShape, \vp)$ comme le carré de la moyenne des longueurs contenu dans $l_{M(\vp)}(\vp)$.
    %
  \end{itemize}
  %
\end{definition}
%
Comme en 2D, des cas pathologiques peuvent apparaître à cause du segment maximal
le plus long et de sa borne $h\rho'(\DigShape, \vp) \in \Theta(1)$ (cas plat).
Dans ces conditions, nous ne pouvons rien espérer en terme de convergence
asymptotique. Mais à nouveau, expérimentalement ce mauvais comportement n'est
pas observé (voir \RefSection{sec:comparaison-courbure}).
%
Supposons alors la conjecture suivante :
%
\begin{conjecture}
\label{conj:slice-mdss-3d}
  %
  Soit $\Shape$ une forme convexe de $\R^3$, avec un bord $C^3$ à courbure
  bornée. Soit $\DigShape \EqDef \DSh$, alors avec une constante positive $h_0$,
  nous avons :
  %
  \begin{equation}
    %
    \forall 0 < h \le h_0,\quad \Theta(h^{\frac{1}{3}}) \le h\rho'(\DigShape) \le \Theta(h^{\frac{1}{3}} \log^2 (\frac{1}{h})) \,.
    %
  \end{equation}
  %
\end{conjecture}
%
La logique de cette conjecture est la suivante : en coupant l'objet (convexe) dans
toutes les directions, avec la \RefProposition{prop:slices-3d}, nous savons
qu'au moins deux des trois coupes définissent des courbes convexes avec une
information de courbure bornée. Puisque deux coupes sont disponibles à partir
d'un surfel comme le montre la \RefFigure{fig:slices-3d-proof}, au moins une coupe
par surfel donne une courbe convexe à courbure bornée. Nous espérons alors qu'au
moins la moitié des longueurs des segments maximaux suivent les bornes de la
\RefConjecture{conj:slice-mdss-3d}. De plus, calculer la moyenne de ces
longueurs fournit une quantité stable et cohérente qui suit également les bornes
de la conjecture.
La \RefFigure{fig:bounds-length-MDSS-fig} confirme expérimentalement ces bornes.

\begin{figure}[ht]{
  \begin{center}
    \includegraphics[height=8cm]{images/Curvature/Ellipsoid_Radius}
  \end{center}}
    \caption{Comportement asymptotique (en échelle logarithmique) de $h\rho'( \DigShape )$ sur l'objet \Ellipsoid.
    \label{fig:bounds-length-MDSS-fig}}
\end{figure}


Si nous supposons la \RefConjecture{conj:slice-mdss-3d}, nous pouvons prouver
alors la convergence asymptotique des estimateurs digitaux sans paramètre de
courbure moyenne $\MeanCurvHS$, courbures principales $\PrincCurvHS{1}$ et
$\PrincCurvHS{2}$ au travers de ces deux observations :
%
\begin{observation}{\fakeTitle{Convergence asymptotique de l'estimateur digital de
courbure moyenne sans paramètre $\MeanCurvHS$}}
  %
  \label{obs:curvature-estimator-3d-mean-pf-conv}
  %
  Soit $\Shape$ une forme convexe de $\R^3$, avec un bord $C^3$ à
  courbure bornée non nulle. Soit $\DigShape \EqDef \DSh$. Alors, avec une
  constante positive $h_0$, nous avons :
  %
  \begin{align}
    &\forall 0 < h \le h_0, \quad \forall \vx \in \dS, \quad \forall \vp \in \BdZ{\DigShape} \text{~avec~} \| h\vp -\vx\|_\infty \le h, \nonumber\\
    &\left | \frac{1}{h}\MeanCurvHS(\DigShape,\vp) - \MeanCurv(\Shape,\vx) \right| \le O\left( h^{\frac{1}{3}} \log^2 \left(\frac{1}{h}\right)\right) \,.
    %
  \end{align}
  %
\end{observation}
\begin{proof}
  %
  En supposons la \RefConjecture{conj:slice-mdss-3d}, la démonstration est la
  même que pour le \RefTheorem{thm:curvature-estimator-2d-pf-conv}.
  %
\end{proof}

\begin{observation}{\fakeTitle{Convergence asymptotique des estimateurs digitaux sans paramètre de courbures principales $\PrincCurvHS{1}$ et
$\PrincCurvHS{2}$}}
  %
  \label{obs:curvature-estimator-3d-k1k2-pf-conv}
  %
  Soit $\Shape$ une forme convexe de $\R^3$, avec un bord $C^3$ à
  courbure bornée non nulle. Soit $\DigShape \EqDef \DSh$. Alors, avec une
  constante positive $h_0$, nous avons :
  %
  \begin{align}
    &\forall 0 < h \le h_0, \quad \forall \vx \in \dS, \quad \forall \vp \in \BdZ{\DigShape} \text{~avec~} \| h\vp -\vx\|_\infty \le h , \nonumber\\
    &\left | \frac{1}{h}\PrincCurvHS{1}(\DigShape,\vp) - \PrincCurv{1}(\Shape,\vx) \right| \le O\left( h^{\frac{1}{3}} \log^2 \left(\frac{1}{h}\right)\right),\\
    &\left | \frac{1}{h}\PrincCurvHS{2}(\DigShape,\vp) - \PrincCurv{2}(\Shape,\vx) \right| \le O\left( h^{\frac{1}{3}} \log^2 \left(\frac{1}{h}\right)\right) \,.
    %
  \end{align}
  %
\end{observation}
\begin{proof}
  %
  En suivant les même étapes que la démonstration du
  \RefTheorem{thm:curvature-estimator-2d-pf-conv} et en supposant la
  \RefConjecture{conj:slice-mdss-3d}, nous avons :
  %
  \begin{itemize}
    %
    \item Si $h\rho'(\DigShape) = \Theta(h^{\frac{1}{3}})$, nous sommes dans les hypothèses du \RefTheorem{thm:multigrid-convergence-curv-k1k2}, et ainsi le terme d'erreur est en $O(h^\frac{1}{3})$.
    %
    \item Si $h\rho'(\DigShape) = \Theta(h^{\frac{1}{3}} \log^2 (\frac{1}{h}))$,
    nous décomposons le terme d'erreur de
    l'\RefEquation{eq:error-term-conv-mean-curv-est}\footnote{avec $\mu_i = 1$
    dans le cas général} :
    %
    \begin{align}
      %
      |\frac{1}{h}\PrincCurvHS{1}(\DigShape, \vp) - \PrincCurv{1}(\Shape, \vx) | =  &| \PrincCurvH{1}{R}(\DSh,\hat{\vx},h) - \PrincCurv{1}(\Shape,\vx) |  \nonumber \\
      & \le O(R) + O( \frac{h}{R^2}) \\
      & \le O\left(h^{\frac{1}{3}} \log^2 \left(\frac{1}{h}\right)\right) + O\left( \frac{h^{\frac{1}{3}}}{\log^4 (\frac{1}{h})} \right) \,.
      %
    \end{align}
    %
  \end{itemize}
  %
  La borne supérieure de ces deux termes d'erreurs est $O\left(h^{\frac{1}{3}}
  \log^2 \left(\frac{1}{h}\right)\right)$. Nous avons alors :
  %
  \begin{equation}
    %
    \Theta(h^{\frac{1}{3}}) \le h{\rho'(\DigShape)} \le \Theta(h^{\frac{1}{3}}
    \log^2 (\frac{1}{h})) \Implies | \frac{1}{h}\PrincCurvHS{1}(\DigShape, \vp) -
    \PrincCurv{1}(\Shape, \vx) | \le O(h^{\frac{1}{3}} log^2({\frac{1}{h}})) \,.
    %
  \end{equation}
  %
  La démonstration pour $\PrincCurvHS{2}$ est similaire.
  %
\end{proof}

\subsection{Estimateur digital local de courbures sans paramètre en 3D}

Comme en 2D, nous pouvons changer le rayon de la boule localement en fonction de
la géométrie de la forme grâce aux longueurs des segments maximaux, et comme en
2D, nous ne pouvons pas espérer de convergence asymptotique à cause du
comportement des segments maximaux les plus longs. Cependant, nous pouvons déjà
les définir, et nous verrons dans la partie expérimentale du
\RefSection{sec:comparaison-courbure} que nous obtenons des convergences.
%
\begin{definition}
  %
  Pour un objet digital $\DigShape \subset \Z^3$, l'estimateur digital local de
  courbure moyenne sans paramètre $\MeanCurvHSL$ au pointel $\vp \in
  \BdZ{\DigShape}$ est défini par :
  %
  \begin{equation}
    %
    \MeanCurvHSL( \DigShape, \vp ) \EqDef \frac{8}{3 {\rho'(\DigShape, \vp)}} - \frac{4 V_{{\rho'(\DigShape, \vp)}}(\DigShape, \vp)}{\pi{{\rho'(\DigShape, \vp)}}^4} \,,
    %
    \label{def:curvature-estimator-3d-mean-pfl}
  \end{equation}
  %
  où $V_{\rho'(\DigShape, \vp)}(\DigShape, \vp) = Card(\Ball{\rho'(\DigShape, \vp)}{\vp} \cap \DigShape)$.
  %
\end{definition}
%
\begin{definition}
  %
  Soit $\DigShape$ une forme digitale de $\Z^3$, nous définissons les estimateurs
  digitaux local sans paramètre de courbures principales $\PrincCurvHSL{1}$ et
  $\PrincCurvHSL{2}$, de directions principales de courbure $\PrincDirHSL{1}$ et
  $\PrincDirHSL{2}$ et de normale $\NormalDirHSL$ au point $\vp \in
  \BdZ{\DigShape}$ de $\DigShape$ comme :
  %
  \begin{align}
      %
      \PrincCurvHSL{1}(\DigShape, \vp)  &\EqDef \frac{6}{\pi (\rho'(\DigShape, \vp))^6}(\hat{\lambda}^{*}_{2l} - 3\hat{\lambda}^{*}_{1l}) + \frac{8}{5 \rho'(\DigShape)},
      &\PrincDirHSL{1}(\DigShape, \vp) &\EqDef \hat{\nu}^{*}_{1l} \\
      \PrincCurvHSL{2}(\DigShape, \vp) &\EqDef \frac{6}{\pi (\rho'(\DigShape, \vp))^6}(\hat{\lambda}^{*}_{1l} - 3\hat{\lambda}^{*}_{2l}) + \frac{8}{5 \rho'(\DigShape)},
      &\PrincDirHSL{2}(\DigShape, \vp) &\EqDef \hat{\nu}^{*}_{2l}\\
      & &\NormalDirHSL(\DigShape, \vp) &\EqDef\hat{\nu}^{*}_{3l}\,,
      %
  \end{align}
  %
  où $\hat{\lambda}^{*}_{1l}$ et $\hat{\lambda}^{*}_{2l}$ sont les deux plus grandes
  valeurs propres de la matrice de covariance de $\Ball{{\rho'(\DigShape, \vp)}}{\vp}
  \cap \DigShape$, et $ \hat{\nu}^{*}_{1l}$, $\hat{\nu}^{*}_{2l}$ et $\hat{\nu}^{*}_{3l}$
  ses vecteurs propres.
  %
  \label{def:curvature-estimator-3d-k1k2-pfl}
\end{definition}
%
\subsection{Optimisation des estimateurs locaux sans paramètre et résultats}
\label{sec:curvature:parameter-free:k-means}

\begin{figure}[ht]{
  \begin{center}
    % \includegraphics[width=.95\linewidth]{images/Curvature/ScaleSpace_Flower_Local}
    \includegraphics[width=.95\linewidth]{images/Curvature/ScaleSpace_Flower_Local_5}
  \end{center}}
  %
  \caption{Valeur de courbure estimée en fonction du rayon $R$ sur le bord de
  l'objet \textsc{AccFlower} allant du bleu (valeur minimale) au jaune (valeur
  maximale). \emph{En abscisse :} position sur la surface de l'objet, \emph{en
  ordonnée :} différents rayons $R$. \emph{En noir sur l'image :} $K = 5$ rayons
  choisis pour l'algorithme de $K$-moyennes avec les rayons de $\CurvHSL$.
  %
  \label{fig:curvature-pfl-radii}}
  %
\end{figure}

Nous pouvons améliorer le temps de calcul des estimateurs qui s'adaptent à la
géométrie locale du bord de l'objet $\CurvHSL$, $\MeanCurvHSL$ et autres. En
effet, sur des objets à forte résolution (\cad $h$ petit), ces estimateurs sont
très coûteux en temps car nous perdons l'optimisation du calcul incrémental dont
nous parlerons dans la partie dédiée à la complexité et aux temps de calcul
(\RefSection{sec:complexite}) et dans la partie implémentation
(\RefSection{sec:applications:dgtal}). Pour donner l'intuition de la nécessité
d'améliorer le temps de calcul, nous optimisons le calcul pour les déplacements
$0$-adjacent en réutilisant une partie du résultat de l'itération précédente,
diminuant ainsi la complexité d'une dimension (passant de la taille de la boule à
uniquement ses éléments de surface) et réduisant fortement le temps de calcul.
Ici, la version locale des estimateurs sans paramètre respectent la contrainte
de déplacement mais le changement local de rayon empêche de précalculer
initialement les masques de déplacement. Ainsi, cela revient à calculer sur des
boules entières à chaque itération. Nous proposons de limiter le nombre de
rayons locaux à $K$ rayons. Cela aura pour conséquence la possibilité de
précalculer les boules de différents rayons, au lieu de les
recalculer localement pour chaque élément de surface. Nous introduisons ici les
variantes des $K$-moyennes (« \anglais{$K$-means} »), limitant ainsi le nombre
de rayons de boule disponibles à $K$. Pour se faire, nous distribuons les
longueurs des segments maximaux dans $K$ partitions en minimisant la somme des
carrés des différences des longueurs. La \RefFigure{fig:curvature-pfl-radii}
montre le résultat de la répartition des rayons pour $K=5$. À noter que la
couverture tangentielle de la surface est très rapide.

\begin{figure}[ht]{
  \begin{center}
    \includegraphics[height=4cm]{images/Curvature/Flower_kMeans-5}
    \includegraphics[height=4cm]{images/Curvature/Ellipsoid_Radius_2}
  \end{center}}
  %
  \caption{Partitionnement du carré des longueurs des segments
  maximaux en utilisant l'algorithme de $K$-moyennes (avec $K=5$), sur le bord
  de l'objet \Flower et de l'objet \Ellipsoid
  %
  \label{fig:curvature-pfl-map}.}
  %
\end{figure}

La \RefFigure{fig:curvature-pfl-map} illustre ces partitions sur
deux objets : \Flower pour un objet 2D et \Ellipsoid en 3D.
%
Une comparaison complète de tous les estimateurs présentés dans cette section
est disponible dans le \RefSection{sec:comparaison-courbure}. La
\RefFigure{fig:mean-curv-different-scale} montre le résultat de l'estimateur
digital local sans paramètre $\MeanCurvHSL$ sur des objets de différentes
tailles. Nous pouvons alors voir que l'estimateur s'adapte parfaitement à la
géométrie locale de la forme et affiche une cohérence de résultat quelque soit
l'échelle de l'objet.

\begin{figure}[ht]
\begin{center}
  \includegraphics[width=.30\linewidth]{images/Curvature/Bunny_64_mean}
  \includegraphics[width=.30\linewidth]{images/Curvature/Bunny_128_mean}
  \includegraphics[width=.30\linewidth]{images/Curvature/Bunny_256_mean}
\end{center}
  \caption{Courbure moyenne affichée sur des objets \Bunny à différentes résolutions : $64^3$, $128^3$ et $256^3$. La valeur de courbure moyenne est obtenue avec l'estimateur digital local sans paramètre $\MeanCurvHSL$ (la couleur jaune est la plus haute valeur de courbure, la couleur bleue est la plus petite). \label{fig:mean-curv-different-scale}}
\end{figure}

% \section{Estimation de la courbure par intégration surfacique de la sphère}
% \subsection{Analyse euclidienne de l'intégration surfacique de la sphère, et relation à la courbure}
% \subsubsection{Estimation par aire de patch de surface}
% \subsubsection{Estimation par aire de surface de sphère}

\section{Analyse comparative des estimateurs de courbure}%
\label{sec:comparaison-courbure}
%
Dans cette section, nous allons évaluer expérimentalement tous les estimateurs
de courbure présentés dans ce chapitre.
%
\subsection{Méthodologie d'évaluation}%
\label{sec:comparaison:methodologie}


\begin{table}[ht]
    \caption{Équations, paramètres et domaines des formes  euclidiennes considérées pour l'évaluation expérimentale ($t\in[0,2\pi]$ pour les courbes paramétriques). Les illustrations sont disponibles avec la \RefFigure{fig:illustrationShapes}.}
\begin{overpic}[width=\textwidth,height=.25\textheight]%
  {images/misc/TransparentPixel}
  \put(-10,20){%
    \scriptsize
    \renewcommand{\arraystretch}{1.5}
    \begin{tabular}{@{}llllrr@{}}
      \toprule
      Forme   & Équation (paramétrique en 2D, implicite en 3D) & Paramètres & Domaine    & $k_{min}$ & $k_{max}$ \\ \midrule
      \multirow{2}{*}{\textsc{Ellipse}} & $(x(t),y(t))=(\rho(t)\cdot\cos(t),\rho(t)\cdot\sin(t))$ & \multirow{2}{*}{$(a,b)=(20,7)$} & \multirow{2}{*}{$[-20,20]^2$} & \multirow{2}{*}{$0.018$} & \multirow{2}{*}{$0.408$}  \\
      & {avec $\rho(t)=\frac{b}{\sqrt{1 - (a^2-b^2)/a^2\cdot \cos(t+\phi)}}$} &     &   & & \\
      \multirow{2}{*}{\textsc{Flower}}  &  $(x(t),y(t))=(\rho(t)\cdot\cos(t),\rho(t)\cdot\sin(t))$ & \multirow{2}{*}{$(r_1,r_2,p)=(20,7,6)$} & \multirow{2}{*}{$[-20,20]^2$} & \multirow{2}{*}{$-1.414$} & \multirow{2}{*}{$0.383$} \\
      & {avec $\rho(t)=r_1+r_2\cdot\cos(p\cdot t)$} &     &   & & \\
      \multirow{2}{*}{\textsc{AccFlower}}  & $(x(t),y(t))=(\rho(t)\cdot\cos(t),\rho(t)\cdot\sin(t))$ & \multirow{2}{*}{$(r_1,r_2,p)=(20,5,3)$} & \multirow{2}{*}{$[-20,20]^2$}  & \multirow{2}{*}{$-10.45$} & \multirow{2}{*}{$3.1482$} \\
      & {avec $\rho(t)=r_1+r_2\cdot\cos(p\cdot t^3)$} &     &   & & \\
      \textsc{Sphere}      & $x^2 + y^2 + z^2 - a^2 = 0$ & $a=9$ & $[-10,10]^3$ & $0.111$    & $0.111$    \\
      \textsc{Rounded Cube} & $x^4 + y^4 + z^4 - a^4 = 0$ & $a=9$ &$[-10,10]^3$ & $0$       & $0.282$     \\
      Surface de \textsc{Goursat} & $ax^4 + ay^4 + az^4 + bx^2 + by^2 + bz^2 + c = 0$  & $(a,b,c)=(0.03,-2,-8)$ & $[-10,10]^3$ & $-0.15$     & $0.453$      \\
        \bottomrule
    \end{tabular}
    }
    \end{overpic}
    \label{tab:shapes}
\end{table}

\begin{figure}[ht]
  \begin{center}
      \includegraphics[width=.30\linewidth]{images/Ellipse}
      \includegraphics[width=.30\linewidth]{images/Flower}
      \includegraphics[width=.30\linewidth]{images/AccFlower}\\
      \includegraphics[width=.30\linewidth]{images/Sphere}
      \includegraphics[width=.30\linewidth]{images/RoundedCube2}
      \includegraphics[width=.30\linewidth]{images/BlobbyCube}
  \end{center}
  \caption{Illustration de formes 2D et 3D considérées pour l'évaluation expérimentale (se référer au \RefTable{tab:shapes} pour les équations et les paramètres utilisés). \emph{De gauche à droite, de haut en bas} : \textsc{Ellipse}, \textsc{Flower}, \textsc{AccFlower}, \textsc{Sphere}, \textsc{Rounded Cube} et Surface de \textsc{Goursat}.}
  \label{fig:illustrationShapes}
\end{figure}

Tous les estimateurs présentés ont été mis en œuvre dans \DGtal \cite{DGtal}.
Nous reviendrons plus tard sur la mise en œuvre et l'optimisation de ces
estimateurs (voir \RefSection{sec:applications:dgtal}). \DGtal est une bibliothèque
C++ open-source dédiée aux outils de géométrie digitale. En plus de proposer nos
estimateurs, cette bibliothèque nous permet de construire des formes paramétriques
ou implicites en dimension 2 et 3 pour n'importe quel pas de discrétisation $h$,
s'inscrivant dans un processus d'évaluation de convergence asymptotique. De
plus, plusieurs estimateurs de courbure de la littérature sont disponibles,
rendant facile l'évaluation comparative. Le \RefTable{tab:shapes} et la
\RefFigure{fig:illustrationShapes} présentent les formes continues considérées
dans cette évaluation. Il est à noter que pour les estimateurs présentés dans ce
chapitre, seulement les formes convexes à bord $C^3$ (\Ellipse, \Sphere,
\RoundedCube, \Ellipsoid) respectent les hypothèses des théorèmes de
convergence. Cependant, nous pouvons expérimentalement évaluer le comportement
de nos estimateurs lorsque nous considérons des formes qui ne satisfont pas les
hypothèses, comme les formes avec des parties non convexes.


Pour l'ensemble des formes que nous analysons, nous allons comparer les valeurs
de courbure estimées avec les valeurs euclidiennes attendues. Grâce à
l'utilisation de courbes et surfaces paramétriques, nous connaissons en tout
point du bord de la forme l'information de courbure, rendant la
comparaison possible. Ainsi les vecteurs d'erreurs entre les valeurs estimées et les valeurs
euclidiennes sont alors calculés avec la norme $l_2$ et la norme $l_\infty$
(équivalent au pire cas).
%
Considérons $a$ comme l'ensemble des valeurs de courbure euclidienne du bord de
l'objet, et $b$ l'ensemble des valeurs estimées; alors les normes $l_2$ (appelée
norme moyenne) et $l_\infty$ (celle qui nous intéresse lorsque nous nous
intéressons à la convergence asymptotique, comprendre convergence uniforme) se
calculent comme suit :
%
\begin{align}
  &|| a - b ||_2 \EqDef \sqrt{(\sum_i(a_i - b_i)^2)} \quad &\text{(norme $l_2$)} \\
  &|| a - b ||_\infty \EqDef \max(|a - b|) \quad &\text{(norme $l_\infty$)} \,.
\end{align}
%
Nous exportons les résultats sur des graphes dont les échelles sont
logarithmiques (avec en abscisse les pas de discrétisation $h$ et en ordonnée
l'erreur $l_2$ ou $l_\infty$) afin de pouvoir comparer plus facilement les
vitesses de convergence des estimateurs. Il est à noter que pour chaque graphe, le pas
de discrétisation $h$ décroît (donc l'objet digital se raffine) de la droite
vers la gauche. La pente nous informe alors sur la vitesse de convergence de
l'estimateur. Plus la pente est forte, plus la vitesse de convergence
est grande. Si elle ne décroît pas, alors nous pouvons dire
qu'expérimentalement, l'estimateur ne converge pas.
%
\subsection{Analyse expérimentale de la convergence asymptotique}
%
Nous allons dans un premier temps confirmer le choix du paramètre $\alpha$
utilisé pour guider le rayon de la boule dans les preuves des
\RefTheoremsTF{thm:multigrid-convergence-curv}{thm:multigrid-convergence-curv-mean}{thm:multigrid-convergence-curv-k1k2}.
%
\paragraph{Choix de $\alpha = \frac{1}{3}$}
%
Dans ces théorèmes, nous avons démontré que choisir un rayon de boule en $R =
kh^\alpha$ avec $\alpha = \frac{1}{3}$ permettait de minimiser les termes
d'erreur et d'obtenir une convergence asymptotique en $O(h^\frac{1}{3})$ pour
les estimateurs digitaux de courbure en 2D $\CurvH{R}$, de courbure moyenne
$\MeanCurvH{R}$ et de courbures principales $\PrincCurvH{1}{R}$ et
$\PrincCurvH{2}{R}$ en 3D par intégration. La première ligne des
\RefFiguresTF{fig:curv-experiments-ellipse}{fig:curv-experiments-flower}{fig:curv-experiments-accflower}
nous montre les vitesses de convergence asymptotique pour $5$ valeurs de
$\alpha$ différentes : $\frac{1}{2}$, $\frac{2}{5}$, $\frac{1}{3}$,
$\frac{2}{7}$ et $\frac{1}{4}$. L'enjeu du choix de $\alpha$ est également
algorithmique, puisqu'il guide la taille du rayon, et donc influe sur le temps
de calcul.


Nous observons expérimentalement pour l'objet \Ellipse
(\RefFigure{fig:curv-experiments-ellipse}) une convergence asymptotique pour
toutes les valeurs de $\alpha$ à l'exception de $\alpha = \frac{1}{2}$. Comme
suggéré dans la preuve du \RefTheorem{thm:multigrid-convergence-curv}, $\alpha =
\frac{1}{3}$ donne expérimentalement un comportement (en norme $l_\infty$) en
$O(h^\frac{1}{3})$ (voir le \RefTable{tab:convergence-speed-alpha-2d} pour les
vitesses de convergence observées). Le théorème est défini pour le cas général
des formes convexes à bord $C^3$, mais comme précisé dans le théorème, le terme
d'erreur en grand « O » peut être amélioré si nous ajoutons des hypothèses sur
la forme. Cela explique pourquoi de meilleures vitesses de convergence semblent
être obtenues lorsque $\alpha = \frac{2}{7}$ et $\frac{1}{4}$. Nous pouvons
également observer la convergence asymptotique en $O(h^\frac{1}{3})$ sur les
formes non-convexes pour $\alpha = \frac{1}{3}$. Il est intéressant de noter que
des valeurs de $\alpha$ supérieures à $\frac{1}{3}$ (donc avec une taille de
boule supérieure) pour cette catégorie de forme semble obtenir de meilleurs
estimations.
%
\begin{table}[ht]
\centering
%
\caption{Vitesse de convergence observée de $\CurvH{R}$ en fonction du paramètre
$\alpha$ avec $R=kh^\alpha$ sur l'objet \Ellipse, en utilisant de
l'approximation aux moindres carré pour approcher les données.}
%
\label{tab:convergence-speed-alpha-2d}
\begin{tabular}{@{}lr@{}}
%\toprule
 $\alpha$ & Vitesse de convergence observée \\ \midrule
 $1/2$    & $O(h^{0.024})$                    \\
 $2/5$    & $O(h^{0.24})$                    \\
 $1/3$    & $O(h^{0.38})$                    \\
 $2/7$    & $O(h^{0.41})$                    \\
 $1/4$    & $O(h^{0.44})$                    \\ \bottomrule
\end{tabular}
\end{table}
%


Nous avons fait également une évaluation de ce paramètre $\alpha$ sur des formes
digitales 3D (voir \RefFigure{fig:curv-experiments-3D-alpha}) \RoundedCube et
\Goursat pour les estimateurs de courbure moyenne $\MeanCurvH{R}$ et de
courbures principales $\PrincCurvH{1}{R}$ et $\PrincCurvH{2}{R}$. Les preuves des
\RefTheoremsDF{thm:multigrid-convergence-curv-mean}{thm:multigrid-convergence-curv-k1k2}
ont statués à $\alpha = \frac{1}{3}$ afin de minimiser les termes d'erreurs et
obtenir une convergence en $O(h^\frac{1}{3})$. Nous pouvons observer que pour les
formes convexes comme non-convexes, $\alpha = \frac{1}{3}$ amène à la vitesse de
convergence attendue pour ces trois estimateurs. Nous pouvons noter que de
meilleures vitesses de convergence peuvent être obtenues avec $\alpha =
\frac{2}{7}$ et $\frac{1}{4}$ pour les mêmes raisons qu'en 2D.


\begin{figure}[ht]
  \begin{center}
    \setlength{\tabcolsep}{0.0pt}
    \begin{tabular}{@{}l c c @{}}
      \multicolumn{3}{c}{\textsc{Ellipse}}
      \\ \toprule
      \rotatebox{90}{~~~~~~Différents $\alpha$} &
      \includegraphics[width=5cm]{images/Ellipse} &
      \includegraphics[width=7cm]{graphs/Ellipse_ALPHA_Loo}
      \\
      \rotatebox{90}{~~~~~~~~~~~~~~$\CurvH{R}$} &
      \includegraphics[width=7cm]{graphs/Ellipse_L2} &
      \includegraphics[width=7cm]{graphs/Ellipse_Loo}
      \\
      \rotatebox{90}{~~~~~~~~~~~~~~$\CurvHS$} &
      \includegraphics[width=7cm]{graphs/Ellipse_PF_L2} &
      \includegraphics[width=7cm]{graphs/Ellipse_PF_Loo}
      \\
      &
      Erreur $l_2$ &
      Erreur $l_\infty$
    \end{tabular}
    \caption[Évaluation expérimentale sur l'objet \Ellipse.]{
      %
      \emph{Première ligne :} Illustration de l'objet \Ellipse et comparaison
      des vitesses de convergence asymptotique de l'estimateur $\CurvH{R}$ avec
      \colorize{différentes valeurs de $\alpha$} pour le rayon $R$ (avec $R=kh^\alpha$).
      %
      \\
      %
      \emph{Seconde ligne :} Comparaison des erreurs $l_2$ (\emph{à gauche}) et
      $l_\infty$ (\emph{à droite}) d'estimation de \colorize{courbure} en 2D de $\CurvH{R}$
      (\II) avec les estimateurs \BC \cite{Esbelin2011}, \MDSS
      \cite{Coeurjolly2001,deVieilleville2007} et \MDCA \cite{Roussillon2011}.
      %
      \\
      %
      \emph{Troisième ligne :} Comparaison des erreurs $l_2$ (\emph{à gauche})
      et $l_\infty$ (\emph{à droite}) d'estimation de \colorize{courbure} en 2D de $\CurvHS$,
      $\CurvHSL$, $\CurvHSLK{5}$, $\CurvHSLK{10}$ et $\CurvHSLK{50}$ avec
      l'estimateur \MDCA \cite{Roussillon2011}.
      %
      }
      \label{fig:curv-experiments-ellipse}
  \end{center}
\end{figure}

\begin{figure}[ht]
  \begin{center}
    \setlength{\tabcolsep}{0.0pt}
    \begin{tabular}{@{}l c c @{}}
      \multicolumn{3}{c}{\textsc{Flower}}
      \\ \toprule
      \rotatebox{90}{~~~~~~Différents $\alpha$} &
      \includegraphics[width=5cm]{images/Flower} &
      \includegraphics[width=7cm]{graphs/Flower_ALPHA_Loo}
      \\
      \rotatebox{90}{~~~~~~~~~~~~~~$\CurvH{R}$} &
      \includegraphics[width=7cm]{graphs/Flower_L2} &
      \includegraphics[width=7cm]{graphs/Flower_Loo}
      \\
      \rotatebox{90}{~~~~~~~~~~~~~~$\CurvHS$} &
      \includegraphics[width=7cm]{graphs/Flower_PF_L2} &
      \includegraphics[width=7cm]{graphs/Flower_PF_Loo}
      \\
      &
      Erreur $l_2$ &
      Erreur $l_\infty$
    \end{tabular}
    \caption[Évaluation expérimentale sur l'objet \Flower.]{
      %
      \emph{Première ligne :} Illustration de l'objet \Flower et comparaison
      des vitesses de convergence asymptotique de l'estimateur $\CurvH{R}$ avec
      \colorize{différentes valeurs de $\alpha$} pour le rayon $R$ (avec $R=kh^\alpha$).
      %
      \\
      %
      \emph{Seconde ligne :} Comparaison des erreurs $l_2$ (\emph{à gauche}) et
      $l_\infty$ (\emph{à droite}) d'estimation de \colorize{courbure} en 2D de $\CurvH{R}$
      (\II) avec les estimateurs \BC \cite{Esbelin2011}, \MDSS
      \cite{Coeurjolly2001,deVieilleville2007} et \MDCA \cite{Roussillon2011}.
      %
      \\
      %
      \emph{Troisième ligne :} Comparaison des erreurs $l_2$ (\emph{à gauche})
      et $l_\infty$ (\emph{à droite}) d'estimation de \colorize{courbure} en 2D de $\CurvHS$,
      $\CurvHSL$, $\CurvHSLK{5}$, $\CurvHSLK{10}$ et $\CurvHSLK{50}$ avec
      l'estimateur \MDCA \cite{Roussillon2011}.
      %
      }
      \label{fig:curv-experiments-flower}
  \end{center}
\end{figure}

\begin{figure}[ht]
  \begin{center}
    \setlength{\tabcolsep}{0.0pt}
    \begin{tabular}{@{}l c c @{}}
      \multicolumn{3}{c}{\textsc{AccFlower}}
      \\ \toprule
      \rotatebox{90}{~~~~~~Différents $\alpha$} &
      \includegraphics[width=5cm]{images/AccFlower} &
      \includegraphics[width=7cm]{graphs/AccFlower_ALPHA_Loo}
      \\
      \rotatebox{90}{~~~~~~~~~~~~~~$\CurvH{R}$} &
      \includegraphics[width=7cm]{graphs/AccFlower_L2} &
      \includegraphics[width=7cm]{graphs/AccFlower_Loo}
      \\
      % \rotatebox{90}{~~~~~~~~~~~~~~$\CurvHS$} &
      % \includegraphics[width=7cm]{graphs/AccFlower_PF_L2} &
      % \includegraphics[width=7cm]{graphs/AccFlower_PF_Loo}
      % \\
      &
      Erreur $l_2$ &
      Erreur $l_\infty$
    \end{tabular}
    \caption[Évaluation expérimentale sur l'objet \AccFlower.]{
      %
      \emph{Première ligne :} Illustration de l'objet \AccFlower et comparaison
      des vitesses de convergence asymptotique de l'estimateur $\CurvH{R}$ avec
      \colorize{différentes valeurs de $\alpha$} pour le rayon $R$ (avec $R=kh^\alpha$).
      %
      \\
      %
      \emph{Seconde ligne :} Comparaison des erreurs $l_2$ (\emph{à gauche}) et
      $l_\infty$ (\emph{à droite}) d'estimation de \colorize{courbure} en 2D de $\CurvH{R}$
      (\II) avec les estimateurs \BC \cite{Esbelin2011}, \MDSS
      \cite{Coeurjolly2001,deVieilleville2007} et \MDCA \cite{Roussillon2011}.
      %
      }
      \label{fig:curv-experiments-accflower}
  \end{center}
\end{figure}

\begin{figure}[ht]
  \begin{center}
    \setlength{\tabcolsep}{0.0pt}
    \begin{tabular}{@{}l c c @{}}
      &
      \textsc{Rounded Cube} &
      Surface de \textsc{Goursat}
      \\ \toprule
      \rotatebox{90}{~~~~~~~$\MeanCurvH{R}$} &
      \includegraphics[width=7cm]{graphs/RoundedCube2_ALPHA_Mean_Loo} &
      \includegraphics[width=7cm]{graphs/BlobbyCube_ALPHA_Mean_Loo}
      \\
      \rotatebox{90}{~~~~~~~$\PrincCurvH{1}{R}$} &
      \includegraphics[width=7cm]{graphs/RoundedCube2_ALPHA_k1_Loo} &
      \includegraphics[width=7cm]{graphs/BlobbyCube_ALPHA_k1_Loo}
      \\
      \rotatebox{90}{~~~~~~~$\PrincCurvH{2}{R}$} &
      \includegraphics[width=7cm]{graphs/RoundedCube2_ALPHA_k2_Loo} &
      \includegraphics[width=7cm]{graphs/BlobbyCube_ALPHA_k2_Loo}
      \\
      &
      Erreur $l_2$ &
      Erreur $l_\infty$
    \end{tabular}
    \caption{
      %
      Comparaison des convergences asymptotiques des estimateurs $\MeanCurvH{R}$
      (\emph{première ligne}), $\PrincCurvH{1}{R}$ (\emph{seconde ligne}) et
      $\PrincCurvH{2}{R}$ (\emph{troisième ligne}) avec \colorize{différentes valeurs de
      $\alpha$} pour le rayon $R$ ($R=kh^\alpha$) sur l'objet \RoundedCube et
      \Goursat. L'erreur est en $l_2$ pour la première colonne, et en $l_\infty$
      pour la seconde.
      %
      }
      \label{fig:curv-experiments-3D-alpha}
  \end{center}
\end{figure}

\begin{figure}[ht]
  \begin{center}
    \setlength{\tabcolsep}{0.0pt}
    \begin{tabular}{@{}l c c @{}}
      \multicolumn{3}{c}{\textsc{Sphere}}
      \\ \toprule
      \rotatebox{90}{~~~~~~~$\MeanCurvH{R}$} &
      \includegraphics[width=7cm]{graphs/Sphere_Mean_L2} &
      \includegraphics[width=7cm]{graphs/Sphere_Mean_Loo}
      \\
      \rotatebox{90}{~~~~~~~$\PrincCurvH{1}{R}$} &
      \includegraphics[width=7cm]{graphs/Sphere_k1_L2} &
      \includegraphics[width=7cm]{graphs/Sphere_k1_Loo}
      \\
      \rotatebox{90}{~~~~~~~$\PrincCurvH{2}{R}$} &
      \includegraphics[width=7cm]{graphs/Sphere_k2_L2} &
      \includegraphics[width=7cm]{graphs/Sphere_k2_Loo}
      \\
      &
      Erreur $l_2$ &
      Erreur $l_\infty$
    \end{tabular}
    \caption{
      %
      \emph{Première ligne :} Comparaison des erreurs $l_2$ (\emph{à gauche}) et
      $l_\infty$ (\emph{à droite}) d'estimation de \colorize{courbure moyenne} de
      $\MeanCurvH{R}$ avec \JetFitting \cite{Cazals2005} sur l'objet \Sphere.
      %
      \\
      %
      \emph{Seconde ligne :} Comparaison des erreurs $l_2$ (\emph{à gauche}) et
      $l_\infty$ (\emph{à droite}) d'estimation de la \colorize{première courbure
      principale} de $\PrincCurvH{1}{R}$ avec \JetFitting \cite{Cazals2005} sur
      l'objet \Sphere.
      %
      \\
      %
      \emph{Troisième ligne :} Comparaison des erreurs $l_2$ (\emph{à gauche})
      et $l_\infty$ (\emph{à droite}) d'estimation de la \colorize{seconde courbure
      principale} de $\PrincCurvH{2}{R}$ avec \JetFitting \cite{Cazals2005} sur
      l'objet \Sphere.
      %
      }
      \label{fig:curv-experiments-sphere}
  \end{center}
\end{figure}

\begin{figure}[ht]
  \begin{center}
    \setlength{\tabcolsep}{0.0pt}
    \begin{tabular}{@{}l c c @{}}
      \multicolumn{3}{c}{\textsc{Rounded Cube}}
      \\ \toprule
      \rotatebox{90}{~~~~~~~$\MeanCurvH{R}$} &
      \includegraphics[width=7cm]{graphs/RoundedCube2_Mean_L2} &
      \includegraphics[width=7cm]{graphs/RoundedCube2_Mean_Loo}
      \\
      \rotatebox{90}{~~~~~~~$\PrincCurvH{1}{R}$} &
      \includegraphics[width=7cm]{graphs/RoundedCube2_k1_L2} &
      \includegraphics[width=7cm]{graphs/RoundedCube2_k1_Loo}
      \\
      \rotatebox{90}{~~~~~~~$\PrincCurvH{2}{R}$} &
      \includegraphics[width=7cm]{graphs/RoundedCube2_k2_L2} &
      \includegraphics[width=7cm]{graphs/RoundedCube2_k2_Loo}
      \\
      &
      Erreur $l_2$ &
      Erreur $l_\infty$
    \end{tabular}
    \caption{
      %
      \emph{Première ligne :} Comparaison des erreurs $l_2$ (\emph{à gauche}) et
      $l_\infty$ (\emph{à droite}) d'estimation de \colorize{courbure moyenne} de
      $\MeanCurvH{R}$ avec \JetFitting \cite{Cazals2005} sur l'objet \RoundedCube.
      %
      \\
      %
      \emph{Seconde ligne :} Comparaison des erreurs $l_2$ (\emph{à gauche}) et
      $l_\infty$ (\emph{à droite}) d'estimation de la \colorize{première courbure
      principale} de $\PrincCurvH{1}{R}$ avec \JetFitting \cite{Cazals2005} sur
      l'objet \RoundedCube.
      %
      \\
      %
      \emph{Troisième ligne :} Comparaison des erreurs $l_2$ (\emph{à gauche})
      et $l_\infty$ (\emph{à droite}) d'estimation de la \colorize{seconde courbure
      principale} de $\PrincCurvH{2}{R}$ avec \JetFitting \cite{Cazals2005} sur
      l'objet \RoundedCube.
      %
      }
      \label{fig:curv-experiments-rounded}
  \end{center}
\end{figure}

\begin{figure}[ht]
  \begin{center}
    \setlength{\tabcolsep}{0.0pt}
    \begin{tabular}{@{}l c c @{}}
      \multicolumn{3}{c}{Surface de \textsc{Goursat}}
      \\ \toprule
      \rotatebox{90}{~~~~~~~$\MeanCurvH{R}$} &
      \includegraphics[width=7cm]{graphs/BlobbyCube_Mean_L2} &
      \includegraphics[width=7cm]{graphs/BlobbyCube_Mean_Loo}
      \\
      \rotatebox{90}{~~~~~~~$\PrincCurvH{1}{R}$} &
      \includegraphics[width=7cm]{graphs/BlobbyCube_k1_L2} &
      \includegraphics[width=7cm]{graphs/BlobbyCube_k1_Loo}
      \\
      \rotatebox{90}{~~~~~~~$\PrincCurvH{2}{R}$} &
      \includegraphics[width=7cm]{graphs/BlobbyCube_k2_L2} &
      \includegraphics[width=7cm]{graphs/BlobbyCube_k2_Loo}
      \\
      &
      Erreur $l_2$ &
      Erreur $l_\infty$
    \end{tabular}
    \caption{
      %
      \emph{Première ligne :} Comparaison des erreurs $l_2$ (\emph{à gauche}) et
      $l_\infty$ (\emph{à droite}) d'estimation de \colorize{courbure moyenne} de
      $\MeanCurvH{R}$ avec \JetFitting \cite{Cazals2005} sur l'objet \Goursat.
      %
      \\
      %
      \emph{Seconde ligne :} Comparaison des erreurs $l_2$ (\emph{à gauche}) et
      $l_\infty$ (\emph{à droite}) d'estimation de la \colorize{première courbure
      principale} de $\PrincCurvH{1}{R}$ avec \JetFitting \cite{Cazals2005} sur
      l'objet \Goursat.
      %
      \\
      %
      \emph{Troisième ligne :} Comparaison des erreurs $l_2$ (\emph{à gauche})
      et $l_\infty$ (\emph{à droite}) d'estimation de la \colorize{seconde courbure
      principale} de $\PrincCurvH{2}{R}$ avec \JetFitting \cite{Cazals2005} sur
      l'objet \Goursat.
      %
      }
      \label{fig:curv-experiments-goursat}
  \end{center}
\end{figure}

\begin{figure}[ht]
  \begin{center}
    \setlength{\tabcolsep}{0.0pt}
    \begin{tabular}{@{}l c c @{}}
      \multicolumn{3}{c}{\textsc{Ellipsoid}}
      \\ \toprule
      \rotatebox{90}{~~~~~~~$\MeanCurvHS$} &
      \includegraphics[width=7cm]{graphs/Ellipsoid_PF_Mean_L2} &
      \includegraphics[width=7cm]{graphs/Ellipsoid_PF_Mean_Loo}
      \\
      \rotatebox{90}{~~~~~~~$\PrincCurvHS{1}$} &
      \includegraphics[width=7cm]{graphs/Ellipsoid_PF_k1_L2} &
      \includegraphics[width=7cm]{graphs/Ellipsoid_PF_k1_Loo}
      \\
      \rotatebox{90}{~~~~~~~$\PrincCurvHS{2}$} &
      \includegraphics[width=7cm]{graphs/Ellipsoid_PF_k2_L2} &
      \includegraphics[width=7cm]{graphs/Ellipsoid_PF_k2_Loo}
      \\
      &
      Erreur $l_2$ &
      Erreur $l_\infty$
    \end{tabular}
    \caption{
      %
      \emph{Première ligne :} Comparaison des erreurs $l_2$ (\emph{à gauche}) et
      $l_\infty$ (\emph{à droite}) d'estimation de \colorize{courbure moyenne} de
      $\MeanCurvHS$, $\MeanCurvHSL$, $\MeanCurvHSLK{5}$, $\MeanCurvHSLK{10}$ et
      $\MeanCurvHSLK{50}$ sur l'objet \Ellipsoid.
      %
      \\
      %
      \emph{Seconde ligne :} Comparaison des erreurs $l_2$ (\emph{à gauche}) et
      $l_\infty$ (\emph{à droite}) d'estimation de la \colorize{première courbure
      principale} de $\PrincCurvHS{1}$, $\PrincCurvHSL{1}$,
      $\PrincCurvHSLK{1}{5}$, $\PrincCurvHSLK{1}{10}$ et $\PrincCurvHSLK{1}{50}$
      sur l'objet \Ellipsoid.
      %
      \\
      %
      \emph{Troisième ligne :} Comparaison des erreurs $l_2$ (\emph{à gauche})
      et $l_\infty$ (\emph{à droite}) d'estimation de la \colorize{seconde courbure
      principale} de $\PrincCurvHS{2}$, $\PrincCurvHSL{2}$,
      $\PrincCurvHSLK{2}{5}$, $\PrincCurvHSLK{2}{10}$ et $\PrincCurvHSLK{2}{50}$
      sur l'objet \Ellipsoid.
      %
      }
      \label{fig:curv-experiments-ellipsoid}
  \end{center}
\end{figure}

\noindent Nous pouvons alors conclure qu'expérimentalement le choix de $\alpha =
\frac{1}{3}$ est cohérent pour obtenir une vitesse de convergence des
estimateurs $\CurvH{R}$, $\MeanCurvH{R}$, $\PrincCurvH{1}{R}$,
$\PrincCurvH{2}{R}$ en $O(h^\frac{1}{3})$.
%
\paragraph{Convergence expérimentale des estimateurs digitaux de courbure en 2D}
%
Avec la seconde ligne des
\RefFiguresTF{fig:curv-experiments-ellipse}{fig:curv-experiments-flower}{fig:curv-experiments-accflower},
nous comparons l'estimateur digital de courbure par intégration $\CurvH{R}$
(nommé \II sur les graphes, avec $R = kh^\frac{1}{3}$) avec les estimateurs \BC
\cite{Esbelin2011}, \MDSS \cite{Coeurjolly2001,deVieilleville2007} et \MDCA
\cite{Roussillon2011} (que nous avons décrit dans l'état de l'art,
\RefSection{sec:estimators:SOTA}) avec les métriques d'erreurs $l_2$ et
$l_\infty$. Sur ces formes non bruitées, nous observons que les vitesses de
convergence en $l_\infty$ de notre estimateur sont proches de l'estimateur \MDCA.
Nous observons aussi que l'estimateur \BC converge mais avec des vitesses de
convergence plus faibles. Nous noterons également que l'estimateur \MDSS
n'affiche pas de convergence asymptotique expérimentalement sur nos données. En
considérant la norme $l_2$, nous pouvons observer que \BC fournit de meilleures
vitesses de convergence que notre estimateur, notamment sur l'objet \Ellipse,
mais lorsque nous mettons en parallèle avec les graphes d'erreur $l_\infty$,
nous pouvons voir que notre estimateur possède un meilleur comportement que \BC
lorsque nous raffinons l'objet. D'ailleurs cette différence de vitesse de
convergence est plus faible sur les objets non convexes.


Nous pouvons noter que les calculs de notre estimateur vont à des résolutions
supérieures à ceux des autres estimateurs. La raison est simple :
dans notre implémentation de l'estimateur \BC dans \DGtal, la taille du
masque\footnote{Dans notre implémentation dans \DGtal, nous utilisons comme
taille de masque : $d / h^\frac{1}{3}$ où $d$ est le diamètre du contour.}
devient trop importante pour de petites valeurs de $h$, ce qui implique des
problèmes de surcharge de la mémoire disponible. Pour l'estimateur \MDCA, la
reconnaissance d'arcs circulaires dans \DGtal est guidé par un prédicat
géométrique basé sur le calcul du déterminant de coordonnées de points au carré.
Alors, sur des valeurs de $h$ petites, des problèmes de capacité numérique et
des instabilités apparaissent (qui peuvent être corrigés en considérant des
nombres entiers de précision arbitraire mais cela contrebalancerait avec des
problèmes d'efficacité). Notre estimateur n'est pas sensible à cela. Pour notre
expérimentation la plus fine, le pas de discrétisation est $h = 0.00017$, ce qui
donne des formes digitales définies sur des domaines digitaux de
$\nombre{235295}^2$. À cette échelle, le contour de l'objet \Ellipse contient
$\nombre{648910}$ éléments.


Sur la troisième ligne des
\RefFiguresDF{fig:curv-experiments-ellipse}{fig:curv-experiments-flower}, nous
comparons la variante sans paramètre de notre estimateur digital de courbure par
intégration $\CurvHS$ ainsi que celui qui adapte localement le rayon $R$ en
fonction de la géométrie locale de la forme $\CurvHSL$ avec comme référence
$\CurvH{R}$ et \MDCA. Nous observons que tous les estimateurs convergent
asymptotiquement avec une vitesse de convergence d'au moins $O(h^\frac{1}{3})$, à
l'exception de $\CurvHSL$ pour l'objet \Ellipse qui a une vitesse un peu
inférieure. Cela n'est pas anormal puisque la borne d'erreur de cet estimateur
est en $O\left(h^\frac{1}{3} \log^2\left(\frac{1}{3}\right)\right)$.


Nous avons également testé l'estimateur $\CurvHSL$ lorsque nous limitons le
nombre de rayons disponibles avec l'approche des $K$-moyennes (voir
\RefSection{sec:curvature:parameter-free:k-means}) pour $K = 5$, $10$ et $50$
(nommés $\CurvHSLK{5}$, $\CurvHSLK{10}$ et $\CurvHSLK{50}$). Ces tests ont été
uniquement faits avec la métrique d'erreur $l_2$ car, comme nous l'avons
expliqué, l'objectif principal de ces estimateurs n'est pas la convergence
uniforme mais la diminution du temps de calcul par rapport à $\CurvHSL$. En
effet, pour de petits pas de discrétisation $h$, $\CurvHSL$ est très
consommateur de temps de calcul (nous devons créer une nouvelle boule digitale à
chaque point). Ainsi, nous voulons analyser les erreurs que génère cette
approximation. Nous remarquons alors que l'erreur $l_2$ de $\CurvHSL$ est
inférieure à celles de $\CurvHS$ et $\CurvH{R}$, grâce à la nature adaptative de
cet estimateur, ce qui était l'objectif visé. Nous remarquons également de très bonnes précisions
asymptotiques des résultats des estimateurs $\CurvHSLK{5}$, $\CurvHSLK{10}$ et
$\CurvHSLK{50}$, même lorsque le nombre de partitions est faible.

\paragraph{Convergence expérimentale des estimateurs digitaux de courbure en 3D}

Tout d'abord, la \RefFigure{fig:sexy-pic} montre le résultat de l'estimateur
$\MeanCurvH{R}$ sur plusieurs objets courant de la littérature : \Bunny,
\Armadillo, \Dragon, \Lucy et \AsianDragon. Nous avons également comparé
expérimentalement nos estimateurs digitaux de courbures moyenne $\MeanCurvH{R}$
et de courbures principales $\PrincCurvH{1}{R}$ et $\PrincCurvH{2}{R}$ avec
l'estimateur \JetFitting \cite{Cazals2005}\footnote{Nous utilisons le même
voisinage que pour notre estimateur.} décrit dans la
\RefSection{sec:estimators:SOTA}, avec les métriques $l_2$ et $l_\infty$ sur les
objets \Sphere (\RefFigure{fig:curv-experiments-sphere}), \RoundedCube
(\RefFigure{fig:curv-experiments-rounded}) et \Goursat
(\RefFigure{fig:curv-experiments-goursat}). Nous observons que \JetFitting
obtient de meilleurs résultats sur \Sphere pour la courbure moyenne et les
courbures principales que nos estimateurs. Pour l'estimation de la courbure
moyenne sur \RoundedCube et \Goursat, nous obtenons des vitesses de convergence
similaires à \JetFitting avec des erreurs $l_\infty$ légèrement moins
importantes avec notre estimateur. Pour les courbures principales sur ces
objets, nos estimateurs obtiennent de bien meilleures vitesses de convergence
que \JetFitting. Si nous regardons les erreurs moyennes (métrique
$l_2$), des comportements similaires sont observés. À noter que pour notre
expérimentation la plus fine, le pas de discrétisation était de $h = 0.04$, les
formes digitales sont définies sur un domaine digital de $\nombre{500}^3$. À
cette échelle, le bord de l'objet digital \RoundedCube contient
$\nombre{1125222}$ éléments.


\begin{figure}[ht]{
  \begin{center}
      \includegraphics[width=6.7cm]{images/Curvature/ii_grid}
      \includegraphics[width=6.7cm]{images/Curvature/ii_arma} \\
      \includegraphics[width=6.7cm]{images/Curvature/ii_dragon512}
      \includegraphics[width=6.7cm]{images/Curvature/ii_lucy} \\
      \includegraphics[width=6.7cm]{images/Curvature/ii_xyz512}
      \includegraphics[width=6.7cm]{images/Curvature/ii_xyzpindir1-zoom}
  \end{center}
      }
      \caption{
      %
      Courbure moyenne estimée avec $\MeanCurvH{R}$ sur des objets courant de la
      littérature (\Bunny, \Armadillo, \Dragon, \Lucy, \AsianDragon), avec
      $R=15$. L'image en bas à droite représente les premières directions
      principales de courbure estimées avec $\PrincDirH{1}{R}$ sur l'objet
      \AsianDragon.
      %
      \label{fig:sexy-pic}}
\end{figure}


Comme en 2D, nous avons testé expérimentalement les versions sans paramètre de
nos estimateurs de courbure moyenne et de courbures principales $\MeanCurvHS$,
$\PrincCurvHS{1}$ et $\PrincCurvHS{2}$, ainsi que les variantes adaptant le
rayon $R$ localement en fonction de la géométrie locale de la forme
$\MeanCurvHSL$, $\PrincCurvHSL{1}$ et $\PrincCurvHSL{2}$ et les versions
utilisant les $K$-moyennes diminuant ainsi le temps de calcul (avec $K = 5$,
$10$ et $50$). Le constat est le même qu'en 2D; la vitesse de convergence de
l'erreur $l_2$ de ces estimateurs est bien en dessous de $O(h^\frac{1}{3})$,
même lorsque le nombre de rayons possibles est bas. En ce qui concerne la vitesse
de convergence de l'erreur $l_\infty$ des estimateurs $\MeanCurvHS$,
$\PrincCurvHS{1}$ et $\PrincCurvHS{2}$, ceux-ci semblent expérimentalement
suivre la vitesse attendue en $O(h^\frac{1}{3})$.

\paragraph{Complexité et temps de calcul}
\label{sec:complexite}

Un des éléments qui peut être important dans le choix d'un estimateur est son
temps de calcul. En fonction de l'utilisation que nous souhaitons en faire, il
peut être judicieux de connaître le comportement d'un estimateur face à des
données de plus en plus grandes afin d'en extraire un profil de temps de calcul.


Bien que nos estimateurs soient des estimateurs intégrant des volumes, ceux-ci
sont comparables en complexité à des estimateurs surfaciques. En effet, une
implémentation naïve calculerait une boule digitale, la centrerait sur le bord
de l'objet digital à analyser, calculerait son intersection avec la forme à
chaque élément de surface de l'objet. Nous obtiendrions une complexité en
$O((R/h)^{d})$ par élément de surface, \cad la taille de la boule digitale au
pas de discrétisation $h$. Cependant, nous pouvons optimiser cet algorithme en
utilisant les avantages que nous offre la structure des surfaces digitales : si
nous nous déplaçons sur des éléments de surfaces adjacents, une partie du calcul
de l'intersection entre la boule et la forme sont alors déjà calculés à
l'itération précédente. Plus formellement, si nous nous déplaçons d'un point
$\vx$ à un point $\vy$ par un vecteur de translation $\vec{\delta}$ %(il y en a
%$8$ directions possibles en 2D, $26$ en 3D pour $||\vec{\delta}||_\infty = h$),
nous avons pour le calcul d'aire ou de volume :
%
\begin{align}
  & \AreaC\left(\DSh \cap \Ball{R}{\vx + \vec{\delta}}, h\right) =
  \AreaC\left(\DSh \cap \Ball{R}{\vx}, h\right) \\
  &\quad + \AreaC\left(\DSh \cap (\Ball{R}{\vx + \vec{\delta}} \setminus \Ball{R}{\vx} ), h\right) \\
  &\quad - \AreaC\left(\DSh \cap ( \Ball{R}{\vx} \setminus \Ball{R}{\vx + \vec{\delta}}),h\right) \,.
\end{align}
%
De la même manière, pour les moments :
%
\begin{align}
  & \DMom{pqs}{h}\left(\DSh \cap \Ball{R}{\vx + \vec{\delta}}, h\right) =
  \DMom{pqs}{h}\left(\DSh \cap \Ball{R}{\vx}, h\right) \\
  &\quad + \DMom{pqs}{h}\left(\DSh \cap ( \Ball{R}{\vx+ \vec{\delta}} \setminus \Ball{R}{\vx} ), h\right)\\
  &\quad - \DMom{pqs}{h}\left(\DSh \cap ( \Ball{R}{\vx} \setminus \Ball{R}{\vx+ \vec{\delta}} ), h\right) \,.
\end{align}
%
Alors, si nous précalculons tous les supports de déplacement
$\DigF{\Ball{R}{\mathbf{0} \pm \vec{\delta}} \setminus
\Ball{R}{\mathbf{0}}}{h}$ avec $||\vec{\delta}||_\infty = h$, la complexité de l'algorithme peut être réduite à
$O((R/h)^{d-1})$. Le cas idéal est la traversée hamiltonienne de la surface :
seul le premier surfel a besoin de calculer l'intersection entre la boule
entière et la forme, tous les autres surfels (du voisinage) n'auront à calculer
que les sous-supports $\DigF{\Ball{R}{\mathbf{0} \pm \vec{\delta}} \setminus
\Ball{R}{\mathbf{0}}}{h}$.


\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=6.7cm]{graphs/Timing2D}
    \includegraphics[width=6.7cm]{graphs/Timing2D_PF}\\
    \includegraphics[width=6.7cm]{graphs/Timing3D}
    \caption{
      %
      Temps de calcul (en échelle logarithmique) en millisecondes pour les
      estimateurs de \colorize{courbure} en 2D sur l'objet \Flower (\emph{en haut à gauche})
      et des estimateurs de \colorize{courbure moyenne} en 3D sur l'objet \RoundedCube
      (\emph{en bas}). Temps de calcul en secondes pour les estimateurs de
      \colorize{courbure} en 2D sans paramètre sur l'objet \Ellipse (\emph{en haut à droite}).
      Les résultats ont été obtenus sur un ordinateur de travail Intel Xeon
      2.27GHz.
      %
      }
      \label{fig:curv-experiments-timings}
  \end{center}
\end{figure}

La \RefFigure{fig:curv-experiments-timings} nous donne les temps de calcul pour
nos estimateurs 2D $\CurvH{R}$ (\II), $\CurvHS$, $\CurvHSL$ et les variantes à
nombre de rayons limités $\CurvHSLK{5}$, $\CurvHSLK{10}$ et $\CurvHSLK{50}$,
ainsi que pour les estimateurs \BC, \MDSS et \MDCA. Comme attendu, les approches
basées sur la reconnaissance d'objets en dimension 2 (\MDSS et \MDCA) ont les
temps de calcul les plus rapides. Nous pouvons observer que $\CurvH{R}$ (\II) est
plus lent que ces deux estimateurs mais plus rapide que \BC. Nous pouvons
également remarquer que $\CurvHSL$ est beaucoup plus lent que $\CurvHS$, ce qui
est normal puisqu'une nouvelle boule prenant en compte les informations locales
est calculée à chaque déplacement. Cet écart est très fortement diminué avec les
variantes limitant le nombre de rayons possibles pour $\CurvHSL$, se rapprochant
des temps de calcul de $\CurvHS$ et $\CurvH{R}$.


En 3D, alors que les vitesses de convergence asymptotique entre \JetFitting et
nos estimateurs sont similaires, nous pouvons observer que nos estimateurs sont
environ $10$ fois plus rapides que notre implémentation de \JetFitting (\CGal \cite{CGal}).

\subsection{Robustesse au bruit}
\label{sec:kanungo-noise}

Grâce à l'approche par intégration de volume de nos estimateurs, nous pouvons
espérer une robustesse aux bruits et aux données aberrantes (\anglais{outliers}).
Définissons dans un premier temps le modèle de bruit utilisé dans nos
expérimentations. Nous utilisons le modèle de bruit \Kanungo \cite{Kanungo1996}
sur les contours des objets digitaux. Ainsi, pour une taille de bruit $t \in
[0,1]$, la probabilité qu'un point digital $\vp$ situé à une distance $dt(\vp)$
de $\dS$ appartienne au contour bruité est $t^{1+dt(\vp)}$. Ainsi la majorité
des perturbations se situe dans une zone d'épaisseur $\frac{2h}{1-t}$ autour du
bord digital de la forme. Ce modèle de \Kanungo est dépendant du pas de
discrétisation $h$, ainsi plus le pas de discrétisation est petit (donc l'objet
de plus grande résolution), plus le bruit obtenu est faible. Ce comportement est
idéal pour évaluer la stabilité des algorithmes en géométrie digitale
\cite{Kerautret2012} puisque plus la résolution de l'objet digital augmente,
plus il se rapproche de la forme sous-jacente. Ce bruit est comparable au bruit
d'acquisition d'un scanner, et est utilisé dans le domaine de l'analyse de
documents.


\begin{figure}[ht]
  \begin{center}
    \setlength{\tabcolsep}{0.0pt}
    \begin{tabular}{@{}l c c @{}}
      \multicolumn{3}{c}{\textsc{Ellipse}}
      \\ \toprule
       &
      \includegraphics[width=4cm]{images/ellipse_noise} &
      \includegraphics[width=6cm]{images/ellipse_noise_border}
      \\
      \rotatebox{90}{~~~~~~~$\MeanCurvH{R}$} &
      \includegraphics[width=7cm]{graphs/Ellipse_Noise_L2} &
      \includegraphics[width=7cm]{graphs/Ellipse_Noise_Loo}
      \\
      &
      Erreur $l_2$ &
      Erreur $l_\infty$
    \end{tabular}
    \caption{
      %
      \emph{Première ligne :} \Ellipse avec du bruit de \Kanungo sur sa
      surface.
      %
      \emph{Seconde ligne :} Comparaison des erreurs $l_2$ (\emph{à gauche}) et
      $l_\infty$ (\emph{à droite}) d'estimation de \colorize{courbure} de
      $\CurvH{R}$ (\II) avec \MDCA, \MDSS et \BC sur l'objet \Ellipse bruité avec
      différents paramètres $t$ de bruit de \Kanungo.
      %
      }
      \label{fig:curv-experiments-ellipse-noise}
  \end{center}
\end{figure}

\begin{figure}[ht]
  \begin{center}
    \setlength{\tabcolsep}{0.0pt}
    \begin{tabular}{@{}l c c @{}}
      \multicolumn{3}{c}{\textsc{Rounded Cube}}
      \\ \toprule
       &
      \includegraphics[width=4cm]{images/RoundedCube2_Noise_Border} &
      \includegraphics[width=6cm]{images/RoundedCube2_Noise_Border_split}
      \\
      \rotatebox{90}{~~~~~~~$\MeanCurvH{R}$} &
      \includegraphics[width=7cm]{graphs/RoundedCube2_Noise_Mean_L2} &
      \includegraphics[width=7cm]{graphs/RoundedCube2_Noise_Mean_Loo}
      \\
      \rotatebox{90}{~~~~~~~$\PrincCurvH{1}{R}$} &
      \includegraphics[width=7cm]{graphs/RoundedCube2_Noise_k1_L2} &
      \includegraphics[width=7cm]{graphs/RoundedCube2_Noise_k1_Loo}
      \\
      \rotatebox{90}{~~~~~~~$\PrincCurvH{2}{R}$} &
      \includegraphics[width=7cm]{graphs/RoundedCube2_Noise_k2_L2} &
      \includegraphics[width=7cm]{graphs/RoundedCube2_Noise_k2_Loo}
      \\
      &
      Erreur $l_2$ &
      Erreur $l_\infty$
    \end{tabular}
    \caption{
      %
      \emph{Première ligne :} \RoundedCube avec du bruit de \Kanungo sur sa
      surface.
      %
       Comparaison des erreurs $l_2$ (\emph{à gauche}) et $l_\infty$ (\emph{à
       droite}) d'estimation de \colorize{courbure moyenne} de $\MeanCurvH{R}$
       (\emph{seconde ligne}), de \colorize{première courbure principale} de
       $\PrincCurvH{1}{R}$ (\emph{troisième ligne}) et de \colorize{seconde
       courbure principale} de $\PrincCurvH{2}{R}$ (\emph{quatrième ligne}) avec
       \JetFitting sur l'objet \RoundedCube bruité avec différents paramètres $t$
       de bruit de \Kanungo.
      %
      }
      \label{fig:curv-experiments-rounded-noise}
  \end{center}
\end{figure}

Nous expérimentons nos estimateurs de courbure en 2D et en 3D avec des objets bruités
dans les
\RefFiguresDF{fig:curv-experiments-ellipse-noise}{fig:curv-experiments-rounded-noise}
sur les objets \Ellipse et \RoundedCube ($h = 0.1$). Nos critères
d'expérimentation sont les suivants : à partir d'objets non bruités, nous
augmentons progressivement le paramètre de bruit de \Kanungo $t$ de 0 à 1 et
nous observons le comportement des estimateurs. En 2D, les approches basées sur
la reconnaissance d'objets géométriques (\MDSS et \MDCA) sont très sensibles aux
perturbations du bord de la forme, comme le montre la
\RefFigure{fig:curv-experiments-ellipse-noise}. Les estimateurs $\CurvH{R}$
(\II) et \BC sont extrêmement robustes au bruit lorsqu'il n'est pas trop grand
($t < 0.5$). Nous pouvons d'ailleurs observer que notre estimateur semble plus
robuste que l'estimateur \BC. En dimension 3, nous pouvons constater que nos
estimateurs et l'approche de \JetFitting obtiennent des résultats plutôt
stables. Ce n'est pas surprenant pour \JetFitting puisque cette approche dépend
d'une analyse en composante principale (\ACP) sur un ensemble de points, ce qui
est plutôt robuste. En conclusion, expérimentalement, nos
estimateurs semblent assez robuste au bruit de \Kanungo.

\section{Conclusion}
\label{sec:estimators:conclusion}

Nous avons présenté dans ce chapitre un ensemble d'estimateurs de courbure,
courbure moyenne, courbures principales, directions principales de courbures et
normale basés sur le principe d'intégration d'invariants sur des formes de
dimension 2 et 3. Nous avons démontré que ces estimateurs convergent
asymptotiquement pour une certaine catégorie de formes continues et nous avons
explicité leurs vitesses de convergence. Nous avons également évalué
expérimentalement ces estimateurs et observé que les vitesses de convergence
respectent bien les vitesses attendues, et la comparaison avec les autres
méthodes rend nos estimateurs très compétitifs, même en présence de bruit.

\comJeremy{Ces estimateurs souffrent par contre de certaines limitations. La
première, et probablement la plus grande limitation vient du fait que nous
intégrons des données volumiques sur la forme $\Shape$, et non surfaciques comme
la plupart des autres méthodes, comme celles basées sur les nuages de points par
exemple. Cela implique une plus grande complexité d'algorithme, mais également
de pouvoir quantifier ces données volumiques lorsqu'elles ne sont pas directes.
Cependant, cela amène un certaine robustesse sur des données bruitées. Nous
verrons dans le \RefChapitre{sec:applications:dgtal} que nous pouvons optimiser
l'algorithme afin de se rapprocher de la complexité des méthodes surfaciques. De
plus, \cauthors{Pottmann}{Pottmann2007,Pottmann2009} définissent des estimateurs
surfaciques de courbure se basant sur le même principe de l'intersection d'une
sphère et de la forme. Il serait intéressant de les définir sur des données
digitales, ainsi de pouvoir les comparer -- théoriquement et expérimentalement --
avec les variantes volumiques (ils seront très probablement plus sensibles au
bruit sur la surface, pour une compléxité similaire).}
% Ces estimateurs souffrent par contre de certaines limitations. La première et
% probablement la plus grande limitation est la nécessité de connaître une
% approximation volumétrique de la forme $\Shape$. Nous ne pouvons pas utiliser
% simplement un approximation du bord de $\Shape$, comme c'est le cas pour les
% algorithmes fonctionnant sur les nuages de points par exemple.


La seconde provient de la constante $k$ guidant le rayon de la boule
d'intégration ($R=kh^\frac{1}{3}$) dans le contexte de la convergence
asymptotique des estimateurs. Le choix du $k$ optimal n'est pas évident,
et est principalement lié aux bornes d'erreur de l'approximation de Taylor
permettant d'extraire l'information de courbure qui ne sont pas explicites.
L'intuition est que ce paramètre $k$ doit certainement dépendre de la dérivée
des courbures maximales, mais cela reste à prouver. Ainsi, pour une forme
donnée, nous pouvons chercher à optimiser cette constante afin d'avoir une
erreur asymptotique plus faible mais celle-ci n'influera d'aucune sorte sur la
vitesse de convergence.


La troisième limitation est le temps de calcul sur de gros objets digitaux. Il
n'est pas rare d'avoir dans l'industrie des objets à analyser avec une haute
résolution (par exemple $\nombre{1024}^3$) et le coût de l'estimation peut
s'avérer élevé pour certaines utilisations. Cependant, cette limitation
est à mettre en parallèle avec le fait que nos estimateurs sont robustes au
bruit, et c'est certainement le prix à payer (les approches de reconnaissances
d'objets par exemple sont plus rapides, mais très sensibles au bruit, comme nous
l'avons montré). Il est à noter que pour obtenir exactement les mêmes résultats avec
\BC et \II (\cad quand l'erreur d'estimation en norme $l_\infty$ par rapport à
la vraie valeur de courbure est égale entre les deux estimateurs), \BC a besoin
d'aller à une résolution de grille dix fois supérieure par rapport à notre
estimateur, comme le montre le \RefTable{tab:comp-BC-II}, avec le coût en temps
de calcul qui en résulte.

\begin{table}[]
\centering
%
\caption{Comparaison des estimateurs \BC et \II sur la forme \Ellipse pour une
erreur d'estimation en norme $l_\infty$ égale (erreur de $0.046$).}
%
\label{tab:comp-BC-II}
\setlength{\tabcolsep}{10pt}
\begin{tabular}{@{}lrr@{}}
\toprule
                                                  & \BC                     & \II                     \\ \midrule
pas de discrétisation $h$                         & $0.003$                 & $0.03$                  \\
temps de calcul (\emph{en ms})                    & $\nombre{421945}$       & $\nombre{350}$          \\
Taille de masque                                  & $\nombre{91336}$        & $\nombre{8349}$         \\
\multicolumn{1}{r}{(\emph{avec optimisation})}    &                         & $8 \times 145$          \\ \bottomrule
\end{tabular}
\end{table}


L'inconvénient avec les estimateurs précédents est que lorsque nous souhaitons
collecter la valeur de courbure pour un objet particulier, nous devons adapter
le rayon de la sphère en fonction de l'objet. Nous avons alors proposé
d'analyser la géométrie de l'objet digital en amont à l'aide de segments
maximaux afin de proposer des estimateurs de courbures en dimension 2 et 3 sans
paramètre. En effet, les segments maximaux permettent d'extraire l'échelle de la
forme digitale lorsque nous ne connaissons pas le pas de discrétisation $h$ pour
une forme digitale donnée. De plus, les propriétés sur les longueurs des
segments maximaux en font de très bons candidats pour remplacer le rayon de
la boule d'intégration des estimateurs de courbures précédents, les rendant
ainsi sans paramètre.


Nous avons alors défini deux approches d'estimateurs grâce à ces longueurs. Une
approche « globale », \cad que l'ensemble des longueurs des segments maximaux de
la forme nous donnent un rayon global pour la sphère d'intégration. La
convergence asymptotique des estimateurs dont le rayon est guidé par la longueur
moyenne des segments maximaux a alors été prouvé en dimension 2
($O(h^\frac{1}{3} \log^2 \left(\frac{1}{3}\right)$), se basant une grande partie
sur la preuve des estimateurs précédents et des lois asymptotiques des longueurs
des segments maximaux. En dimension 3, la preuve de convergence en
$O(h^\frac{1}{3} \log^2 \left(\frac{1}{3}\right)$ des estimateurs de courbure
sans paramètre repose sur une supposition non vérifiée à ce jour, qui mériterait
d'être étudiée. L'analyse comparative de ces estimateurs sans paramètre ont
montré que la convergence asymptotique était plutôt en $O(h^\frac{1}{3})$ (le
terme en $\log^2 \left(\frac{1}{3}\right)$ de l'erreur semble ne pas intervenir
dans notre évaluation expérimentale), comme les versions avec le paramètre de
rayon.


Ensuite, nous avons défini une approche « locale », où le rayon de la sphère est adapté localement en
fonction de la géométrie de la forme (en fonction des longueurs des segments
maximaux). À cause de la borne d'erreur d'un cas pathologique des segments
maximaux, nous ne pouvons espérer de preuves théoriques de convergence
asymptotique de ces estimateurs. Cependant, d'après l'évaluation expérimentale,
ces estimateurs semblent suivre le comportement asymptotique de l'approche «
globale ». La distinction se fait avec l'erreur moyenne, l'erreur de norme $l_2$
où ces estimateurs obtiennent de très bons résultats.


La principale limitation de ces méthodes sans paramètre est qu'elles sont très
dépendantes de la qualité de discrétisation de l'objet digital, comme la plupart
des estimateurs reconnaissant des primitives sur la surface digitale.
